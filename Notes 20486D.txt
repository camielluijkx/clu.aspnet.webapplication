[Abbreviations]

Internet Information Services (IIS)
Integrated Development Environment (IDE)
Model-View-Controller (MVC)
Internet Service Provider (ISP)
Asynchronous JavaScript and XML (AJAX)
Single Page Application (SPA)
Representational State Transfer (REST)
Extensible Markup Language (XML)
JavaScript Object Notation (JSON)
Transport Layer Security (TLS)
Test Driven Development (TDD)
Application Programming Interface (API)
Create, Read, Update and Delete (CRUD)
Command Line Integration (CLI)
Object-Relational Mapping / Mapper (ORM)
Entity Framework (EF)
Extreme Programming (XP)
Unified Modeling Language (UML)
Logical Data Model (LDM)
Database Administrator (DBA)
Hypertext Transfer Protocol (HTTP)
Secure Sockets Layer (SSL)
Windows Communication Foundation (WCF)
Language-Integrated Query (LINQ)
Globally Unique Identifier (GUID)
Dependency Injection (DI)
Uniform Resource Locator (URL)
Search Engine Optimization (SEO)
Data Definition Language (DDL)
Universal Windows Platform (UWP)
plain-old CLR objects (POCO)
Package Manager Console (PMC)
Cascading Style Sheets (CSS)
Content Delivery Network (CDN)
Node Package Manager (npm)
Cross-Site Scripting (XSS)
Cross-Site Request Forgery (CSRF or XSRF)
Cross-Origin Resource Sharing (CORS)
World WIde Web (WWW)
Remote Procedure Calls (RPC)
Remote Method Invocation (RMI)
Uniform Resource Identifier (URI)
Multipurpose Internet Mail Extensions (MIME)
Simple Object Access Protocol (SOAP)
Open Web Interface (OWIN)
Distributed Denial-of-Service (DDOS)
Framework-Dependent Deployment (FDD)
Self-Contained Deployment (SCD)
Infrastructure as a service (IaaS)
Platform as a service (PaaS)
Software as a service (SaaS)
Server Message Block (SMB)
Database Transaction Unit (DTU)
Azure Stack Development Kit (ASDK)
time-to-live (TTL)
hardware security modules (HSMs)
Active Directory (AD)
JSON Web Token (JWT)


[Summary 20486D]

Module 1: Exploring ASP.NET Core MVC
	Lesson 1: Overview of Microsoft Web Technologies
		Introduction to Microsoft Web Technologies:
			Developer Tools:
				• Visual Studio
				• Visual Studio Code
			Hosting Technologies:
				• Web Server:
					• Microsoft Internet Information Services
					• Microsoft Azure
				• Database Server:
					• Microsoft SQL Server
					• Microsoft Azure SQL Database
			Code Execution Technologies:
				• Server-Side Execution (on web server)
				• Client-Side Execution (in web browser)
		Overview of ASP.NET:
			Programming Models:
				• Web Pages (ASP.NET 4.x only)
				• Web Forms (ASP.NET 4.x only)
				• MVC (both ASP.NET Core and ASP.NET 4.x):
					• Model: An MVC model defines a set of classes that represent the object types that the web application manages.
					• Controller: An MVC controller is a class that handles user interaction, creates and modifies model classes, and selects appropriate views.
					• View: An MVC view is a component that builds the web pages that make up the web application’s user interface. Controllers often pass an instance of a model class to a view. The view displays properties of the model class.
					MVC versus Web Pages / Web Forms: separation of concerns, improves testability, more control over generated HTML	
				• Razor Pages (ASP.NET Core only)
				• Web API (both ASP.NET Core and ASP.NET 4.x):
					• Controller: A Web API Controller is a class that handles the client request that was sent to the server. It accesses the database, retrieves information, updates the database, if needed, and returns the HTTP response including a status code that indicates whether the action succeeded and the data, if needed.
					• Model: As With MVC, it is a set of classes that represent the object types that the web application manages.
					• Client: The client sends requests to the server to run specific actions in the Web API Controller. On the server side, you create an interface that consists of functions that can be accessed via HTTP. Those calls are sent from the client to the server to retrieve specific information and perform read-write operations.
		Client-Side Web Technologies:
			JavaScript
			jQuery: Provides elegant functions for interacting with the HTML elements on your page and with CSS styles.
			AJAX
			Angular (Typescript, Transpiling, Client-Side Component-Based Framework)
			React (Component-Based Library)
		Hosting Technologies:
			Internet Information Services Features:
				• Deployment Protocols
				• Centralized Web Farm Management
				• High-Performance Caches
				• Authentication and Security
				• ASP.NET Support
				• Other Server-Side Technologies
			Scaling Up IIS:
				• scalability
				• resilience
			Perimeter Networks:
				A perimeter network has a network segment that is protected from the internet through a firewall that validates and permits incoming HTTP requests. A second firewall, which permits requests only from the web server, separates the perimeter network from the internal organizational network. 
			IIS Express
			Other Web Servers (Apache, NGINX)
			Microsoft Azure:
				• Flexible Scaling
				• Flexible Pricing
				• Worldwide presence
				• First-class security and reliability
				Hosting with Azure: Web Apps, Databases, Virtual Machines, Mobile Apps, Media Services
	Lesson 2: Overview of ASP.NET 4.x
		Overview of Web Pages:
			The @ symbol is used to distinguish server-side code from HTML and JavaScript. When users request the page, the ASP.NET 4.x runtime compiles and runs the server-side code to render HTML and returns that HTML to the web browser for display.
			The Web Pages programming model has the following advantages:
				• It is simple to learn.
				• It provides precise control over the rendered HTML.
			Using a Web Pages site has some disadvantages:
				• It provides no control over URLs that appear in the address bar.
				• Large websites require large numbers of pages, each of which must be coded individually.
				• There is no separation of business logic, input logic, and the user interface.
		Overview of Web Forms:
			ASP.NET 4.x provides a wide variety of highly-functional controls that you assemble on Web Forms:
				• Input controls, such as text boxes, option buttons, and check boxes.
				• Display controls, such as image boxes, image maps, and ad rotators.
				• Data display controls, such as grid views, form views, and charts.
				• Validation controls, which check data entered by the user.
				• Navigation controls, such as menus and tree views.
			Web Forms Code Files:
				• page markup and code-behind file (example Default.aspx + Default.aspx.cs)
				• control markup and code-behind file (example CustomControl.ascx + CustomControl.ascx.cs)
			Binding Controls to Data:
				bind controls to data sources
			The Web Forms programming model has the following advantages:
				• You can design your page visually by using server controls and the Design view.
				• You can use a broad range of highly functional controls that encapsulate a lot of functionality.
				• You can display data without writing many lines of server-side code.
				• The user interface in the .aspx file is separated from input and business logic in the code-behind files.
			Using a Web Forms site has some disadvantages:
				• The page lifecycle is an abstraction layer over HTTP and can behave in unexpected ways. You must have a complete understanding of this life cycle, to write code in the correct event handlers.
				• You do not have precise control over the markup generated by the server-side controls.
				• Controls can add large amounts of markup and state information to the rendered HTML page. This increases the time taken to load pages.
		Overview of MVC:
			Strong separation of business logic, data access code, and the user interface into models, controllers, and views.
			Models:
				A model contains application business logic, validation, and database access logic. Each website presents information about different kinds of objects to site visitors. 
			Views:
				Views are markup pages that store both HTML and C# code in .cshtml files. This means that they are like Web Pages, but they include only user interface code. Other logic is separated into models and controllers.
			Controllers:
				Controllers respond to user actions, load data from a model, and pass it to a view so that it will render a webpage.
				Controllers are .NET classes that inherit from the System.Web.Mvc.Controller class and store code in .cs files.
				The MVC programming model has the following advantages:
					• Views enable the developer to take precise control of the HTML that is rendered.
					• You can use the Routing engine to take precise control of URLs.
					• Business logic, input logic, and user interface logic are separated into models, controllers, and views.
					• Unit testing techniques and Test Driven Development (TDD) are possible.
				Using an MVC site has some disadvantages:
					• MVC is potentially more complex to understand than Web Pages or Web Forms.
					• MVC forces you to separate your concerns (models, views, and controllers). Some programmers may find this challenging.
					• You cannot visually create a user interface by dragging controls onto a page.
					• You must have a full understanding of HTML, CSS, and JavaScript to develop views.
		Shared ASP.NET 4.x Features:
			The ASP.NET 4.x API:
				• Configuration:
					• Using the Web.config files, you can configure your web application, regardless of the programming model. The Web.config files are XML files with specific tags and attributes that the ASP.NET 4.x runtime accepts.
					• In code, you can access the configuration through the System.Web.Configuration namespace.
				• Authentication and Authorization:
					• You can use ASP.NET 4.x membership providers to authenticate and authorize users and restrict access to content. 
					• You can also build pages that enable users to register a new account, reset a password, recover a lost password, or perform other account management tasks. 
					• Membership providers belong to the System.Web.Security namespace.
				• Caching:
					• You can use caching to mitigate this delay. ASP.NET 4.x caches a rendered page in memory so that it can return the same page to subsequent user requests without having to render it again from the start. 
					• In a similar manner, .NET Framework objects can also be cached. 
					• You can access cached pages by using the System.Runtime.Caching namespace and configure the caches in Web.config.
			Compiling ASP.NET Code:
				Because ASP.NET 4.x server-side code uses .NET Framework, you must write code in a .NET managed programming language such as C# or Visual Basic. Before running the code, it must be compiled into native code so that the server CPU can process it. 
				This is a two-stage process:
					1. Compilation to MSIL. When you build a website in Visual Studio, the ASP.NET 4.x compiler creates .dll files with all the code compiled into Microsoft Intermediate Language (MSIL). This code is both independent of the language you used to write the application and the CPU architecture of the server.
					2. Compilation to native code. When a page is requested for the first time, the Common Language Runtime compiles MSIL into native code for the server CPU.
				This two-stage compilation process enables components written in different languages to work together and enables many errors to be detected at build time. 
				When you use the default compilation model, delays can arise when the first user requests a page. This is because ASP.NET 4.x must compile the page before serving it to the browser. To avoid such delays and to protect source code, use pre-compilation. When you pre-compile a site, all the ASP.NET 4.x files, including controllers, views, and models, are compiled into a single .dll file.
				Configuration:
					When you configure an ASP.NET 4.x site, you can control how errors are handled, how the site connects to databases, how user input is validated, and many other settings. You can configure ASP.NET 4.x sites by creating and editing the Web.config files. The Web.config file in the root folder of your site configures the entire site, but you can override this configuration at lower levels by creating Web.config files in sub-folders.
					Web.config files are XML files with a set of elements and attributes that the ASP.NET runtime accepts.
					If you need to access configuration values at runtime in your server-side .NET code, you can use the System.Web.Configuration namespace.
				Authentication:
					ASP.NET 4.x supports several mechanisms for authentication. 
					Integrated Windows Authentication mechanism uses your Windows user account to identify you. 
					Forms Authentication is supported by many browsers and it can be configured to check credentials against a database, directory service, or other user account stores.
				Membership and Roles:
					In ASP.NET 4.x, a membership provider is a component that implements user account management features. Several membership providers are supported by ASP.NET 4.x, such as the SQL membership provider, which uses a SQL database to store user accounts. You can also create a custom membership provider, inheriting from one of the default providers if you have unique requirements.
					When you have more than a few users, you may want to group them into roles with different levels of access. ASP.NET 4.x role providers enable you to create and populate roles with the minimum of custom code.
					You can enable access to pages on your website for individual user accounts or for all members of a role. This process is known as authorization.
				State Management:
					Web servers and web browsers communicate through HTTP. This is a stateless protocol in which each request is separate from requests before and after it. Any values from previous requests are not automatically remembered.
					However, when you build a web application, you must frequently preserve values across multiple page requests. ASP.NET 4.x provides several locations where you can store such values or state information across multiple requests.
				Caching:
					ASP.NET 4.x runtime on web server might run C# code that perform complex and time-consuming operations. It may run multiple queries against a database or call services on remote servers. You can mitigate these time delays by using ASP.NET 4.x caches.
					You can use the ASP.NET 4.x page cache to store the rendered version of a commonly requested page in the memory of the web server.
		Overview of Web API:
			Web API is an API that exists over the web and can be accessed using the HTTP protocol.
			Web API also allows you to implement REST services in your application and with that reduce application overhead and limit the data that is transmitted between client and server systems. 
			Web API allows you to call methods it exposes by using the server-side or client-side code and it allows you to implement REST-style Web APIs in your application.
			Call a Web API using AJAX:
				• JavaScript. Allows you to perform AJAX calls using the XMLHttpRequest object or the fetch method.
				• jQuery. Simplifies the process of writing AJAX calls by using the $.ajax method or other methods available in jQuery such as $.get or $.post.
				• Angular. Has dedicated objects for performing AJAX calls.
			The Web API programming model has the following advantages:	
				• Helps in creating RESTful APIs.
				• Enables external systems to use your application's business logic and features.
				• Can be accessed by various HTTP clients such as Windows, Android, IOs, and more.
				• Helps obtain data in different formats such as JSON, XML, and custom formats.
				• Supports create, read, update and delete (CRUD) actions since it works with HTTP verbs such as GET, POST, PUT, and DELETE.
				• Is ideal for mobile application integration.
			Using Web API has some disadvantages:
				• With Web API you have a complete separation between server-side code and client-side code. For some programmers, this is an advantage, but others may find this challenging and hard to understand.
				• The data returned from Web API usually is not indexed by search engines such as Google, more work needs to be done to make it crawlable.
	Lesson 3: Introduction to ASP.NET Core MVC
		Introduction to ASP.NET Core:
			ASP.NET Core is a cross-platform, open-source framework that allows you to create and host dynamic, powerful, and extensible web-based applications. With ASP.NET Core you can develop on various platforms – Windows, macOS, and Linux, deploy your application to the cloud, and create services and mobile backends.
			Why should you use ASP.NET Core?
				ASP.NET Core is a redesign of ASP.NET 4.x and it brings in many architectural shifts. ASP.NET Core is much leaner and modular than ASP.NET 4.x and has many changes within it.
				ASP.NET Core includes three main programming models that can help you to develop your applications smoothly:
					• MVC: 
						When you build a website using ASP.NET Core MVC, you separate your code into three parts: model, view, and controller. This separation of model, view, and controller code ensures that MVC applications have a logical structure, even for the most complex sites. It also improves the testability of the application.
					• Razor Pages:
						While in MVC we have a controller that receive requests, a model and a view that generates the response, with Razor Pages it is different. The request goes straight to a page that is generally located in the Pages folder.
						Each Razor Page file has an accompanying .cshtml.cs file that contains all the methods, model handlers, and logic. It contains a class that inherits from PageModel that will initialize the model inside the OnGet method. The OnGet method is also the place to get data from the database. Each public property inside the model can be displayed on the page using Razor syntax.
						What makes it different is the @page directive that must be at the first line of the .cshtml file. @page converts the file into a Razor Page, which means that it handles requests directly without going through a controller like in the MVC programming model.
						Razor syntax can be used inside Razor Pages. It allows you to insert the properties from the model and other sources into the HTML markup. Razor syntax starts with @ character. When you want to display properties from the model inside the HTML markup, you will use @model. @model will have a reference to the properties you initialized inside the OnGet method.
						The main difference between Razor Pages and MVC:
							In Razor Pages, the model and the controller are included in the Razor Pages classes and the request goes straight to the page. On the other hand, MVC programming model separates the code into models, views, and controllers.
					• Web API:
						• Controller. A Web API controller is a class that handles the client request that was sent to the server. It accesses the database, retrieves information, updates the database, if needed, and returns the HTTP response, including status code that indicates whether the action succeeded and data, if needed.
						• Model. As with MVC, it is a set of classes that represent the object types that the web application manages.
						• Client. The client sends requests to the server to run specific actions in the Web API controller. On the server side, there is an interface that consists of functions that can be accessed via HTTP. Those calls are sent from the client to the server to retrieve specific information and perform read-write operations.
		Choose between .NET Core and .NET Framework:
			.NET Standard:
				.NET Standard is a formal specification of the .NET APIs. It is intended to be available on all .NET implementations and its goal is to create a unification of the .NET ecosystem. It enables you to create code that is portable across all .NET implementations. Each .NET implementation is targeting a specific version of .NET Standard.
				A higher version number means that the previous versions are also supported but it has some extensions.
			When to use .NET Core for your application?
				You want your code to work cross-platform
				You want to create microservices
					In microservices architecture, the services layer is broken down into multiple independent components. A microservices architecture also allows you to create a mix of technologies across a service boundary.
				You want to use Docker containers (cross-platform, smaller size)
				You want to achieve a high-performing scalable system
			When to use .NET Framework for your application?
				You want to extend an existing application that uses .NET Framework (extend instead of migrate)
				You want to use NuGet packages or third-party .NET libraries that are not supported in .NET Core (not yet or not compatible)
				You want to use .NET technologies that aren't supported in .NET Core
				You want to use a platform that doesn’t support .NET Core.
		Models, Views, and Controllers:
			Models represent data and the accompanying business logic. Controllers interact with user requests and implement input logic. Views build the user interface.
			Models and Data:
				A model is a .NET class that represents an object handled by your website.
				Like any other .NET classes, model classes can include a constructor, which is a procedure that runs when a new instance of that class is created. You can also include other procedures, if necessary. These procedures encapsulate the business logic. 
				In an MVC application, the model includes code that reads and writes database records. ASP.NET Core MVC works with many different data access frameworks.
			Controllers and Actions:
				A controller is a .NET class that responds to web browser requests in an MVC application. There is usually one controller class for each model class. Controllers include actions, which are methods that run in response to a user request. 
				Controllers inherit from the Microsoft.AspNetCore.Mvc.Controller base class. Actions usually return an object that implements the Microsoft.AspNetCore.Mvc.IActionResult interface.
			Views and Razor:
				A view is a .cshtml file that includes both the HTML markup and programming code. A view engine interprets the view files, runs the server-side code, and renders HTML to the web browser. Razor is the default view engine in ASP.NET Core MVC.
				@ symbol delimits server-side code
			Request Life Cycle:
				The Request life cycle comprises a series of events that happen when a web request is processed.

Module 2: Designing ASP.NET Core MVC Web Applications
	Lesson 1: Planning in the Project Design Phase
		Project Development Methodologies:
			Waterfall Model:
				• Feasibility analysis. In this phase, planners and developers study and determine the approaches and technologies that can be used to build the software application.
				• Requirement analysis. In this phase, planners and analysts interview the users, managers, administrators, and other stakeholders of the software application to determine their needs.
				• Application design. In this phase, planners, analysts, and developers record a proposed solution.
				• Coding and unit testing. In this phase, developers create the code and test the components that make up the system individually.
				• Integration and system testing. In this phase, developers integrate the components that they have built and test the system as a whole.
				• Deployment and maintenance. In this phase, developers and administrators deploy the solution so that users can start using the software application.
			Iterative Development Model:
				break the project into small parts, for each part perform activities related to all the stages of the waterfall model, testing at each stage to ensure quality, corrective actions at the end of each iteration, requirements are added at the end of each iteration, revisit planned efforts and schedule
			Prototyping Model:
				 few or meagerly defined business requirements, vague understanding of needs and how to solve them, simplified version of software application, seek feedback from stakeholders, used to define the detailed requirements, consensus on requirements after couple of iterations, complete solution is built and tested, can lead to a poorly-designed application, at no stage in the project clear focus on the overall architecture
			Agile Software Development Model:
				The waterfall model, the iterative development model, and the prototyping model are based on the premise that business requirements and other factors do not change from the beginning to the end of the project. In reality, this assumption is often invalid. Agile software development is a methodology designed to integrate changing circumstances and requirements throughout the development process.
				• Incremental development. Software is developed in rapid cycles that build on earlier cycles. Each iteration is thoroughly tested.
				• Emphasis on people and interactions. Developers write code based on what people do in their role, rather than what the development tools are good at.
				• Emphasis on working software. Instead of writing detailed design documents for stakeholders, developers write solutions that stakeholders can evaluate at each iteration to validate if it solves a requirement.
				• Close collaboration with customers. Developers discuss with customers and stakeholders on a day-to-day basis to check requirements.
			Extreme Programming:
				preliminary design phase is reduced to a minimum and developers focus on solving a few critical tasks, developers test the simplified solution and obtain feedback from stakeholders, feedback helps developers identify the detailed requirements, XP defines a user story for every user role, user story describes all the interactions that a user with a specific role might perform within the completed application, collection of all the user stories for all user roles describes the entire application, developers often work in pairs, one developer writes code and other developer reviews code to ensure that it uses simple solutions and adheres to best practices, TDD is a core practice in extreme programming
			Test Driven Development:
				developers write test code as their first task in a given iteration, after you define the tests, you write the component such that it will pass those tests
			Unified Modeling Language:
				industry standard notation to record the design of any application that uses object-oriented technology, often used for planning and documenting application architecture and components, across all project development methodologies
				• Behavior diagrams. These diagrams depict the behavior of users, applications, and application components.
				• Interaction diagrams. These diagrams are a subset of behavior diagrams that focus on the interactions between objects.
				• Structure diagrams. These diagrams depict the elements of an application that are independent of time. This means they do not change through the lifetime of the application.
		Gathering Requirements:
			• Functional requirements. They describe how the application behaves and responds to users. Functional requirements are often called behavioral requirements. They include:
				o User interface requirements. They describe how the user interacts with an application.
				o Usage requirements. They describe what a user can do with the application.
				o Business requirements. They describe how the application will fulfill business functions.
			• Technical requirements. They describe technical features of the application and relate to availability, security, or performance. These requirements are sometimes called non-functional or non-behavioral requirements.
			Usage Scenarios and Use Cases:
				A usage scenario is a specific real-world example, with names and suggested input values, of an interaction between the application and a user.
				A use case is similar to a usage scenario, but is more generalized. Use cases do not include user names or input values. They describe multiple paths of an interaction, which depends on what the user provides as input or other values. 
				By analyzing usage scenarios and use cases, you can identify functional requirements of all types.
			Agile Requirements Modeling:
				• Initial requirement modeling. In the initial design phase, developers identify and record a few broad use cases in an informal manner without complete details.
				• Just-in-time modeling. Before writing code that implements a use case, a developer discusses it with the relevant users. At this point, the developer adds complete details to the use case. In an agile development project, developers talk to users and other stakeholders at all times, and not just at the beginning and end of the project.
				• Acceptance testing. An acceptance test is a test that the application must pass for all the stakeholders to accept and sign off on the application. When you identify a functional requirement, you can also specify a corresponding acceptance test that must be run to ensure that the requirements are met.
			User Stories in Extreme Programming:
				developers perform even less functional requirement analysis at the beginning of the project compared with other development models, create user stories, instead of use cases or user scenarios
				User stories contain just the minimal details to enable developers to estimate the effort involved in developing the solution.
				Extreme programmers discuss each user story with stakeholders just before they write code to implement each user story.
		Planning the Database Design:
			Logical Modeling:
				A domain model diagram, also known as a conceptual data model, shows the high-level conceptual objects that your web application manages.
				The domain model does not include details of the properties of each concept, but shows the relationships between the concepts.
			Physical Database Structure:
				• Tables. These are the fundamental storage objects in a database. When you define a table, you need to specify the columns for that table. For each column, you must define a data type such as integer, string, usually the nvarchar type in Microsoft SQL Server, or date and time. You should also define the primary key for the table—the value of this column uniquely identifies each record and is essential for defining the relationships with records in other tables.
				• Views. These are common presentations of data in tables and are based on queries. For example, a view can join two tables, such as a products table and a stock levels table.
				• Stored procedures. These are common sequences of database operations that you define in the database. Some operations are complex and might involve a complex transformation of the data. You can define a stored procedure to implement such a complex routine.
				• Security. You need to consider how the web application will authenticate with the database server and how you will authorize access to each database table.
			Working with Database Administrators:
				in larger organizations or in projects where the database stores critical business information, there might be a dedicated team of database administrators
				DBAs are usually highly skilled in database design and administration, and it is their job to ensure data integrity based on the organization’s data storage policy
				DBAs are critical contributors in delivering the web application
			Database Design in Agile Development and Extreme Programming:
				In agile development and extreme programming, the database design changes throughout the project until deployment. Therefore, developers should be able to alter the database whenever necessary without consulting DBAs or complying with complex data policies.
		Planning for Distributed Applications:
			Distributed web applications often use a layered architecture:
				• Presentation layer. Components in this layer implement the user interface and presentation logic. If you are building an MVC web application, views and controllers make up your presentation layer.
				• Business logic layer. Components in this layer implement high-level business objects such as products, or customers. If you are building an MVC web application, models make up your business logic layer.
				• Data access layer. Components in this layer implement database access operations and abstract database objects, such as tables, from business objects. If you are building an MVC web application, models often make up both business logic and data access layers. However, with careful design and coding practices, it is possible to refactor code to separate these layers.
				• Database layer. The database itself.
			If you implement such a layered architecture in your web application, you can host each layer on separate servers.
			This approach has the following advantages:
				• You can specify server hardware that closely matches each role.
				• You can dedicate multiple servers to each role to ensure that failure of a single server does not cause an interruption in service.
				• Only the web servers must be on the perimeter network. Both middle-tier servers and database servers can be protected by two firewalls without direct access from the Internet.
				• Alternatively, you can host middle-tier layers and databases on a cloud service, such as Microsoft Azure.
			Communication Between Layers:
				When you run different layers on different servers, you must consider two factors:
				• How does each layer exchange information and messages?
				• How does each server authenticate and secure communications with other servers?
				The communication of information and security is performed in different ways between the various layers:
					• Between the browser and presentation layer web server. In any web application, the web browser, where the presentation layer runs, communicates with the web server by using Hypertext Transfer Protocol (HTTP). If authentication is required, it is often performed by exchanging plain text credentials. You can also use Secure Sockets Layer (SSL) to encrypt this sensitive communication.
					• Between the web server and the middle-tier server. The communication and security mechanisms used for communication between the web server and the middle-tier server depends on the technology that you use to build the business logic components. Two common technologies are Web API and Windows Communication Foundation (WCF).
					• Between the middle-tier server and database. The middle-tier server sends T-SQL queries to the database server, which authenticates against the database by using the required credentials. These are often included in the connection string.
		Planning State Management:
			In application development, the application state refers to the values and information that are maintained across multiple operations. HTTP is fundamentally a stateless protocol, which indicates that it has no mechanism to retain state information across multiple page requests. However, there are many scenarios, such as the following, which require state to be preserved:
				• User preferences. Some websites enable users to specify preferences. If this preference information is lost between page requests, users have to repeatedly apply the preference.
				• User identity. Some sites authenticate users to provide access to members-only content. If the user identity is lost between page requests, the user must re-enter the credentials on every page.
				• Shopping carts. If the content of a shopping cart is lost between page requests, the customer cannot buy anything from your web application.
			Client-Side State Storage:
				Use client-side state storage only for small amounts of data:
					• Cookies. Cookies are small text files that you can pass to the browser to store information. A cookie can be stored:
						o In the memory of the client computer, in which case, it preserves information only for a single user session.
						o On the hard disk drive of the client computer, in which case, it preserves information across multiple sessions.
					• Most browsers can store cookies only up to 4,096 bytes and permit only 20 cookies per website. Therefore, cookies can be used only for small quantities of data. Also, some users might disable cookies for privacy purposes, so you should not rely on cookies for critical functions.
					• Query strings. A query string is the part of the URL after the question mark and is often used to communicate form values and other data to the server. You can use the query string to preserve a small amount of data from one page request to another. All browsers support query strings, but some impose a limit of 2,083 characters on the URL length. You should not place any sensitive information in query strings because it is visible to the user, anyone observing the session, or anyone monitoring web traffic.
			Server-Side State Storage:
				The following locations store state information in server memory:
					• TempData. This is a state storage location that you can use in MVC applications to store values between one request and another. This information is preserved for a single request only and is designed to help maintain data across a webpage redirect.
					• Application state. This is a state storage location that you can use to store values for the lifetime of the application. The values stored in application state are shared among all users. Application state is not an appropriate place to store user-specific values, such as preferences, because if you store a preference in application state, all users share the same preference, instead of having their own unique value.
					• Session state. This is a state storage location that you can use to store information for the lifetime of a single browser session and values stored here are specific to a single user session; they cannot be accessed by other users. Session state is available for both authenticated users and anonymous users. By default, session state uses cookies to identify users, but you can configure ASP.NET to store session state without using cookies.
					• Database tables. If your site uses an underlying database, like most sites do, you can store state information in its tables. This is a good place to store large volumes of state data that cannot be placed in server memory or on the client computer.
		Planning Globalization and Localization:
			The process by which you make a web application available in multiple languages is called globalization or internationalization. 
			The process by which you make a web application available in a specific language and culture is called localization.
			Managing Browsers for Languages and Regions:
				There is an internationally-recognized set of language codes that specify a culture on the Internet. These codes are in two parts:
					1. The language.
					2. The region. This specifies regional variations within a language and affects spellings and formats.
				The preferred language that users choose is available as the language code in the HTTP header of the user’s browser. This is the value that you respond to, so as to globalize your site. Alternatively, you can provide a control, such as a drop-down list, in which users can choose their preferred language. This is a good example of a user-preference that you can store in the session state.
			Using Resource Files:
				A resource file is a simple dictionary of terms in a given language. For each term in the file, you need to specify a name, a value, and optionally, a comment. The file has an .resx extension. The file name also includes the language code that applies to the resources. Resource files can also have a neutral culture. This means that the file applies to any region in a given language. You should also create corresponding default resource files, without any language code in the file name.
			Using Separate Views:
				When you use separate views to globalize and localize a site, views are more readable, because most of the text and labels remain in the view file. However, you must create view files, which requires you or your team members to be proficient in the target language.
		Planning Accessible Web Applications:
			Users have different requirements depending on their abilities and disabilities.:
				• Users with low vision might use a standard browser, but they might increase text size with a screen magnifier so that they can read the content.
				• Profoundly blind users might use a browser with text-to-speech software or text-to-Braille hardware.
				• Color-blind users might have difficulty if color is used to highlight text.
				• Deaf users might not be able to access audio content.
				• Users with limited dexterity might find it difficult to click small targets.
				• Users with epilepsy might have seizures if presented with flashing content.
			You can ensure that your content is accessible to the broadest range of users by adhering to the following guidelines:
				• Do not rely on color differences to highlight text or other content.
				• Always provide equivalent alternative content for visual and auditory content.
				• Use markup and style sheets to separate content from structure and presentation code. This helps text interpretation software to render content to users without being confused by structural and presentation code. Apply the following best practices to display content on your webpage:
					o Avoid using tables to display the content. You should use tables only to present tabulated content. Tables can be used to render graphics and branding on a webpage, but in an accessible site, use positional style sheets to display content.
					o Avoid using nested tables. In a nested table, a table cell contains another table. These are particularly confusing to text readers because they read each table cell in a sequential order. The user is likely to become disoriented and unable to determine which cell is being read and what it means.
					o Avoid using images that include important text. Text readers cannot render text from within an image file. Instead, use markup to render this text.
	Lesson 2: Designing Models, Controllers and Views
		Designing Models:
			Identifying Model Classes and Properties:
				The use cases, usage scenarios, or user stories that you gathered during the analysis phase of the project should enable you to determine the model classes that you need to create. 
				Each model class has a range of properties.
			Using Diagrams:
				use diagrams to analyze the information your website manages and suggest a physical data model or database design for the site, use these diagrams to plan
			Relationships and Aggregates:
				Numbers at the ends of each link show whether the relationship is one-to-one, one-to-many, or many-to-many.
				Aggregates place further limits on the behavior of model classes and clarify relationships.
			Entity Framework:
				Entity Framework is an Object-Relational Mapping (ORM) framework for .NET Framework-based applications. An ORM framework links database tables and views to classes that a developer can program against, by creating instances or calling methods.
			Design in Agile and Extreme Programming:
				characterized by short design phases in which data models are not completed, simple design, with little detail, is created and developers fill in details as they build code by continuously discussing requirements with users and other stakeholders
		Designing Controllers:
			In an ASP.NET Core MVC web application, controllers are .NET Framework-based classes that inherit from the Microsoft.AspNetCore.Mvc.Controller base class. They implement input logic—this means that they receive input from the user in the form of HTTP requests and select both the correct model and the correct view to use, to formulate a response.
			Identify Controllers and Actions:
				In an ASP.NET Core MVC web application, there is usually one controller for each model class. If you follow the naming convention in your design, you can use the MVC default routing behavior to select the right controller for a request.
				However, for each controller there can be many actions—each action is implemented as a method in the controller and usually returns a view. You often require separate actions for the GET and POST HTTP request verbs. Similar to designing a model, you can identify the actions to write in each controller by examining the use cases you gathered during analysis. 
			Design in Agile and Extreme Programming:
				only make generalized plans for controllers during the design phase of an agile development or extreme programming project
		Designing Views:
			In an ASP.NET Core MVC web application, the user interface is created by building views. There is a many-to-one relationship between MVC controllers and views.
			As you plan views, you should also consider parts of the user interface that appear on all pages. You can place these user interface components in a layout to create a consistent look and feel across pages.
			Some user interface components do not appear on all pages, but are re-used on several pages. By creating a partial view or a view component, you can create a re-usable user interface element that can appear in many locations in this manner, without duplicating code.
			Design in Agile and Extreme Programming:
				do not design many parts of the user interface during the initial phases of agile development or extreme programming projects, design views in close consultation with users during the development phase, create the layout during the early iterations of the development phase of the project
		Information Architecture:
			When the information your web application manages is complex and multi-faceted, it is easy to present objects in a confusing way. Unless you think carefully about the way users expect information to be structured and how they expect to navigate to useful content, you might unintentionally create an unusable web application. During development, when you work with a limited amount of data, this confusion might not become apparent. Then, when real-world data is added to your database at the time of deploying to production, it becomes clear that the web application is confusing. You can avoid this eventuality by planning the information architecture.
			Information architecture is a logical structure for the objects your web application manages. You should design such architecture in a way that users can find content quickly without understanding any technical aspects of your web application.
			Planning a Logical Hierarchy:
				Presenting a Hierarchy in Navigation Controls:
					• Site Menus. Most websites have a main menu that presents the main areas of content. For simple web applications, the main menu might include a small number of static links. For larger web applications, when users click a site menu link, a submenu appears.
					• Tree Views. A tree view is a menu that shows several levels of information hierarchy. Usually, users can expand or collapse objects at each level, to locate the content they require. Tree views are useful for presenting complex hierarchies in navigable structures.
					• Breadcrumb Trails. A breadcrumb trail is a navigation control that shows the user where they are in the web application. Usually a breadcrumb trail shows the current pages and all the parent pages in the hierarchy, with the home page as the top level page. Breadcrumb trails enable you to understand how a page fits in with the information architecture shown in menus and tree views.
				Presenting a Hierarchy in URLs:
					You can increase the usability of your web application by reflecting the information architecture in the URLs, which the users see in the address bar of the web browser. In many web applications, URLs often include long and inscrutable information such as Globally Unique Identifiers (GUIDs) and long query strings with many parameters. Such URLs prevent users from manually formulating the address to an item, and these URLs are difficult to link to a page on your web application. Instead, URLs should be plain and comprehensible, to help users browse through your content.
					You can control the URLs that your ASP.NET web application uses, by configuring the ASP.NET routing engine.

Module 3: Configure Middleware and Services in ASP.NET Core
	Lesson 1: Configuring Middleware
		Application Startup:
			Startup class is used for configuring middleware, which is performed in the Configure method, and for configuring services, which is performed in the ConfigureServices method.
			As part of startup, the ConfigureServices method will be called first if it is present, before calling the Configure method, which may have dependencies on the services that were declared in ConfigureServices.
			The ConfigureServices Method:
				The ConfigureServices method is an optional method where services that will be injected during the application lifespan can be registered. 
				It can also be used to set up various additional configuration options for the application.
				The ConfigureServices method receives a parameter that implements the IServiceCollection interface. This parameter is used to add services to the application.
				Some services are predefined in the framework and will not need to be declared (ILogger, IHostingEnvironment, etc.).
			The Configure Method:
				The Configure method is where the middleware pipeline can be defined. It is possible to add both existing middleware, as well as custom ones. 
				The Configure method supports Dependency Injection, allowing us to inject any services that have been set up and use them inside the middleware logic. 
				In general, the Configure method is where the order of handling requests and responses is determined. 
		Middleware Fundamentals:
			Middleware allows us to add code to manipulate the request and response pipeline in ASP.NET Core applications. 
			Every middleware in the pipeline can interact with the provided request, and write to the response, creating the desired result.
			Middleware is intended to work in a pipeline with each item inspecting the request, performing an action, or a manipulation on the response object and on completing its run, either calling the next middleware in the pipeline or if appropriate for the scenario, short-circuiting the pipeline and sending the finalized response.
			Short-circuiting should be done whenever the handler deems the request ended:
				Scenarios where you may want to short circuit the request:
					• When you match the correct route for the request.
					• When an error has been discovered in the request.
					• When you want to prevent the default handler.
			Every single instance of middleware has access to an HTTPContext parameter from the Microsoft.AspNetCore.Http namespace.
			This parameter allows us to access and manipulate information regarding the current request and response, and to many other details such as the authentication details, the current user, and more:
				• The Request property allows us to investigate details about the request made to the server, including the Query property, which allows us to read the Path and Body properties from the query string as a collection of key-value pairs:
					• The Path property allows us to find the relative path within the application.
					• The Body property is data sent as part of the request body and more. 
				• The Response property allows us to manipulate the response which will be sent back to the client:
					• It allows us to write to it using the WriteAsync method, or alternatively allows us to directly manipulate the response Body, ContentType, or even manually set StatusCode. 
					• Setting the StatusCode allows us to return custom errors.
			Middleware always run in the order they are defined. Additionally, all middleware operate in sequence, and are of several potential types:
				• Use. Middleware added with Use, support a parameter named next, which is a reference to the next middleware in the pipeline. You can short-circuit the pipeline by not invoking next, although all Use middleware should preferably support at least one flow in which they call next.Invoke() to proceed to the next middleware in the pipeline.
				• Run. Unlike Use, the Run middleware will always be the final middleware in the pipeline. It does not support the next parameter, and any middleware appearing after it will never be called.
				• Map. A third variation is the Map middleware. It does not continue with the current pipeline, instead, if the relative path is matched to the Map middleware, it will continue down a new pipeline, which is provided as a delegate to the Map middleware. Note that since Map middleware creates its own pipeline it is not affected by the Run middleware which occurs after it and can support a call to Run at the end of its own pipeline.
			Order of Middleware:
				It is important to remember that middleware runs in the order in which they were added to the pipeline:
					• A Run middleware should always be present at the very end of the pipeline. All middleware defined after the Run middleware will never be run.
					• Every application should only have a single Run middleware inside a specific pipeline. Remember that by using Map you create a new pipeline, which means the Run middleware inside of a Map pipeline is separate from the main pipeline.
					• Whenever multiple middleware share the same condition, it is important to order them to handle the pipeline in the desired way and be mindful of the possibility of the pipeline being short-circuited.
		Working with Static Files:
			Default Static File Serving:
				By calling UseStaticFiles, the application will automatically match relative paths to files inside the wwwroot folder of the application and serve them as the response for the current request. 
				Any file placed outside the wwwroot folder, will not be served and will be ignored by the server.
				Serving Static Files from Outside the wwwroot:
					In some cases, you may want to serve static files from a different folder, rather than wwwroot. To do this, you will need to call the UseStaticFiles middleware with a StaticFileOptions parameter. 
					By setting the FileProvider property, you can assign PhysicalFileProvider, which allows you to choose a file path. All files under this path will then be served as if they were on wwwroot. 
					It is important to note, that this replaces wwwroot. If you wish to host static files on both, you will need to call UseStaticFiles twice. Once without parameters, and the second time with the StaticFileOptions parameter.
			Serving Files under a Separate Relative Path:
				Sometimes you may wish to group the static files under a separate relative path. This can be particularly useful to help avoid conflicts. 
				By doing this, you can ensure that a specific relative path is kept separate from the remaining middleware pathing logic. This can also be done by supplying static files with StaticFileOptions while supplying the RequestPath property. 
				The request path must always begin with a / character and can contain several hierarchies if you so wish. After doing this, to use the static files, the application relative path matches the request path and then it will check for a match in the remaining path later.
			Problems with Serving Static Files:
				Some drawbacks to using UseStaticFiles:
					• All files served by using UseStaticFiles will always be available, there is no way to require authorization for them, and they will be available to all users.
					• Files served from UseStaticFiles will potentially be able to reveal information about the application structure, as all of them and their paths are accessible.
					• Depending on the hosting operating system, these files may or may not be case sensitive.
				Therefore, it is important that any file you wish to protect should not be inside folders you are serving. In particular, any cs code, or cshtml pages (cshtml pages will be covered in Module 5, “Developing Views”) should never be placed under a folder designated for serving to the client.
	Lesson 2: Configuring Services
		Introduction to Dependency Injection:
			Dependency Injection is a technique by which it is possible to facilitate separation of concerns in the application. Normally, to utilize a class in the application, you will need to be able to also manage any classes it depends on. This can cause a situation where whenever you instantiate a class, you may need to pass along to it a large number of parameters, which are then used to instantiate the class and potentially instantiate additional classes it depends on.
			By implementing Dependency Injection, you are able to simplify the process and adopt a more loosely coupled approach. By explicitly defining interfaces you wish to work within the constructor, you are able to specify only the requirements for the current class and you don’t need to worry about any possible sub-dependencies or making any changes should any sub dependency code change.
			By default, ASP.NET Core uses a simple built-in Dependency Injector container, which gives us the support of constructor injection. You can use the IServiceProvider interface to interact with the container, and the most prominent interaction is through the ConfigureServices method of the Startup class, which allows you to register additional services that you wish to run throughout the application.
		Using the Startup Class to Configure Services:
			Injecting Custom Services:
				After creating a service, you will need to register it in the ConfigureServices method of the Startup class. This can be done by using the services parameter, which implements the IServiceCollection interface.
				This method exposes to us several methods that can be used to add the service to the Dependency Injection container. The main methods for adding custom services are AddSingleton, AddTransient, and AddScoped.
			Injecting System Services:
				AddMvc vs AddMvcCore:
					AddMvcCore contains only the very basic components required to run an ASP.NET Core MVC application and lacks several additional services that are used for various additional functionalities, such as Cross Site Scripting handling and the Razor View engine.
				Injecting Services into Other Services:
					To inject a service into another, all you require is to add a constructor within the service, with a parameter type matching the interface you want to instantiate. After this is done, inside the constructor you can perform whatever logic you need from the dependency, including storing it locally inside of the class.
			Services inside ConfigureServices can be registered in any order, unlike middleware, and are resolved by the Dependency Injection container:
				• Keep them in order of dependencies, as it can provide a quick visual reference to the order in which services will be raised.
				• Dependency Injection cannot help us resolve circular references, and they will cause the application to crash.
		Inject Services to Controllers:
			One of the places where you benefit the most from Dependency Injection in an ASP.NET Core MVC application is the controllers. In ASP.NET Core MVC, a controller is a class with one or more methods, which are referred to as actions, designed with the intention of receiving requests and formulating a response based on the request.
			Once you have the controller set up, you will be able to inject services through the constructor. This behaves in a similar way to Dependency Injection into services, where you can add support for a service by explicitly adding a reference to the interface inside the constructor. Inside the constructor, you will be able to save the instance for use in the specific methods.
			The built-in Dependency Injection container in ASP.NET Core does not support multiple constructors. Should the need arise to support multiple constructors alongside Dependency Injection, you will need to use a different dependency injector.
		Service Lifetime:
			correctly preserve service state in an application:
			If you keep all of the services in the application running constantly, you run the risk of creating deadlocks while dealing with external resources as well as potential issues with threading, as each singleton service is locked to one thread since it is first instantiated. 
			On the other hand, keeping all services completely stateless can deprive us of the ability to retain data temporarily and prevent us from being able to manage data correctly.
			AddSingleton:
				Tells the Dependency Injection container to create this service once and then to inject the same instance as long as the application remains running.
				Use it to handle data, which you want to persist inside the memory rather than in an external source (such as a database, file, or another source).
				Requires a service instance to persist throughout the entire application lifespan, helping to keep the same data consistent and persistent.
			AddScoped:
				 When a service is registered by using AddScoped, all instances where it is injected as a result of the current request being processed will receive the same instance. However, for any other request, a new instance will be created.
				 Use it to maintain data throughout the lifetime of a single request without affecting data for other requests.
				 Can be useful for dealing with particular parameters for the specific request such as query string parameters or data which is retrieved due to a specific request, such as user information relating to the user that made the request.
			AddTransient:
				Instantiated individually every time it’s injected, with each instance being completely separate from all others. Due to this, data should never be stored on a transient service.
				Use if for services that will end up being stateless and will not need to store data. 

Module 4: Developing Controllers
	Lesson 1: Writing Controllers and Actions
		Responding to User Requests:
			When an MVC web application receives a request from a web browser, it instantiates a Controller object to respond to the request. Then it determines the action method that should be called in the Controller object.
			The application uses a model binder to determine the values to be passed to the action as parameters. Often, the action creates a new instance of a model class.
			This model object may be passed to a view to display results to the user. Action methods can do many other things such as rendering views and partial views, and redirecting to other websites.
			The User Request:
				MVC relies on convention over configuration. 
		Writing Controller Actions:
			Controller actions are public methods that return an IActionResult interface. Therefore, actions can return objects of classes that implement the IActionResult interface.
			For every action that returns a ViewResult object, there should be a corresponding view template. The view will generate the output sent to the browser.
			Actions usually use ActionResult as the return type. The ActionResult class is an abstract class, and you can use a range of derived classes that inherit from it to return different responses to the web browser.
			The RedirectToActionResult Class:
				Use this action result to return an HTTP 302 response to the browser. This causes the browser to send the request to another action method.
			The RedirectToRouteResult Class:
				Use this action result to redirect the web browser to another route. In the constructor of the RedirectToRouteResult class, you can pass the route.
			The StatusCodeResult Class:
				The StatusCodeResult class provides a way to return an action result with a specific HTTP response status code and description.
				Other common action results include:
					• PartialViewResult. You can use this action result to generate a section of an HTML page rather than a complete HTML page. Partial views can be reused in many views throughout a web application.
					• RedirectResult. You can use this action result to redirect to a specific URL, either within your web application or in an external location.
					• FileContentResult. You can use this action result to return a file from an action method.
		Using Parameters:
			The model binders obtain parameters from a user request and pass them to action methods. The model binders can locate parameters in a posted form, routing values, a query string, or posted files.
			If the model binders find a parameter declared in the action method that matches the name and type of a parameter from the request, the action method is called and the parameter is passed from the request.
			The RouteData Property:
				The RouteData property encapsulates the information about the route. It contains a property named Values, which can be used to get the value of the parameter.
		Using ViewBag and ViewData to Pass Information to Views:
			You can pass a model object from the action method to the view by using the View() method. This method is frequently used to pass information from a controller action to a view. This approach is recommended because it adheres closely to the MVC pattern, in which each view renders the properties found in the model class that it receives from the controller.
			Using the ViewBag Property:
				The ViewBag property is a dynamic object that is part of the base Controller class. Because it is a dynamic object, you can add to it values of any type in the action method. In the view, you can use the ViewBag property to obtain the values added in the action.
			Using the ViewData Property:
				You can pass extra data to views by using the ViewData property. This feature is available for developers who prefer to use dictionary objects. In fact, ViewBag is a dynamic wrapper above the ViewData dictionary. This means that you could assign a value in a controller action by using ViewBag and read the same value in the view by using ViewData.
	Lesson 2: Configuring Routes
		The ASP.NET Core Routing Engine:
			Routing is used to determine the controller class that should handle a request. Routing is also used to decide which action should be called and what parameters need to be passed to that action.
			The following steps occur when a request is received from a web browser:
				1. A controller is instantiated to respond to the request. Routing is used to determine the right controller class to use.
				2. The request URL is examined. Routing is used to determine the action that needs to be called in the controller object.
				3. Model binding is used to determine the values that should be passed to the action as parameters. The model binding mechanism consults the routing entries to determine if any segments of the URL should be passed as parameters. The model binding mechanism can also pass parameters from a posted form, from the URL query string, or from uploaded files.
				4. The action is then invoked. Often, the action creates a new instance of a model class, perhaps by querying the database with the parameters passed to it by the invoker. This model object is passed to a view to render the results and send them to the response.
			Routing does not operate on the protocol, server, domain, or port number of a URL, but it operates only on the directories and file name in the relative URL.
			In ASP.NET Core, routes are used for two purposes:
				• To parse the URLs requested by browsers. This analysis ensures that requests are forwarded to the right controllers and actions. These are called incoming URLs.
				• To formulate URLs in webpage links and other elements. When you use helpers such as Html.ActionLink in the MVC views, the helpers construct URLs according to the routes in the routing table. These are called outgoing URLs.
			ASP.NET Core MVC allows us to configure routes in two different ways:
				• Configure routes by using convention-based routing. In this case, the routes are configured in the Startup.cs file and have an impact on the entire application.
				• Configure routes by using attributes. As the name implies, attribute routing uses attributes to define routes.
			You can combine convention-based routing and attribute-based routing in the same MVC application.
			The Default Route:
				UseMvcWithDefaultRoute adds MVC to the IApplicationBuilder request execution pipeline with a default route named default and the {controller=Home}/{action=Index}/{id?} template.
				The default route is simple and logical. It works well with many web applications. The default route examines the first three segments of the URL. Each segment is delimited by a forward slash:
					• The first segment is interpreted as the name of the controller. The routing engine forwards the request to this controller. If a first segment is not specified, the default route forwards the request to a controller called HomeController.
					• The second segment is interpreted as the name of the action. The routing engine forwards the request to this action. If a second segment is not specified, the default route forwards the request to an action called Index.
					• The third segment is interpreted as an id value, which is passed to the action as a parameter. This parameter is optional, so if a third segment is not specified, no default value is passed to the action.
		Discussion: Why Add Routes?
			• To make URLs easier for site visitors to understand:
				If you use Globally Unique Identifiers (GUIDs) in the database, the ID segment of the URL can be long and difficult to remember. Ideally, you should consider what users know and create routes that accept that information.
			• To improve search engine rankings:
				Search engines do not prioritize webpages that have GUIDs or long query text in the URL. Some web bots do not even crawl such links. In addition, some search engines boost a page’s ranking when one or more of its keywords appear in the URL. User-friendly URLs are therefore a critical tool in Search Engine Optimization.
		What Is Search Engine Optimization?
			Most users find web applications by using search engines. Users tend to visit the links that appear at the top of the first page of search engine results more frequently than those that appear at the bottom of the page or on the later pages.
			SEO ensures that your web application appears higher at the top of the search engine results. SEO ensures that more people visit your web application.
			Search engines examine the content of your web application by crawling it with a program called a web bot. If you understand the priorities that web bot and search engine indexes use to order search results, you can create a web application that conforms to those priorities and thereby appears high in search engine results.
			Information architecture is a subject that is closely related to SEO. This is because both information architecture and SEO are relevant to the structure, hierarchy, and accessibility of the objects in your web application.
		Configuring Routes by Using Convention-Based Routing:
			Define convention-based routing in startup class.
			Before adding a route, it is important to understand the properties of a route. This is to ensure that these properties match the URL segments correctly and pass the requests and parameters to the right location.
			Properties of a Route:
				The properties of a route include name and template, both of which are string properties.
				The name property assigns a name to a route. It is not involved in matching or request forwarding.
				The template property is a URL pattern that is compared to a request URL to determine if the route should be used. You can use segment variables to match a part of the URL. Use braces to specify a segment variable.
			Adding Segments to a Template:
				T model binding mechanism will map the value entered in the URL to the value of the param parameter in the action.
			The defaults Property:
				This property can assign default values to the segment variables in the URL pattern. Default values are used for segment variables when the request does not specify them.
				You can set the default values of a route directly in the template property.
			The constraints Property:
				The constraints property enables you to specify a regular expression for each segment variable. The route will match a request only if all the segment variables match the regular expressions that you specify.
				It is possible to specify route constraints directly in the template property.
			The dataTokens Property:
				The dataTokens property enables you to specify the data tokens for the route. Each data token contains a name and a value.
			Order of Evaluation of Routes:
				Routes are evaluated in the order in which they are added. If a route matches a request URL, it is used. If a route does not match, it is ignored and the next route is evaluated. For this reason, you should add routes in the appropriate order. You should add the most specific routes first, such as the routes that have no segment variables and no default values. Also, routes with constraints should be added before routes without constraints.
		Using Routes to Pass Parameters:
			The routing engine separates the relative URL in a request into one or more segments. Each forward slash delimits one segment from the next.
			If you want one of the segments to specify the controller’s name, you can use the {controller} segment variable. This variable is interpreted as the controller to be instantiated. 
			Alternatively, to fix a route to a single controller, you can specify a value for the controller variable in the defaults property.
			If you want one of the segments to specify the action method, you can use the {action} segment variable. The action invoker always interprets this variable as the action to call.
			To fix a route to a single action, you can specify a value for the action variable in the defaults property.
			Segment variables or default values with other names have no special meaning to MVC and are passed to actions. You can access the values of these variables by using the RouteData.Values collection or by using model binding to pass values to action parameters.
			Using the RouteData.Values Collection:
				In the action method, you can access the values of any segment variable by using the RouteData.Values collection.
				In a similar way, you can use the RouteData.Values collection to access other segment variables.
			Using Model Binding to Obtain Parameters:
				The default MVC action invoker passes the appropriate parameters to actions. For this it examines the definition of the action to which it must pass parameters. The model binding mechanism searches for values in the request, which can be passed as parameters by name.
				One of the locations that it searches is the RouteData.Values collection. Therefore, you can use the default model binder to pass parameters to action. If the name of an action method parameter matches the name of a route segment variable, the model binder passes the parameter automatically.
				You can set several segments in the template of a route, and all of them can be matched to parameters of an action.
		Configuring Routes by Using Attributes:
			Attribute-based routing allows you to configure routes by using attributes. The primary advantage of attribute-based routing is that it allows you to define your routes in the same file as the controller that the routes refer to.
			You can use the Route attribute to specify the route for a specific action.
			Pass Parameters Using Attribute Routing:
				You can use attribute routing to pass parameters to an action.
			Optional Parameters:
				You can configure a segment variable to be optional by using a question mark.
			Annotate a Controller Class with a Route Attribute:
				Typically, all the routes in a controller class start with the same prefix.
				You can set a common prefix for an entire controller class by using the [Route] attribute above the controller class.
			Combine Convention-Based Routing with Attribute-Based Routing:
				Convention-based routing and attribute-based routing can be used in the same application.
			Attribute Routing with Http[Verb] Attributes:
				In attribute routing you can use the Http[Verb] attributes instead of using the Route attribute. The Http[Verb] attributes should be used when you want to limit access to the action to a specific HTTP verb.
				To make the attribute routing less repetitive, you can apply the Route attribute to a controller, and apply the Http[Verb] attribute to an action. In such cases, the Route attribute is used to set the common prefix for an entire controller class, and its template is prepended to the template of the action.
	Lesson 3: Writing Action Filters
		Cross cutting concerns: Types of requirements that are relevant to many parts of the application and cut across logical boundaries.
		Filters are MVC classes that you can use to manage cross-cutting concerns in your web application. You can apply a filter to a controller action by annotating the action method with the appropriate attribute. You can also apply a filter to every action in a controller by annotating the controller class with the attribute. 
		Filters run after an action is chosen to run. They run within a filter pipeline. The stage in the filter pipeline in which the filter is run is determined by the type of the filter.
		Filter Types:
			• Authorization filters. Authorization filters run before any other filter and before the code in the action method. They are used to check a user’s access rights for the action. 
			• Resource Filters. Resource filters appear in the filter pipeline after the authorization filters and before any other filters. They can be used for performance reasons. Resource filters implement either the IResourceFilter interface or the IAsyncResourceFilter interface. 
			• Action Filters. Action filters run before and after the code in the action method. You can use action filters to manipulate the parameters that an action receives. They can also be used to manipulate the result returned from an action. Action filters implement either the IActionFilter interface or the IAsyncActionFilter interface. 
			• Exception Filters. Exception filters run only if an action method or another filter throws an exception. These filter classes are used to handle errors. Exception filters implement either the IExceptionFilter interface or the IAsyncExceptionFilter interface. 
			• Result Filters. Result filters run before and after an action result is run. Result filters implement either the IResultFilter interface or the IAsyncResultFilter interface. 
			TODO: implement some examples
		You can create your own filter classes or use existing filter classes: 
			• The ResponseCacheAttribute filter class. Annotating an action with the ResponseCache attribute will cache the response to the client. The ResponseCache attribute contains several properties.
			• The AllowAnonymousAttribute filter class. Annotating an action with the AllowAnonymous attribute will allow users to access an action without logging in.
			• The ValidateAntiForgeryTokenAttribute filter class. Annotating an action with the ValidateAntiForgeryToken attribute will help prevent cross-site request forgery attacks.
	Creating and Using Action Filters:
		You can create custom filters by implementing the IActionFilter interface or the IResultFilter interface. However, the ActionFilterAttribute base class implements both the IActionFilter and IResultFilter interfaces for you. 
		By deriving your filter from the ActionFilterAttribute class, you can create a single filter that can run code both before and after an action is run, and both before and after the result is returned. 
		Using a Custom Action Filter:
			After you create a custom action filter, you can apply it to any action method or class in your web application by annotating the method or class with the action filter name.
		Action Filter Context:
			The OnActionExecuting, OnActionExecuted, OnResultExecuting and OnResultExecuted event handlers take a context object as a parameter. The context object inherits from the FilterContext class. The FilterContext class contains properties that you can use to get additional information regarding the action and the result.
		Using Dependency Injection in Filters:
			In case you need your filters to get dependencies using their constructor, you can add filters by type. To add a filter by type, you can use the ServiceFilter attribute. 

Module 5: Developing Views
	Lesson 1: Creating Views with Razor Syntax
		When an MVC web application receives a request, a controller action processes the request. Often, the controller action passes a model object to a view. The view builds the data by inserting the properties from the model object and other sources into the HTML markup. Then, the view renders the completed data to the browser. The browser displays the data it receives from the view. 
		Adding Views:
			In an MVC application, controllers handle all the application logic. Each controller contains one or more action methods, and an action handles the incoming web requests. 
			Views handle the presentation logic and should not contain any application logic. Views are used to render a response to the browser.
			In an MVC application, there is usually one controller for every model class. There might be some controllers that do not correspond to any model classes. However, each controller can have multiple views.
			By convention, all the views in an MVC application reside within the top-level Views folder. Within this folder, there is a folder for each controller that is used in the application.
			How to Create a View File:
				Property							Description
				View name							This is the name of the view. The view file name is this name with 
													the appropriate extension added. The name you choose should match the name	
													returned by the corresponding controller action. If the action controller does 
													not specify the name of the view to use, MVC assumes the name of the view 
													matches the name of the controller action. 
				Model class							If you create a strongly-typed view, you need to specify the model class to 
													bind to the view. Visual Studio will use this class when it formulates 
													IntelliSense prompts and checks for compile-time errors. 
				Template							A template is a basic view that Visual Studio can use to create the view. If 
													you specify a model class for the view, Visual Studio can create simple 
													templates for Create, Edit, Details, Delete, and List views. If you don’t 
													specify the model class for the view, you may choose to build a view from the 
													Empty (without model) scaffold template. 
				Reference script libraries			When you select this check box in MVC 5, references to common client-side 
													script files are included in the project. In ASP.NET Core MVC, this check box 
													has no effect. 
				Create as a partial view			A partial view is a section of Razor code that you can re-use in multiple in 
													your application. 
				Use a layout page					A layout page can be used to impose a standard layout and branding on many 
													pages within the web application.
				Within a controller, you can use the View method to create a ViewResult object. The view renders data to the browser. 
		Differentiating Server-Side Code from HTML:
			The Razor view engine interprets view files and runs any server-side code contained in the view files. To do this, the Razor view engine must distinguish the server-side code from the HTML content. 
			The HTML content should be sent to the browser unchanged. The Razor view engine looks for the @ symbol to identify the server-side code. 
			A section of code marked with the @ symbol is referred to as a Razor code expression. In the Razor syntax, you mark the start of a Razor code expression with the @ symbol. However, there is no expression ending symbol. Instead, Razor infers the end of a code expression by using a fairly sophisticated parsing engine. 
			Writing a for Loop by Using Razor:
				You can write for loops by using the Razor syntax if you know exactly how many times you want to loop.
			Razor has sophisticated logic to distinguish code from content and often the @ symbol is all that is required. However, occasionally, you might find that an error occurs because Razor misinterprets content as code. To fix such errors, you can use the @: delimiter, which explicitly declares a line as content and not as code.
			Modifying the Interpretation of Code and Content:
				Sometimes, you might need to modify the logic that Razor uses to interpret code expressions.
			If you want to declare several lines as content, use the <text> tag instead of the @: delimiter. Razor removes the <text> and </text> tags before returning the content to the browser.
		Features of the Razor Syntax:
			Razor includes many useful features that you can use to control the way in which ASP.NET Core MVC renders your view as HTML. These features include the following: 
				• Razor comments
				• Implicit code expressions and parentheses	
				• Razor code blocks and conditions
				• Razor loops
			Razor Comments:
				You can declare a Razor comment by using the @* delimiter.
			Implicit Code Expressions and Parentheses:
				Razor uses implicit code expressions to determine parts of a line that are server-side code. Usually, implicit code expressions render the HTML you want, but occasionally, you might find that Razor interprets an expression as HTML when it should be run as server-side code.
				You can control and alter behavior by using parentheses to enclose the expression so that Razor can evaluate the expression. 
			Razor Code Blocks and Conditions:
				If you want to write multiple lines of server-side code without prefacing every line with the @ symbol, you can use a Razor code block. 
				Razor includes code blocks that run conditional statements. 
			Razor Loops:
				Razor also includes code blocks that loop through collections. You can loop through all the objects in an enumerable collection by using the foreach code block.
				Razor loops are useful for creating index views, which display many objects of a particular model class. To implement an index view, a controller action passes an enumerable collection of objects to the view.
		Alternative View Engines:
			A view engine is a component of the MVC framework that is responsible for locating view files, running the server-side code they contain, and rendering HTML that the browser can display to the user. 
			The Razor view engine is the default view engine in MVC and is highly flexible and efficient. Furthermore, it is easy to learn if someone has experience in HTML and C#. Also, Visual Studio provides good IntelliSense feedback for Razor code. 
			Despite its many advantages, some developers might prefer to use a different a view engine. The reasons can include: 
				• View Location Logic. Razor assumes that all views are located in the Views top-level folder. Within this, Razor assumes that each controller has a separate folder. Razor also looks for some views in the Shared subfolder. These views can be used by more than one controller. If you do not want to use this view location logic, you can use an alternate view engine. 
				• View Syntax. Each view engine uses a different syntax in view files. Other view engines can use different delimiters. You might prefer one syntax over another based on the technologies that you or your team previously worked with. 
			You can create your own view engine to implement custom logic for locating views or to implement custom syntax for view files. To create a custom view engine, you should create a class that implements the IViewEngine interface. 
			The IViewEngine interface defines the contract for a view engine. It contains a FindView method, which finds a specified view by using a specified action context. The FindView method returns a ViewEngineResult object. 
			Custom view engines are rarely created because they require the developers to write sophisticated string-parsing code. The default view engine, Razor, is powerful and flexible. The default logic for finding view files is also simple and rarely needs modification. 
		Dependency Injection into Views:
			With Razor views, you can use dependency injection to inject services directly into views. By injecting a service directly into a view, you can call it whenever you wish by using the Razor logic, potentially gaining immediate access to the functionality of the service, without needing to rely on a controller. 
			A very common use case where this is useful is localization services. As part of localization requirements, you will often want to localize large sections of text in a view, and it will be very awkward to have the model manage the text for specific segments. By using a localization service in a view, you can easily maintain localization in the view, without having to create complex controller logic. 
			On the opposite end of the spectrum, you should also avoid using services on a view with the intention to present data. By using services to retrieve data, you are ignoring the principles of MVC and creating a direct dependency between our view and the data. 
			Mostly, you will have to use only stateless services in views, and mainly services designed to perform transformations rather than create data. For instance, a service that performs formatting on dates for an application is logical to keep in views because this kind of service will be repeated often, and result in a lot of view-related logic ending in a controller. On the other hand, a service that retrieves data from a database is a bad choice to use in views because retrieving data off a database is a costly operation. If this operation is repeated multiple times, the performance of a view will suffer. 
			Dependency injection into views is performed by using directives, which are special keywords that you can use after the @ symbol to perform specific actions. To inject services, you will use the inject directive. 
			Inject Directive:
				@inject <type> <instance name>
				The type is the interface of the service that you wish to inject. To find a type, you will have to use the full path for the namespace of the interface. 
				The instance name is an alias you create to refer to a service. Throughout a view, calling the alias will grant you access to the methods exposed by an interface. 
				After you inject an interface, you can access the service at any time inside the code contained by the @ symbol. 
	Lesson 2: Using HTML Helpers and Tag Helpers
		Introduction to HTML Helpers and Tag Helpers:
			In general, views are composed of correctly formed HTML elements. However, because the views are not static (like normal HTML) and can even run code at creation time, you might sometimes need HTML behaviors. But creating the entire behavior manually could be time consuming.
			To resolve this issue, and allow you to develop applications faster with less repetition on repeated logic and less dependency on JavaScript, you can use HTML helpers and tag helpers. 
			HTML helpers are C# methods that you can use to create common and complex HTML elements, such as links and various input controls and forms, and manage the elements and their relationship to controls. This is done by using Razor syntax. 
			Tag helpers are an alternative to HTML helpers. Tag helpers help you add properties to existing HTML elements and custom html elements, instead of adding Razer code. This will help you keep your html more consistent. 
			As a result, code with tag helpers looks a lot more like HTML code than HTML helpers. Regardless of this, the end result is consistent between the two. To use tag helpers, use the @addTagHelper directive, which is used to load tag helper classes. 
		Using HTML Action Helpers:
			HTML helpers are simple methods that, in most cases, return a string. This string is a small section of HTML that the view engine inserts into the view file to generate a completed webpage. 
			It is not mandatory to use HTML helpers and you have the flexibility to create views that render any type of HTML content without using any HTML helpers. 
			You can use the Html.ActionLink and Url.Action helpers to render HTML that calls controller actions. 
			The Html.ActionLink Helper:
				You can use the Html.ActionLink helper to render a link to an action. The helper returns an <a> element with the correct href parameter value. 
			Using Html.ActionLink to Call Actions in Other Controllers:
				You can also use the @Html.ActionLink helper to call actions in other controllers.
			Passing Parameters to Html.ActionLink:
				You can pass parameters to the Html.ActionLink helper.
			The Url.Action Helper:
				The code in a view can contain the Url.Action helper. You can use the Url.Action helper to render a URL without the enclosing <a> element. To render the URL, the routing engine is used. 
		Using Tag Helpers:
			ASP.NET Core MVC includes several predefined tag helpers. These predefined tag helpers are located in the Microsoft.AspNetCore.Mvc.TagHelpers namespace. To use them, you need to add the @addTagHelper directive to the view to define the tag helpers that this view will use. Alternatively, you can add the @addTagHelper directive to a _ViewImports.cshtml file that is located under the Views folder, so that it is available to all the views. 
			Calling Actions in Other Controllers:
				You can use tag helpers to call actions in other controllers.
			Passing Parameters to an Action:
				In case you want to pass parameters to an action, you can use the asp-route-{value} attribute. The {value} placeholder should be identical to the name of the route parameter. 
			Using the _ViewImports.cshtml File:
				If you add the @addTagHelper directive to a view, the tag helpers are available only in this view. In case you want that, the tag helpers will be available for all views in the Views folder and its sub directories. 
				You can add the @addTagHelper directive to a _ViewImports.cshtml file. The _ViewImports.cshtml file should be located in the Views folder.
				All tag helpers implement the ITagHelper interface, and many tag helpers inherit from the TagHelper class. 
					TODO: implement InputTagHelper examples
					TODO: implement ValidationMessageTagHelper
					TODO: implement custom TagHelper
	Lesson 3: Reusing Code in Views
		A better practice is to use a partial view. A partial view renders only a section of HTML content, which you can then insert into several other views at run time. Because a partial view is a single file that is reused in several locations in a web application, if you implement a change in one location, the change is updated in other locations. Partial views increase the manageability of MVC web applications and facilitate a consistent presentation of content throughout a web application. 
		A view component is another way to reuse code in views. View components are similar to partial views. However, view components have many benefits when compared to partial views. They can be treated as mini controllers because they render a chunk of data instead of rendering the whole response. A view component consists of a class and a view. The class is usually inherited from the ViewComponent class and the view uses Razor to call the methods in the class. 
		Creating Partial Views:
			Creating and Naming Partial Views:
				By convention, the names of partial views are prefixed with an underscore. This convention is optional but is often helpful to distinguish partial views from regular views in Solution Explorer. 
				Partial views are often created inside the /Views/Shared folder in your site. Views in the Shared folder are accessible to many controllers.
			Strongly-typed and Dynamic Partial Views:
				You can create strongly-typed partial views if you are sure that the views will always display the same model class. Visual Studio provides the most informative IntelliSense feedback and error-checking for strongly-typed partial views. A strongly-typed view has a declaration of the @model directive at the top of the file. 
				You can create a dynamic partial view if you are not sure that the partial view will always display the same model class. You can create dynamic partial views by omitting the @model directive declaration. 
		Using Partial Views:
			You can use the Html.PartialAsync method to render a partial view within another view file. MVC inserts the HTML content that the partial view renders into the parent view and then returns the complete webpage to the browser.
			Passing Model Object to Partial Views:
				Models can be passed from action methods to views. A parent view will share the same model object with the partial view. This is a good way of sharing data between the controller action, view, and the partial view. 
			Using ViewBag in Partial Views:
				You can use the ViewBag and ViewData collections to pass data between an action method and a view. A parent view will share the same ViewBag or ViewData with the partial view. This is a good way to share data between a controller action, a view, and a partial view. 
		Creating View Components:
			You can use view components to render identical or similar HTML content in different locations of your web application. In this manner, view components are similar to partial views. However, view components are much more powerful than partial views. 
			A view component can be treated as a mini-controller which means that you can use it to render a small chunk of markup instead of a whole response. 
			A view component consists of two parts:
				• A class. The class should be public and non-abstract. This class is usually derived from the ViewComponent base class. It can be annotated with the ViewComponentAttribute attribute. You can place the view component class in any folder of the project. However, a good practice is to place it in a folder named ViewComponents. The class should have a method called InvokeAsync, which defines its logic. The ViewComponent base class has a method named View, which returns a ViewViewComponentResult object. The View method can get a parameter that specifies the name of the partial view that needs to be rendered. 
				• A view. The view will typically be located in a folder under the Views\Shared\Components folder. This is because view components are usually not specific to a controller. The name of the folder should be the same as the name of the view component class. However, if the name of the class has the ViewComponent suffix, then the folder name should not include the ViewComponent suffix. 
		Using View Components:
			You can include a view component in a view by using the @Component.InvokeAsync method. The @Component.InvokeAsync method gets the name of the view component class as a parameter. In case the view component class name ends with the suffix ViewComponent, then the parameter will be the name of the view component class without the suffix.
			Invoking a View Component using a Tag Helper:
				In addition to invoking a view component from a view by using the @Component.InvokeAsync method, it is also possible to invoke a view component from a view by using a tag helper. 
				To invoke a view component by using a tag helper, you should use a vc element followed by a colon and the name of the view component. To use a view component as a tag helper, you must use the @addTagHelper directive, and pass to it the name of the assembly in which the tag helper is declared.
			Invoking a View Component from a Controller:
				To invoke a view component from an action, you can return from the action a ViewComponentResult object. To return a ViewComponentResult object, you can use the ViewComponent method, and pass to it the name of the view component.
		Invoking View Components with Parameters:
			The InvokeAsync method can take any number of parameters. Those parameters will be passed when the view component is invoked in a view or in a controller. 
			Passing Parameters to a View Component using a Tag Helper:
				It is possible to pass parameters to a view component from a view by using a tag helper. To pass parameters from a view to a view component by using a tag helper, you can add attributes to the vc element. 
			Passing Parameters to a View Component from a Controller:
				It is possible to pass parameters to a view component from a controller by using the ViewComponent method. The second parameter of the ViewComponent method represents an object with properties representing arguments to be passed to the InvokeAsync method of the view component.

Module 6: Developing Models
	Lesson 1: Creating MVC Models
		Developing Models
			create models in a folder named Models, because MVC relies on convention
			create public properties for each property in the model and include the data type
			create read-only properties by omitting the set; keyword (or private set)
		Passing Models to Views:
			When an ASP.NET Core MVC web application receives a request from a web browser, it instantiates a controller object to respond to the request. Next, the ASP.NET Core MVC web application determines the action method that should be called on the controller object. Often, the action creates a new instance of a model class, which can be passed to a view to display results to the user.
			For large applications, it is recommended to use ViewModels to separate the presentation from the domain.
			You can use the Razor @model operator to specify the strongly typed domain model type of the view. To access the value of a property in the domain object, use @Model.PropertyName.
			Passing a Collection of Items:
				Some views display several instances of a model class. In such cases, the controller passes a list of model objects to the view, instead of a single model object. You usually loop through the items in the list by using a Razor foreach loop.
			Passing a Model to a Different View:
				You can use another version of the Controller.View method to call a view whose name is different from the name of the action. When you use this version, you need to explicitly specify the name of the view.
		Binding Views to Model Classes and Displaying Data:
			The views that are designed to display properties from a specific model class are called strongly typed views. You can bind such views to the model class whose properties they are displaying, which enables you to get IntelliSense feedback as you write the Razor code. 
			Other views might display properties from different model classes in different cases, or may not use a model class at all. These types of views are called dynamic views and you cannot bind them to a specific model class.
			Strongly Typed Views:
				A strongly typed view includes a declaration of the model class. When you declare the model class in the view, Microsoft Visual Studio provides additional IntelliSense feedback and error-checking as you write the code because it can check the properties of the model class. This also simplifies troubleshooting run-time errors. Therefore, Whenever you can, create strongly typed views because this extra IntelliSense and error-checking features can help you reduce coding errors. A strongly typed view only works with an object of the model class in the declaration.
				In the view files, you can access properties of the model object by using the Model keyword. To access a property in a model, use: @Model.PropertyName.
			Using Dynamic Views:
				Sometimes, you might want to create a view that can display more than one model class. Sometimes, the controller action does not pass any model class to the view. In such cases, you can create a dynamic view. A dynamic view does not include the @model declaration at the top of the page.
				When you create dynamic views, Visual Studio does not provide much IntelliSense feedback and error checking because it cannot check the model class properties to verify the code. In such scenarios, it is your responsibility to ensure that you access only the properties that exist. To access a property that may or may not exist, check the property for null before you use it.
				There are several HTML helpers and tag helpers that you can use to simplify working with models in views.
			Using the @Html.EditorForModel HTML Helper:
				The @Html.EditorForModel HTML helper returns an HTML input element for each property in the model.
			Using the @Html.BeginForm HTML Helper:
				@Html.BeginForm HTML helper is considered as a better approach.
		What Are Model Binders?
			A model binder is a component of an ASP.NET Core MVC application that creates an instance of a model class, based on the data sent in the request. ASP.NET Core MVC includes a default model binder that meets the needs of most web applications. In addition, you may choose to create a custom model binder for advanced situations.
			What Does a Model Binder Do?
				A model binder ensures that the correct data is sent to the parameters in a controller action method. This enables ASP.NET Core MVC to create instances of model classes that satisfy the user’s request.
			How the Default Model Binder Passes Parameters:
				In a default ASP.NET Core MVC application, there is only one model binder to use – the default model binder. The default model binder passes parameters by using the following logic:
					1. The binder examines the definition of the action to which it must pass parameters.
					2. The binder searches for values in the request that can be passed as parameters. The binder searches for values in the following locations, in order:
						o Form Values. If the user fills out a form and clicks a submit button, you can find parameters in the Request.Form collection.
						o Route Values. Depending on the routes that you have defined in your web application, the model binder may be able to identify parameters in the URL.
						o Query Strings. If the user request includes named parameters after a question mark, you can find these parameters in the Request.QueryString collection.
						o Files. If the user request includes uploaded files, these can be used as parameters.
				Notice that if there are form values and route values in the request, form values take precedence. Query string values are only used if there are no form values and no route values available as parameters.
			Handling Form Values:
				You can overload an action method. You can create two action methods with the same name that differ by the parameters they get.
				A common scenario is an action that loads a form and an action that saves a form, both of them have the same name. The ASP.NET Core MVC runtime knows which method should be invoked by using the HttpPostAttribute attribute and HttpGetAttribute attribute. You can annotate an action with the HttpGetAttribute attribute, which indicates that it is a GET operation that is usually used to load a form. You can also annotate an action with the HttpPostAttribute attribute, which indicates that it is a POST operation that is usually used to save a form.
		Adding CRUD Operations to Controllers:
			Often, a controller has CRUD operations. The create operations are used to create or add new items. The read operations are used to read, retrieve, search, or view existing entries. The update operations are used to update or edit existing entries. The delete operations are used to delete existing entries
	Lesson 2: Working with Forms
		Using Display and Edit Data Annotations:
			A model class usually contains several properties each of which usually includes:
				• The name of the property. 
				• The data type of the property. 
				• The access levels of the property to indicate read and write access. 
			In addition, you can supply more metadata to describe the properties of models in ASP.NET Core MVC. The ASP.NET Core MVC runtime uses this metadata to determine how to render each property in views for displaying and editing. These attributes are called display and edit annotations. 
			Data annotations that are mentioned above are included in the System.ComponentModel.DataAnnotations namespace.
		Using Display Helpers:
			Html.DisplayNameFor renders the name of a model class property. Html.DisplayFor renders the value of a model class property. Both these helpers examine the definition of the property in the model class, including the data display annotations, to ensure that they render the most appropriate HTML.
			The Html.DisplayNameFor Helper:
				You can use the Html.DisplayNameFor helper to render the display name for a property from the model class. If your view is strongly typed, Visual Studio checks whether the model class contains a property with the correct name as you type the code. Otherwise, you must ensure that you use a property that exists or verify that it is not null before you use it. You specify the property of the model class to the Html.DisplayNameFor HTML helper by using a lambda expression. 
				The text rendered by the Html.DisplayNameFor helper depends on the model class. If you used a DisplayAttribute attribute to give a more descriptive name to a property, Html.DisplayNameFor will use the value of the Name parameter. Otherwise, it will render the name of the property.
			The Html.DisplayFor Helper:
				The Html.DisplayFor helper considers any display annotations that you specify in the model class, and then renders the value of the property. It generates different HTML markups depending on the data type of the property that is being rendered.
		Using Editor Helpers:
			Within HTML forms, there are many HTML input controls that you can use to gather data from users. In Razor views, the Html.LabelFor and Html.EditorFor HTML helpers make it easy to render the most appropriate input controls for the properties of a model class. 
			Control					Example													Description
			Text box				<input type="text" name="Title" />						Renders a single-line text box in which the user can enter a 
																							string. The name attribute is used to identify the entered 
																							value in the query string or to send form data by using the 
																							POST method. 
			Multi-line text box		<textarea name="Description" rows="20" cols="80" />		Renders a multi-line text box in which the user can enter 
																							longer strings. 
		Check box					<input type="checkbox" name="ContactMe" />				Renders a check box to submit a boolean value.
		The Html.LabelFor HTML Helper:
			The Html.LabelFor helper is similar to the Html.DisplayNameFor helper because it renders the name of the property that you specify, considering the DisplayAttribute attribute if it is specified in the model class. However, unlike the Html.DisplayNameFor helper, the Html.LabelFor helper renders a <label> element.
		The Html.EditorFor HTML Helper:
			The Html.EditorFor helper renders the most appropriate HTML input elements and other form controls for each property data type in a model class.
			Control					Model Class Property									HTML Rendered by EditorFor()
			Text box				public string Title { get; set; }						<input type="text" name="Title" />
			Multi-line text box		[DataType(DataType.MultilineText)]						<textarea name="Description" rows="20" cols="80" />
									public string Description { get; set;}
			Check box				public bool ContactMe { get; set; }						<input type="checkbox" name="ContactMe" />
			If the action passes an existing model object to the view, the Html.EditorFor helper also populates each control with the current values of each property. 
		The LabelTagHelper and InputTagHelper Tag Helpers:
			Tag helpers are an alternative to HTML helpers. Similar to HTML helpers, the role of tag helpers is to embed server-side code inside a view. However, it is much easier for non-programmers, such as designers, to understand them because tag helpers look like regular HTML elements. Tag helpers are implemented by using programming languages such as C#. 
			The LabelTagHelper tag helper is an alternative to the Html.LabelFor HTML helper. It generates a <label> element for a property in a model and you can use it by adding an asp-for attribute to a <label> element. The value of the generated for HTML attribute matches the name of the property in the model class. The content of the <label> element matches the Name property of the DisplayAttribute attribute that is specified in the model class. 
			The InputTagHelper tag helper is an alternative to the Html.EditorFor HTML helper. It generates an <input> element for a property of a model and you can use it by adding an asp-for attribute to a <input> element. The InputTagHelper tag helper adds an id and a name based on the property name specified in the asp-for attribute. Similar to the Html.Editor HTML helper, the type of the input element is determined based on the .NET type of the property of the model class. 
		Using Form Helpers:
			To accept user input, you can provide a form on your webpage. A typical form consists of a set of labels and input controls. The labels indicate to the user the property for which they should provide a value. The input controls enable the user to enter a value. Input controls can be text boxes, check boxes, radio buttons, file selectors, drop-down lists, or other types of control. There is usually a submit button and cancel button on forms. 
			The Html.BeginForm HTML Helper:
				To build a form in HTML, you must start with a <form> element on the HTML webpage and all labels and input controls must be within the <form> element. In an MVC view, you can use the Html.BeginForm HTML helper to render this element and set the controller action to which the form sends data. 
				You can also specify the HTTP method that the form uses to submit data. If the form uses the POST method, which is the default, the browser sends form values to the web server in the body of the form. If the form uses the GET method, the browser sends form values to the web server in the query string in the URL. 
				In the rendered HTML, the <form> element must be closed with a </form> tag. In Razor views, you can ensure that the form element is closed with a @using code block. Razor renders the </form> tag at the closing bracket of the code block.
			The FormTagHelper Tag Helper:
				The FormTagHelper tag helper is an alternative to the Html.BeginForm HTML helper. It generates a <form> HTML element. You can add to the FormTagHelper helper an asp-controller attribute to bind it to a specific controller. You also can add to the FormTagHelper helper an asp-action attribute to bind it to a specific action. Although the default form method value is post, you can specify another form method.
	Lesson 3: Validating MVC Application
		Validating User Input with Data Annotations:
			You can use data annotations in MVC models to set validation criteria for user input. Input validation is the process by which MVC checks data provided by a user or a web request to ensure that it is in the right format.
			The Required, Range, StringLength, and RegularExpression annotations implement input validation in MVC. If users do not enter data that satisfies the criteria specified for each property, the view displays a standard error message that prompts the user to enter the correct data. 
			To specify the error message that the user sees when data is not valid, use the ErrorMessage property on the validation data annotations.
		Using ModelState.IsValid in the Controller:
			A controller can use the ModelState.IsValid property to check whether the user has submitted valid data. If the data is valid, value of ModelState.IsValid will be true, otherwise it will be false.
		Using Validation Helpers:
			When you request information from users, you often want the users to enter the data in a specific format so that it can be used further in the web application. In controller actions, you can check the ModelState.IsValid property to verify if a user has entered valid data. 
			When users submit a form with invalid data, most websites display validation messages. These messages are often highlighted in red, but other colors or formats can be used. 
			The Html.ValidationSummary HTML Helper:
				Use the Html.ValidationSummary HTML helper to render a summary of all the invalid data in the form. This helper is usually called at the top of the form. When the user submits invalid data, the validation messages are displayed for each invalid field in a bulleted list. 
			The Html.ValidationMessageFor HTML Helper:
				Use the Html.ValidationMessageFor HTML helper to render validation messages next to each input in the form.
			The ValidationMessageTagHelper and ValidationSummaryTagHelper Tag Helpers:
				The ValidationMessageTagHelper tag helper is an alternative to the Html.ValidationMessageFor HTML helper. You can use the ValidationMessageTagHelper tag helper to display a validation message for a specific property of a model. You can use this tag helper by adding an asp-validation-for attribute to a <span> element. 
				The ValidationSummaryTagHelper tag helper is an alternative to the Html.ValidationSummary HTML helper that you can use to display validation messages that apply to the entire model. You can use this tag helper by adding an asp-validation-summary attribute to a <div> element. The value of the attribute asp-validation-summary attribute can be one of the following values: 
					• All. Using this value will cause both property and model level validation messages to be displayed. 
					• ModelOnly. Using this value will cause only model level validation messages to be displayed (excludes all property errors). 
					• None. Using this value will cause no validation summary to be displayed.
		Adding Custom Validations:
			You can use data annotations to indicate to MVC how it should validate the data that a user enters in a form or passes in query strings. Although the built-in validation attributes are very flexible, in some situations you might want to run some custom validation code: 
				• Running a Data Store Check. You want to check the data entered against the data that has already been stored in the database or is in another database store. 
				• Comparing Values. You want to compare two entered values with each other. 
				• Mathematical Checks. You want to calculate a value from the entered data and check that the value is valid. 
			To do this, you create a class that inherits from the System.ComponentModel.DataAnnotations.ValidationAttribute class. 
			Notice that the IsValid method of the ValidationAttribute class is overloaded. In case you need an access to the whole model class, you can override another version of the IsValid method, which gets as a second parameter an object of type ValidationContext. 
			You can use the ValidationContext parameter to use a service that was registered in the ConfigureServices method of the Startup class. 

Module 7: Using Entity Framework Core in ASP.NET Core
	Lesson 1: Introduction to Entity Framework Core
		Connecting to a Database Using ADO.NET:
			ADO.NET and Databases:
				When you create .NET Framework applications, including ASP.NET Core MVC web applications, you can use the ADO.NET technology to access databases. ADO.NET is a basic data access API in the .NET Framework and contains a set of data providers that support most of the free and commercial databases available today. A data provider implements database-specific protocols and features and at the same time presents a consistent API so that replacing the application's data provider does not involve many code changes.
				ADO.NET contains several data providers, which include:
					• System.Data.SqlClient. Used to connect to SQL Server databases and Microsoft Azure SQL databases. 
					• System.Data.Odbc. Used to connect to databases that support the ODBC API. 
				You can also find third-party data providers online and you can implement your own data provider.
			The ADO.NET Connection Object:
				You can use the ADO.NET connection object to connect to a database. Each provider has an ADO.NET connection object that implements the IDbConnection interface: 
					• SqlConnection
					• OdbcConnection 
				A connection object connects to the database and initiates additional operations, such as executing commands or managing transactions. Typically, you create a connection object with a connection string, which is a locator for your database and might contain connection-related settings, such as authentication credentials and timeout settings. 
			The ADO.NET Command Object:
				You can use the ADO.NET command object to send commands to a database. Commands can return data, such as the result of a select query or a stored procedure. Alternately, commands may not return any data, such as when you use an insert or delete statement or a Data Definition Language (DDL) query. 
				Each provider has an ADO.NET command object that implements the IDbCommand interface: 
					• SqlCommand
					• OdbcCommand
				A command object can represent a single command or a set of commands. Query commands return a set of results as a DataReader object or a DataSet object or a single value, usually, the result of an aggregated action, such as a row count, or calculation of an average. 
			The ADO.NET DataReader Object:
				You can use the ADO.NET data reader object to dynamically iterate a result set obtained from a database. If you use a data reader to access data, you must maintain a live connection while you read from the database. Additionally, data readers can only move forward while iterating the data. This data-access strategy is also referred to as the connected architecture. 
				Each provider has an ADO.NET data reader object that implements the IDataReader interface: 
					• SqlDataReader
					• OdbcDataReader
				When you use a data reader, you can access only one database record at a time, as shown in the preceding example. If you need multiple records at once, you must store them as you move to the next record. Although this seems like a major inconvenience, data readers are very efficient in terms of memory utilization because they do not require the entire result set to be fetched into memory. 
			The ADO.NET DataAdapter Object:
				You can use the ADO.NET data adapter object to load a result set obtained from a database into the memory. After loading the entire result set and caching it in the memory, you can access any of its rows, unlike the data reader, which only provides forward iteration. You should use this data-access strategy, referred to as the disconnected architecture, when you do not want to maintain a live connection to the database while processing the data. 
				Data adapters store the results in a tabular format. You can also change the data after it is loaded and use the data adapter to apply the changes back to the database. Each provider has an ADO.NET data adapter object that implements the IDataAdapter interface: 
					• SqlDataAdapter
					• OdbcDataAdapter
				Although data adapters are convenient to use (especially in conjunction with the DataSet class, they impose a larger overhead than data readers because the entire result set must be fetched into memory before you can perform any operations.
			The DataSet Object:
				The DataSet object is one of the most frequently used objects in ADO.NET. You use it to retrieve tabular data from a database. Although you can fill a DataSet object manually with data, you typically load it by using the DataAdapter class. 
				You can use the DataSet objects to hold information from more than one table at one time and to maintain relationships between the tables. 
		Object Relational Mapper (ORM):
			Developers write code that works with classes and objects. In contrast, databases store data in tables with columns and rows, and database administrators create and analyze databases by running Transact-SQL queries. 
			When you use ADO.NET to interact with your database, your application becomes strongly linked to the database. With ADO.NET, you implement most of your data access as plain-text SQL statements or you call stored procedures implemented on the database server in some SQL dialect. This approach is error-prone and inflexible. Changing the database schema might require considerable modifications to the code of the application. 
			ORM is a programming technique that simplifies the application’s interaction with data by using a metadata descriptor to connect object code to a relational database. ORM provides an abstraction that maps application objects to database records. 
			There are multiple ORM frameworks. You can choose the one which is appropriate for you. One of the most popular ORM frameworks is Entity Framework. Entity Framework was created by Microsoft and it maps the tables and columns found in a database to the objects and properties that are used in .NET code. 
			Besides Entity Framework, there are also other ORM frameworks such as Hibernate and Django. Nevertheless, Entity Framework was made for .NET Framework by Microsoft as the best match for .NET applications. The EF Core is a version which can run on different operating systems and work with different databases.
		Overview of Entity Framework:
			Entity Framework provides a one-stop solution to interact with data that is stored in a database. Instead of writing plain-text SQL statements, you can work with your own domain classes, and you do not have to parse the results from a tabular structure to an object structure. These domain classes are called entities. Entity Framework keeps track of the changes you make to the entities to enable updating the database with data that exists in memory. 
			Entity Framework introduces an abstraction layer between the database schema and the code of your application, which makes your application more flexible. Using Entity Framework, you can access objects by using strongly typed code and query them by using LINQ. Therefore, you no longer have to rely on SQL statements. This makes your code more robust.
			Entity Framework Approaches:
				Entity Framework provides three general approaches to create your data access layer (DAL) of the application: 
					• Database First
					• Model First
					• Code First
				In each of these approaches, Entity Framework can either create a new database, according to the data model or use an existing database. 
				Model First and Database First:
					In these approaches, the data model is generated by using a designer in Visual Studio. The model is stored in a .edmx file. The entity classes and the relationships are generated from the model. 
					If you do not already have a database, after you design your data model, you can generate database scripts from the model by using the Visual Studio designer. You can then run the script on a new database to create the tables. On the other hand, if your database existed prior to creating the data model, you can use the Visual Studio designer to reverse engineer the data model from the database tables.
				Code First:
					In this approach, you do not use a .edmx file to design your model, and you do not rely on the Visual Studio designer. The domain model is simply a set of classes with properties that you provide. In the Code First approach, Entity Framework scans your domain classes and their properties and tries to map them to the database. 
					You can use the Code First approach both with new databases and with existing ones. If you do not have a database, the default behavior of the Code First approach is to create the database the first time you run your application. If your database already exists, Entity Framework will connect to it and use the defined mappings between your model classes and the existing database tables. 
					Entity Framework Versions
						There are different versions of Entity Framework and it is important to be familiar with them. 
					Entity Framework 6 (EF6)
						Entity Framework 6 (EF6) was first released in 2008. With EF6 you can control the data stored in the database and use it within your application. It supports the Model First, Database First, and Code First approaches, which allow you to choose the design approach for your application. However, EF6 is not supported on operating systems other than Windows. 
					Entity Framework Core (EF Core)
						After the decision of making .NET Framework cross platform, supporting several operating systems and not only Windows, there was a need to re-implement Entity Framework. As a result, EF Core was created. EF Core is the .NET Core version of Entity Framework. The EF Core is extensible; hence it comes lightweight giving you the minimum needed features to work with and is capable of adding more features by request. Moreover, EF Core is open source, hence you can find solutions or specific features as well as creating your own. It is important to note that EF Core works only with the Code First approach. However, it is a modern, advanced and rising framework that can be used widely and by a great variety of platforms. 
		Database Providers:
			EF Core is a layer between your code and a database. It is used to connect your code to the database. A database provider is a library which is used by EF Core to connect to a specific database. Database providers are distributed as NuGet packages. If you want to use a database provider in your application, you can install the corresponding NuGet package. 
			The Microsoft SQL Provider:
				A very commonly used database provider in EF Core is the SQL Server provider. The SQL Server provider enables you to connect a SQL Server database to your application by using EF Core. SQL Server database is a Microsoft’s relational database with a tabular structure. 
			The SQLite Provider:
				SQLite is a popular, open source database that is widely used by developers. You can use the SQLite provider to connect an SQLite database to your application by using EF Core. 
			The InMemory Provider:
				Entity Framework Core introduced the InMemory provider. It is important to point out the InMemory provider does not connect to a real relational database and does not mimic a database, but it is designed to be a tool for testing your application. 
			Other Providers:
				There are plenty of other database providers. Using these providers, it is possible to connect to various database engines such as MySQL, Maria DB and Db2. However, notice that while some database providers are maintained by the EF Core Project (Microsoft) vendor, others database providers might be maintained by other vendors. Before using a database provider, ensure that it meets your requirement. 
			Adding a Database Provider to Your Application:
				Usually a database provider is distributed as a NuGet package. Therefore, it is quite simple to add a database provider to your application. To add a database provider to your application you can either use the dotnet tool or use the NuGet Package Manager Console.
				Add a Database Provider by using the dotnet Tool
					dotnet add package provider_package_name 
				Adding a Database Provider by using a NuGet Package Manager Console Command:
					install-package provider_package_name
	Lesson 2: Working with Entity Framework Core
		Using an Entity Framework Context:
			In EF Core, a context is how you access the database, without the need for additional wrappers or abstractions. An Entity Framework context is the glue between your domain model classes and the underlying framework that connects to the database and maps object operations to database commands. 
			An Entity Framework context is used to: 
				• Provide basic create, read, update, and delete (CRUD) operations, and to simplify the code that you need to write to execute these operations. 
				• Handle the opening and closing of database connections. 
				• Handle the database generation when working with the Code First approach.
			Adding an Entity Framework Context:
				To coordinate with Entity Framework functionalities, you will have to create an Entity Framework context. A context in Entity Framework Core is a class that inherits from the DbContext class. 
			Domain Model Classes (Entities):
				In addition to creating an Entity Framework context, you should also create the domain model classes of your application. Domain model classes are POCO classes. The reason for this name is that they do not have any dependency on EF Core. They just define the properties of the data that is stored in the database. The domain model classes are called entities. In your code, you specify which entities are included in the data model and you can also customize certain Entity Framework behavior for each entity. 
			Adding DbSet Properties to an Entity Framework Context:
				When working with EF Core, you want to ensure that the entities defined in the code are mapped to tables defined in the database. To achieve this, you can add the DbSet<> properties to the Entity Framework context. Each DbSet<> property gets an entity type as a type parameter. In relational databases, the entity is mapped to a table in the database. EF Core will map the properties according to the mapping information provided by the classes.
			Connecting an Entity Framework Context to SQLite Database:
				You can connect an Entity Framework context to a database. To connect an Entity Framework context to a database, you need to use a database provider which corresponds to the database chosen. 
				One of the most popular databases that you can access when using EF Core is SQLite. SQLite is a serverless database, which means that you don’t need to have a separate server to use it (it is file based). One of the big advantages of SQLite, when compared to other databases, is that it is very easy to configure. 
				To use an Entity Framework context in an ASP.NET Core application, you need to register a service by using the AddDbContext<> method, passing it the Entity Framework context class type as a generic type parameter. You should register the service in the ConfigureServices method of the Startup class.
				Using an Entity Framework Context in a Controller:
					After configuring the Entity Framework context, you can use it in a controller. The Entity Framework context is passed to the constructor of the controller you are using due to the dependency injection mechanism.
				Creating and Deleting a Database:
					You can use the Database property of a DbContext object to create a database or delete a database. By using the Database property of the DbContext object, you can call the EnsureCreated and EnsureDeleted methods. When you call the EnsureCreated method, Entity Framework creates a database based on the information in your DbContext-derived class if the database does not already exist. When you call the EnsureDeleted method, Entity Framework deletes the database if it already exists.
				Data Seeding:
					You can use a data seeding to populate the database with sample data when a database is created. When you call the EnsureCreated method, a new database will be created and initialized with the sample data. Notice that in case a database already exists when EnsureCreated is called, the sample data won’t be added to the database.
		Using LINQ to Entities:
			LINQ is a set of extension methods that enable you to write complex query expressions. You can use these expressions to extract data from databases, enumerable objects, XML documents, and other data sources. The expressions are similar to Transact-SQL queries, but you can get IntelliSense support and error checking in Visual Studio. 
			What is LINQ to Entities?
				LINQ to Entities is the version of LINQ that works with EF Core. LINQ to Entities enables you to write complex and sophisticated queries to locate specific data, join data from multiple objects, and take other actions on objects from an Entity Framework context. If you are using EF Core, you can write LINQ queries wherever you require a specific instance of a model class, a set of objects, or for more complex application needs. You can write LINQ queries in query syntax, which resembles SQL syntax, or method syntax, in which operations such as “select” are called as methods on objects. 
			The LINQ Query:
				The query is specifying the information which you want to get from your database. Moreover, you have the option to specify how the retrieved information will be sorted, grouped or formatted before it is returned. The LINQ query is stored in a query variable and initialized with a query expression. A query expression is a query expressed in query syntax, it is a first-class language construct and it can be used in any context in which a C# expression is valid. You should notice that the writing of LINQ query is slightly different than writing a SQL query. 
		Loading Related Data:
			In EF Core you can load related entities by using navigation properties. To load related data, you need to choose an ORM pattern. EF Core contains several ORM patterns, which include: 
				• Explicit loading. Using this pattern, the related data is loaded explicitly from the database after the original query is completed. 
				• Eager loading. Using this pattern, the related data is loaded from the database as part of the original query. 
				• Lazy loading. Using this pattern, the related data is loaded from the database as you access the navigation property.
			Navigation Properties:
				In EF Core you can link an entity to other entities using by navigation properties. When an entity is related to another entity, you should add a navigation property to represent the association. When the multiplicity of the association is one or zero-or-one, the navigation property is represented by a reference object. When the multiplicity of the association is many, the navigation property is represented by a collection. 
			Explicit Loading:
				To load related entities by using the explicit loading ORM pattern, you should use the Entry method of the Entity Framework context class.
				You can also use the explicit loading ORM pattern in conjunction with LINQ. To do so, you should first call the Query method, and then you can call the LINQ methods such as Count and Where.  
			Eager Loading:
				In addition to loading related data by using the explicit loading ORM pattern, you can also choose to load related data by using the eager loading ORM pattern. However, while loading related data by using the explicit loading ORM pattern is executed explicitly after the original query is completed, when the eager loading ORM pattern is used the related data is loaded as part of the original query. 
				To load related entities by using the explicit loading ORM pattern you need to use the Include method. The Include method specifies related entities to be included in the query results. 
				In case you need to include related data from multiple relationships, you can use the Include method several times in the same query. 
				If you need to include more levels of related data, you can use the ThenInclude method. The ThenInclude method can be used to drill down through the relationships. 
			Lazy Loading:
				The lazy loading ORM pattern can be used when you want to load related data from the database as you access the navigation property. 
				In case you want that EF Core will use lazy loading to load a navigation property, you should change the navigation property to be overridden. To change a navigation property to be overridden you can use the virtual keyword. 
				If you want to use the lazy loading ORM pattern, in addition to changing the navigation properties to be overridden, you should also turn on the creation of lazy-loading proxies. To turn on the creation of lazy-loading proxies, you can call the UseLazyLoadingProxies method. 
		Manipulating Data by Using Entity Framework:
			EF Core can track entities that you retrieve from the database. EF Core uses change tracking so that when you call the SaveChanges method on the Entity Framework context object, it can synchronize your updates with the database. You can check the status of any entity (such as whether it was modified), inspect the history of your changes, and undo changes if required. 
			Each entity in EF Core can be in one of the following states: 
				• Added. The entity was added to the context and does not exist in the database. 
				• Modified. The entity was changed since it was retrieved from the database. 
				• Unchanged. The entity was not changed since it was retrieved from the database. 
				• Deleted. The entity was deleted since it was retrieved from the database. 
			Inserting New Entities:
				To add an entity to the database, you can use the Entity Framework context object. When you use the Entity Framework context object to add a new entity to a database, the context marks the change tracking status of the entity as Added. When you call the SaveChanges method, the Entity Framework context object adds the entity to the database. No changes are applied to the database until you call the SaveChanges method. 
			Deleting Entities:
				To delete an entity from the database, you can use the Entity Framework context object. When you delete an entity from a database, the context marks the change tracking status of the entity as Deleted. When you call the SaveChanges method, the Entity Framework context object deletes the entity from the database. 
			Updating Entities:
				To update an entity in the database, you can use the Entity Framework context object. When you update an entity, the context marks the change tracking status of the entity as Modified. When you call the SaveChanges method, the Entity Framework context object updates the entity in the database. 
	Lesson 3: Using Entity Framework Core in ASP.NET Core
		Connecting to Microsoft SQL Server:
			The UseSqlServer method configures the Entity Framework context to connect to a SQL Server database.
			The name, data type and constraints of each column are determined by EF Core using a set of conventions. 
			By convention, the entity key of an entity in EF Core is either a property named Id or a property named <type name>Id.
			Configuration in EF Core:
				EF Core uses conventions to build a database based on the content of the Entity Framework context and the entities which belong to the Entity Framework context. In case you find that some of those conventions are not suitable for your needs, you can use configuration to override the conventions. 
				One way to specify configuration in EF Core is by using Fluent API. When you use Fluent API, you write the configuration in the OnModelCreating method of the Entity Framework context class. 
				In addition to Fluent API, you can also specify the configuration in EF Core by using another method called data annotations. However, the data annotations method is quite different from the Fluent API method. While in the Fluent API method the configuration is specified in the OnModelCreating method of the Entity Framework context class, in the data annotations method the configuration in specified by using attributes in the entities. 
		Configuration in ASP.NET Core:
			Usually, an application needs to get parameters. These parameters are used to define settings and preferences which are needed when running the application. An example of such a parameter is the connection string which is used by the application to connect to a database. A recommended approach is to store such parameters in a configuration file. Storing the parameters in a configuration file will allow you to change the settings and preferences without recompiling the application. 
			ASP.NET Core is capable of reading parameters from various sources. To access a source ASP.NET Core uses a configuration provider. There are configuration providers for several configuration file formats, including JSON, XML, and INI. In addition, there are configuration providers for other sources such as command line arguments and environment variables. You can even create a custom provider if needed. 
			JSON Configuration:
				JSON is a text format which can be used for several purposes, such as transmitting data between applications and storing data in a file. One of the biggest advantages of JSON is that it is very easy for people to read and write JSON. JSON can also be parsed and generated easily by applications. 
				To use a JSON configuration file in ASP.NET Core, you should add the JSON configuration provider. To add the JSON configuration provider, you can call the AddJsonFile method on an object that implements the IConfigurationBuilder interface. The AddJsonFile method gets the path to the configuration file as a parameter. Then you can call the Build method on the object, which implements the IConfigurationBuilder interface, to build an object that implements the IConfiguration instance with settings from the set of sources registered in the IConfigurationBuilder.Sources property. Finally, you can call the UseConfiguration method, passing it the object which implements the IConfiguration instance as a parameter to use the given configuration settings on the web host. 
				The call to the WebHost.CreateDefaultBuilder method initializes a new instance of the WebHostBuilder class with pre-configured defaults. One of the defaults is loading values from a configuration file named appsettings.json. Therefore, if you need to load values from a file named appsettings.json, calling the CreateDefaultBuilder method is sufficient and you don’t need to call the AddJsonFile method and pass to it appsettings.json as a parameter. 
				To load a value from the configuration file, you can use an indexer of an object which implements the IConfiguration interface and pass to it the name of the name-value pair as a parameter. 
				If the name of the name-value pair is hierarchical, you should use a colon to separate the sections of the name to retrieve the value. 
			XML Configuration:
				In addition to using JSON configuration files in ASP.NET Core, it is also possible to use XML configuration files in ASP.NET Core. To use an XML configuration file in ASP.NET Core, you should add the XML configuration provider. To add the XML configuration provider, you can call the AddXmlFile method on an object which implements the IConfigurationBuilder interface. 
				You can configure an ASP.NET Core application to read parameters from multiple configuration files. To do that you should add several configuration providers. For example, in case you want to read parameters from both JSON and XML files, you should call both AddJsonFile and AddXmlFile on an object which implements the IConfigurationBuilder interface. Notice that in case you read data from multiple sources, the order in which the data providers are added matter. In case the same name exists in multiple sources, only the value from the last source will be returned. Values from other sources will be ignored.
		Specifying a Connection String in a Configuration File:
			Storing the connection string for your application in a configuration file instead of storing it as a hard-coded string in your code is a good practice. In older versions of ASP.NET, all configurations including connection strings were stored in the web.config file. In ASP.NET Core, you can store the connection strings in other configuration files in various formats, including JSON, XML and INI. You can also store the connection strings in other sources such as environment variables or command line arguments. 
			To read a connection string from a configuration file, you can replace the call to the indexer of IConfiguration with a call to the GetConnectionString method. 
		The Repository Pattern:
			Web applications usually need to access databases and typically, the simplest approach is to use the controller to access the database directly. However, calling the database directly from the controller is not always the best practice. A more flexible approach is that the controller will call a repository that will access the database. Using this approach, the controller will not access the database directly.
			To call a repository from a controller, you should perform the following steps: 
				1. Define an interface for the repository class. This interface declares the methods that the repository class uses to read and write data from and to the database. 
				2. Create and write code for the repository class. This class must implement all the data access methods declared in the interface. 
				3. Use dependency injection to inject the repository class to a controller.
				4. Modify the controller class to use the repository class. 
		Using Migrations:
			Working with an application that interacts with a database requires you to create models and sometimes, you might find yourself changing or upgrading those models according to the application needs. When working with EF Core, your database is relying on the models you have created. To reflect the changes in the models to the database, you can use migrations. Using migrations, you will be able to create a database, upgrade it and manipulate it according to your application models. 
			If you created your database by using an Entity Framework context, and you later want to change something in your domain model classes e.g. your schemas, EF Core will not update the database automatically. You might encounter exceptions while running queries or saving your changes to the database. You can use migrations to update the database schema automatically to match the changes you made in your classes without having to recreate the database. 
			With migrations, you define the initial state of your classes and your database. After you change your classes and execute the migrations in design time, the set of changes you performed over your classes is translated to the required migration steps for the database, and then those steps are generated as database instructions in the code. You can apply the changes to the database in design-time, before deploying the version of the application. Alternatively, you can have the application execute the migration code after it starts. 
			Entity Framework Core Package Manager Console Tools:
				You can work with migrations by using the Entity Framework Core Package Manager Console (PMC) Tools. You can use NuGet’s Package Manager Console to run the PMC tools in Visual Studio. 
				To create a database by using migrations, you should first add an initial migration. Then, you need to apply the migration to the database. As a result, the schema of the database will be created.
				Adding an Initial Migration:
					Add-Migration InitDatabase
				Create the Schema of the Database:
					Update-Database
				When you change an entity, the database and the code are not in sync anymore. To sync the code and the database, you should add another migration.
				Adding a New Migration:
					Add-Migration AddAge
				Applying the new Migration to the Database:
					Update-Database

Module 8: Using Layouts, CSS and JavaScript in ASP.NET Core MVC
	Lesson 1: Using Layouts
		What are Layouts?
			The ASP.NET Core MVC Razor engine includes a feature called layouts. Layouts are also called template views. Layouts enable you to define a common style template and then apply it to all the views in a web application. The functionality of layouts is similar to that of the master page in a traditional ASP.NET web application. You can use layouts to define the content layout or logic that is shared across views. 
			You can define multiple layouts in an ASP.NET Core MVC application, and each layout can have multiple sections. You can define these sections anywhere in the layout file, even in the <head> section of the HTML. Sections enable you to output dynamic content to multiple, non-contiguous regions of the final response that is sent to the browser. 
		Creating a Layout:
			When you create layouts, you should store the layout files in the \Views\Shared folder of the project. The \Views\Shared folder is the default location where you can store common view files or templates.
			The @RenderBody method indicates to the rendering engine where the content of the view should be placed. 
			The layout and view share the same ViewBag. You can use the ViewBag property to pass information between a view and a layout. To pass information, you can add a property to the ViewBag property in the View file and use the same property in the layout file. Properties help you control the content in the layout to dynamically render webpages from the code in the view file. For example, consider that a template uses the ViewBag.Title property to render the <title> content in the view. This property helps you to not only define the Title property of the ViewBag property in the view but also to retrieve the property in the layout. This retrieval is possible because the code in the view file runs before the layout. 
		Linking Views and Layouts:
			After defining a layout, you can link the layout to the view files. To link a layout to a view, you should add the Layout directive at the top of the view file. 
			You can use the ViewBag.Title property to pass the page title information from the view to the layout. You can define other properties along with the ViewBag property, such as <meta> elements in the <head> section and enable them to pass information to the layout. 
			The _ViewStart.cshtmlFile:
				Usually, you have the same layout across an entire web application. You can define the layout for an application by using the _ViewStart.cshml file. During run time, the code in the _ViewStart.cshtml file runs before all the other views in the web application. Therefore, you can place all common application logic in the _ViewStart.cshtml file. 
				To use the _ViewStart.cshtml file, you should add it to the \Views folder of your project. The following code illustrates the content of the _ViewStart.cshtml file.
				If the web application contains the _ViewStart,cshtml file shown above, you don’t have to include the link to the layout in the Index view. 
		Using Sections in a Layout:
			A section is a region of content within a layout. It is a placeholder within a layout that allows a view to insert content. You can add sections to a layout by using the RenderSection method. The RenderSection method gets the name of the section as a parameter. In a view, you can set the content of the section by using the @section directive followed by the name of the section. 
			The Required Parameter:
				The RenderSection method gets the name of the section as a parameter. It can get another parameter that indicates if the section is required. This is an optional parameter of type bool. Consider a section in a layout that is required and you do not have a corresponding @section directive in the view file. In this case, an exception will be thrown at run time. 
	Lesson 2: Using CSS and JavaScript
		Importing Styles:
			Cascading Style Sheets (CSS) is an industry standard for applying styles to HTML pages. After creating CSS styles, you should import these styles into the web application. Different methods of applying CSS to a webpage are available. These methods include external CSS file, inline CSS, and CSS code block in HTML. Developers usually use an external CSS file because this file is shared across multiple pages and it helps to apply a consistent style across the application. It is common to add a link to the external CSS file from a layout. Therefore, after importing the CSS file into the web application, you need to modify the layout of the web application, so that you can use the CSS styles that you imported. You can modify the layout of a web application by using the <link> element.
			CSS selectors help browsers to determine how the CSS styles should be applied. You can use various selectors, such as class and id selectors to apply styles to HTML elements.
			CSS class Selector:
				You can define a CSS class selector to specify a style for a group of elements. To apply the class selector to an HTML element, you need to add the class attribute to the HTML element. You can use the .<class> syntax to add the style in the CSS file.
				Using a class attribute in a view:
					Example:
						<p class="menu">this is menu</p>
				Applying a style to a class:
					Example:
						.menu {
							font-weight:bold;
						}
			CSS id Selector:
				You can use the CSS id selector to specify a style for any unique element in your HTML code. To apply the id selector to an HTML element, you need to add the id attribute and a unique name to the HTML element. You can use the #<id> syntax to add the style in the CSS file. 
				Using an id attribute in a view:
					Example:
						<p id="leftmenu">this is menu</p>
				Creating an id selector:
					Example:
						#leftmenu {
							font-size:16px;
						}
		Rendering and Executing javaScript Code:
			JavaScript code helps add interactive functionalities to the webpages of your application. You can add JavaScript code to web applications by:
				• Adding the JavaScript code to a view file. 
				• Defining the JavaScript code in dedicated JavaScript files.
			Inserting a JavaScript function:
				Example:
					<script>
						function helloWorld() {
							alert('Hello World');
						}
					</script>
			If you have multiple views in a web application, you need to add the JavaScript code for each view. You cannot simultaneously add JavaScript code for multiple views. Therefore, you can define the JavaScript code in a JavaScript file (.js file). Then, you can reference the JavaScript file in multiple views. This enables you to maintain a single JavaScript file to edit the JavaScript code for multiple views. You can also have a reference to multiple JavaScript code files from a single view. 
			The example.js file:
				Example:
					function helloWorld() {
						alert('Hello World');
					}
			Referencing a JavaScript file:
				Example:
					<script src="~/example.js"></script>
			Referencing a JavaScript File from a Layout:
				In ASP.NET Core MVC applications, it is common to add a reference to the JavaScript file from a layout. Therefore, after importing the JavaScript file into the web application, you need to modify the layout of the web application, so that you can use the JavaScript code. You can modify the layout of a web application, by using the <script> element. 
			Calling JavaScript Functions:
				You can call functions defined in JavaScript files by using script blocks or event handlers. 
			Using a script block to call a function:
				Example:
					<script>
						helloWorld();
					</script>
			If you want to avoid calling the JavaScript function directly, you can use the onclick JavaScript event to trigger JavaScript functions. The onclick event initiates the JavaScript function when you click the corresponding HTML element. JavaScript functions that are attached to the document object model (DOM) events are called event handlers. 
			Using an event handler in a view:
				<input type="button" value="Hello" onclick="helloWorld();" />
			Calling JavaScript Built-In Functions:
				JavaScript has a large variety of built-in functions that allow you to perform different actions. There are functions that deal with converting one type of variable to another, functions that perform mathematic calculations and there are many others. 
			The following functions are examples of functions that deal with converting one type of variable to another: 
				• parseInt(). This function accepts a string and returns an integer. For example, the string "20.6" will be converted to 20. 
				• parseFloat(). This function accepts a string and returns a floating point number. For example, the string "50.1" will be converted to 50.1. 
			In addition to built-in functions, there are also built-in objects in JavaScript. For example, there is a built-in object in JavaScript named Math. Math is an object that allows to perform mathematic calculations on numbers:
				• Math.round(). A function that is used to round off a number to the nearest integer. 
				• Math.floor(). A function that is used to round down a number to the nearest integer. 
				• Math.ciel(). A function that is used to round up a number to the nearest integer.
			Using built-in JavaScript functions:
				Example:
					var userInput = "50.5";
					var parsedNumber = parseFloat(userInput); // Output 50.5
					var roundedNumber = Math.round(parsedNumber); // Output 51
					var flooredNumber = Math.floor(parsedNumber); // Output 50
					var ceiledNumber = Math.ceil(parsedNumber); // Output 51
		Using External Libraries:
			Serving Files from a Local Folder:
				One of the ways to add a library to an ASP.NET Core application is to download the source files from an official source such as jQuery from the jQuery website or Bootstrap from the Bootstrap website. 
				After downloading the files, in order to load them in the application, you should add them to the static files folder which by default is the wwwroot folder. 
			Linking a layout to the jQuery library:
				Example:
					<script src="~/js/jquery-3.3.1.js"></script>
			One of the disadvantages of serving files from a local folder is that the loading time of those files is long. To make the loading time shorter you can serve the files by using a Content Delivery Networks (CDN) instead of serving them from a local folder. 
			Serving Files using CDN:
				A CDN is a group of geographically distributed servers used for hosting content for web applications. In many cases, you can bring web content geographically closer to your application’s users by using a CDN to host libraries. This will also improve the scalability and robustness of the delivery of that content. 
				Another benefit of CDN is that many websites use the same CDN, so the files they use are cached in the user's browser. This can help to decrease the loading time of your website because the files you are using have already been downloaded to the client’s browser. 
				The amount of content stored in a CDN varies among different web applications. Some applications store all their content on a CDN, while other applications store only some of their content. 
				Microsoft has a dedicated CDN called Microsoft Ajax CDN that hosts some popular libraries, such as jQuery and Bootstrap. 
				You can often reduce the loading time of your web application, by using the libraries hosted on Microsoft Ajax CDN. Web browsers can cache these libraries on a local system. 
				Linking to the jQuery library by using a CDN:
					Example:
						<script src="http://ajax.aspnetcdn.com/ajax/jquery/jquery-3.3.1.min.js"></script>
				Having a .min.js extension means this file is minified and is more lightweight than the file with the .js extension.
				Applications in production should not entirely depend on CDN assets because they might be unavailable. Applications should test the CDN asset referenced and when it is not available, to use a fallback method such as serving a local file. 
			Using Package Managers:
				You can avoid adding libraries manually to your web application by using a package manager. Package manager is a tool that automates the process of installing, upgrading and configuring packages. It also allows developers to create, share and use handy code. Using a package manager helps to reduce the need for configuration tasks while adding libraries to an application. 
				There are several package managers that you can choose from, which include: 
					• NuGet
					• Yarn
					• Webpack
					• Bower
					• npm
				NuGet:
					NuGet is a free and open source package manager which Microsoft has created for the Microsoft development platform and it is part of Visual Studio. In past versions of ASP.NET, it was recommended to use the NuGet package manager to install client-side libraries such as jQuery and Bootstrap. In ASP.NET Core this has changed and the NuGet package manager should be used mainly to install server-side packages. 
				Yarn:
					Yarn is a very fast and secure package and dependency manager. It allows to use and share code with other developers. Yarn checks the integrity of the packages being installed before executing the code. 
				Webpack:
					Webpack is a module bundler for JavaScript applications and serves as a task runner. It processes your application and creates a dependency graph. This graph maps exactly what are the modules that your application needs. Webpack creates bundles including the project’s dependencies. 
				Bower:
					Bower is a lightweight package manager that helps to manage frameworks, libraries, and assets on the web. It does not bundle the packages or perform tasks as Webpack but only installs the packages your application needs. 
				npm:
					One of the most popular package managers that allows adding libraries and front-end frameworks to your application. It has many registered packages and the number is constantly growing. 
				Using npm in ASP.NET Core Applications:
					To start using npm in your ASP.NET Core application, you should add a package.json file to your solution in your project’s root folder. This file gives npm information which allows it to identify the project and to handle the project's dependencies. It can also contain more information about the project such as a project description and the version number in a specific distribution. 
					The package.json file includes several kinds of properties. It includes information describing your application which includes application version, name, and privacy settings. The most important properties for developers are dependencies and devDependencies. 
						• dependencies. This is a list of packages that your client-side logic depends on such as jQuery and Bootstrap. When a package is located in the dependencies section, npm will always install it as part of the application. 
						• devDependencies. This is a list of packages that will be installed only in a development environment. Those packages are not related to the client-side logic but to bundling, minification, and compilation among others. 
					Each package listed in the dependencies and devDependencies section has a version number that indicates which version of the package will be installed. Visual Studio will always recommend installing the latest stable version of each package.
				Adding jQuery by using npm:
					Example:
						{
						  "version": "1.0.0",
						  "name": "asp.net",
						  "private": true,
						  "dependencies": {
							"jquery": "3.3.1"
						  },
						  "devDependencies": {
   
						  }
						}
				After adding jQuery inside the dependencies section and saving the package.json file, the jQuery package will be downloaded and placed inside the node_modules folder. This folder is located inside the root folder of the application and will contain all the code the application needs. 
				By default, static files in an ASP.NET Core application can be served only if they are located in the wwwroot folder. If, however you want to allow static files to be served from a different folder, for example, the node_modules folder, you can use the UseStaticFiles method and pass an instance of type StaticFileOptions to it as a parameter. 
				Add the node_modules folder to the static files middleware inside the Startup.cs file:
					Example:
						app.UseStaticFiles(new StaticFileOptions
						{
							FileProvider = new PhysicalFileProvider(
								Path.Combine(env.ContentRootPath, "node_modules")
							),
							RequestPath = "/node_modules"
						});
				Using scripts installed with npm:
					 Example:
						<script src="../node_modules/jquery/dist/jquery.js"></script>
				Directly referencing the node_modules folder from inside of views is not considered a good practice. A better practice is to use bundling and minification.
				Bundling and Minification:
					Script libraries that you add to an application can slow down the loading time of webpages because it takes time to download the scripts into the browser. Bundling and minification help reduce the loading time of web applications by reducing both the number and size of HTTP requests. 
						• Bundling helps combine multiple JavaScript libraries into a single HTTP request. 
						• Minification compresses code files before incorporating them in the client application.
					To use bundling and minification in an ASP.NET Core application, you can use the bundleconfig.json configuration file. The bundleconfig.json configuration file defines how the application’s static files will be bundled and minified. 
				Adding jQuery to a bundleconfig.json File:
					Example:
						[
						  {
							"outputFileName": "wwwroot/css/site.min.css",
							// An array of relative input css file paths. 
							"inputFiles": [
							  "wwwroot/css/site.css"
							]
						  },
						  {
							"outputFileName": "wwwroot/js/scripts.min.js",
							"inputFiles": [
							  // An array of relative input js file paths including jQuery. 
							   "wwwroot/lib/jquery/dist/jquery.js",
							   "wwwroot/js/site.js"
							],
							// Optionally specify minification options
							"minify": {
							  "enabled": true,
							  "renameLocals": true
							},
							// Optionally generate .map file
							"sourceMap": false
						  }
						]
				To enable the execution of bundling and minification during the build time, you can download the BuildBundlerMinifier NuGet package from the NuGet package manager. This package injects MSBuild Targets which are grouped tasks that run at build and clean time. 
				Each time that you run a Build or Clean in your application, the bundleconfig.json file will be analyzed by the build process and the output files will be produced based on the defined configuration. 
	Lesson 3: Using jQuery
		Introduction to jQuery:
			jQuery helps query the HTML Document Object Model (DOM) and obtain a set of HTML DOM elements.
			This feature of jQuery helps to:
				• Reduce the amount of code that you need to write to perform a task.
				• Reduce the client-side development time of web applications.
			jQuery includes the following features:
				• DOM element selections.
				• DOM traversal and modification.
				• DOM manipulation, based on CSS selectors.
				• Events.
				• Effects and animations.
				• AJAX.
				• Extensibility through plug-ins.
				• Utilities.
				• Compatibility methods.
				• Multi-browser support.
			jQuery Files:
				The jQuery original version and jQuery minified version provide similar functionalities; however, they optimize web applications for different purposes:
					• jQuery original version (jQuery-<version>.js). This is the uncompressed version of jQuery.
					• jQuery minified version (jQuery-<version>.min.js). This includes the compressed and gZip versions of jQuery.
				When you are in the production environment, you can use the jQuery minified version to reduce the loading time of the web application. However, if you use the minified version while working on the development environment, you cannot access the source code of the JavaScript libraries during the debug operation. Therefore, you can use the original version of jQuery, while you are in the development environment.
				In addition to the regular version of jQuery, there is also a slim version of jQuery. The slim version of jQuery excludes modules that exist in the original version of jQuery, such as ajax and effects. It also excludes deprecated code.
			The jQuery Syntax:
				Every jQuery line of code starts with the $ or jQuery variables and it allows you to call functions that exist inside the jQuery interface, to query the DOM and perform DOM manipulations.
				After adding the jQuery library to your application, a global variable is automatically added to the window object named $. It holds the whole interface which jQuery exposes and it also includes the query function that allows it to traverse the DOM. jQuery also adds a global variable named jQuery that is pointing to the same object as the $ variable. This variable is used when another library or code is using the $ sign and you want to avoid collision between the jQuery library and the other library.
				Example:
					<script>
						$(function() {
							$.each([4, 9], function(index, value) {
								alert(index + ":" + value);
							});
						});
					</script>
				Example:
					<script>
						jQuery(function() {
							jQuery.each([4, 9], function(index, value) {
								alert(index + ":" + value);
							});
						});
					</script>
		Accessing HTML Elements by using jQuery:
			jQuery helps to access and manipulate HTML elements, to create interactive web applications. jQuery selector syntax starts with a call to the jQuery object which accepts a string consisting of CSS selectors.
			Using jQuery selectors:
				Example:
					$(element name|#id|.class)
			Using a jQuery selector to access an HTML element with hello id:
				Exmple:
					$("#hello")
			Accessing all instances of the a element:
				Example:
					$("a")
			You can use more than one selector in a single query to access multiple types of elements at once.
				Using more than one selector in a query:
					Example:
						$("a, div.big, #hello")
				After accessing the HTML elements, you can perform actions on the elements, such as:
					• Defining event handlers to respond to events associated with the selected HTML elements.
					• Reading the attributes of the HTML elements.
				Using a jQuery event handler:
					Example:
						$("#hello").click(function(event) {
							alert("Hello World");
						});
			The document.ready() and $() Functions:
				If the jQuery scripts load before the webpage loads, you might encounter errors such as object not defined. You can place the jQuery code in the document.ready function, to prevent the code from loading until all HTML elements in the page load.
				Using the document.ready function:
					Example:
						$(document).ready(function() {
							// Code placed here will not execute before all HTML elements in the page load.
						});
				Experienced developers usually shorten this code by using the $() syntax instead of $(document).ready.
				Shorthand for $(document).ready:
					Example:
						$(function() {
							// Code placed here will not execute before all HTML elements in the page load.
						});
				Using jQuery in a view:
					Example:
						<div>
							Hello
							<input type="button" value="Hello" id="hello" />
						</div>
						<script>
							$(function() {
								$("#hello").click(function(event) {
									alert("Hello World");
								});
							});
						</script>
				It is important to load jQuery library to the page before calling the $() or $(document).ready() functions. Using the $ object before the jQuery library is loaded might cause an exception. When using a layout file to load scripts, place the jQuery library source script inside the <head> tag before all the other files.
			Use jQuery to Read Attributes of HTML Elements:
				You can use functions in jQuery to read the attributes of the HTML elements. An example of such a function is the text function. You can use the text function to get the text inside of HTML elements. If there are multiple HTML elements inside a specific element, the function will return the combined text of all HTML elements.
				The text function:
					Example:
						<h1>This is the main headline of a <span>HTML document</span></h1>
						<script>
							$(function() {
								let res = $('h1').text(); 
								console.log(res); // Result: "This is the main headline of a HTML document".
							});
						</script>
		Modifying HTML Elements by using jQuery:
			You can use jQuery to query HTML DOM and obtain HTML elements. You can use jQuery functions to modify the attributes associated with the HTML elements. The following are some commonly used jQuery functions, which enable you to modify HTML elements.
			The val Function:
				You can use the val function to get or set the value of an HTML element.
				Example: 
					$("#hello").val('Hello World');
			The css Function:
				You can use the css function to get or set the inline CSS style associated with an HTML element.
				Example: 
					$('#hello').css('background-color', 'blue');
			The addClass Function:
				You can use the addClass function to assign a CSS class to an HTML element.
				Example:
					$('#hello').addClass('input_css_class');
			The animate Function:
				You can use the animate function to animate CSS properties of HTML elements.
				Example:
					$("#hello").animate({
						"fontSize": "32px",
						"letterSpacing": "10px"
					}, 1200);
			Calling a Server by using jQuery:
				jQuery includes the ajax function that helps:
					• Perform asynchronous calls to a server.
					• Retrieve the data returned from a server.
				You can pass several parameters to the ajax function to control how to call the server. :
					• method. This parameter controls the request method that you can use while calling the server. Common method values are: GET, POST, PUT and DELETE.
					• dataType. This parameter defines the data type you expect to get from the server.
					• url. This parameter provides the URL of the server.
					• data. This parameter defines the data that you should provide as a parameter to the server.
					• contentType. This parameter defines the HTTP content type that you should use, when you submit HTTP requests to the server.
				The ajax function returns an object that implements the Promise interface. The object returned from the ajax function has several functions, which include:
					• done. This function is triggered when the call completes successfully.
					• fail. This function is triggered when the call completes with errors.
				When a call to a server completes, jQuery triggers one of the two callback functions based on the success of the call.
				Using the ajax function:
					Example:
						$.ajax({
							method: "GET",
							dataType: "json",
							url: "http://server-domain/api/Customer",
							data: {'id': '123'},
							contentType: "application/json; charset=utf-8"
						}).done(function(msg) {
							alert("Data received: " + msg);
						}).fail(function (msg) {
							alert(msg);
						});
		Client-Side Validation by using jQuery:
			In ASP.NET Core MVC applications, data validation can happen both on the client-side and the server-side. You could potentially leave the validation to the server-side, but server-side validation consumes more time. Server-side validation alone is inconvenient to the users because they will have to wait until the whole round trip to the server is complete in order to know whether their input is correct or not. Even though it might seem to be only a few fractions of a second, it adds up to be a lot of time and causes frustration.
			On the other hand, client-side validation alone is not enough. Any advanced user can easily bypass it and send any information they want to the server. That is why it is a best practice to have both client-side validation and server-side validation.
			Adding Client-Side Validations:
				To perform client-side validation in an ASP.NET Core MVC application, you need to add links to two JavaScript scripts, along with the jQuery library:
					• jQuery Validate. A jQuery plugin that makes client-side form validations easy. The plugin has a built-in bundle of useful validation methods, including URL and email validation. You can perform these validations before a form submit or after any user interaction.
					• jQuery Unobtrusive Validation. A custom Microsoft library that is built based on the jQuery Validate plugin. This plugin allows you to use the same validation logic you wrote on the server-side in the form of data annotations and metadata and apply it to the client-side immediately.
				Adding jQuery validation scripts to package.json:
					Example:
						{
						  "version": "1.0.0",
						  "name": "asp.net",
						  "private": true,
						  "devDependencies": {},
						  "dependencies": {
							"jquery": "3.3.1",
							"jquery-validation": "1.17.0”,    
							"jquery-validation-unobtrusive": "3.2.10”
						  }
						}
				Using scripts with npm:
					Example:
						<!DOCTYPE html>
						<html>
						<head>
							<meta name="viewport" content="width=device-width" />
							<title>@ViewBag.Title</title>
							<link href="~/css/site.css" rel="stylesheet" />
							<script src="../node_modules/jquery/dist/jquery.js"></script>
							<script src="~/node_modules/jquery-validation/dist/jquery.validate.min.js"></script>    <script src="~/node_modules/jquery-validation-unobtrusive/dist/jquery.validate.unobtrusiv e.min.js"></script>
						</head>
						<body>
							<div>
								@RenderBody()
							</div>
						</body>
						</html>
				When using jQuery Unobtrusive Validation, instead of writing validation logic in two places, you will write it once. MVC's Tag Helpers and HTML Helpers consume the validation attributes and type metadata from model properties and render HTML 5 attributes starting with data- on form elements needing validation. MVC will generate the data- attributes for both built-in and custom attributes.
				jQuery Unobtrusive Validation will then parse the data- attributes and passes the logic to jQuery Validate. This way you can duplicate the server-side validation logic to the client without writing the same logic twice.
				A model with validation attributes:
					Example:
						public class Customer
						{
							[Required(ErrorMessage = "Please enter your name")]
							public string Name { get; set; }
 
							[DataType(DataType.EmailAddress)]
							[Required(ErrorMessage = "Please enter your email address")]
							public string Email { get; set; }
						}
				Client-side validation by using jQuery unobtrusive validation:
					@model Customer
					<form method="post" asp-action="SomeAction">
						<div class="form-field">
							<label asp-for="Name"></label>
							<input asp-for="Name" />
							<span asp-validation-for="Name"></span>
						</div>
						<div class="form-field">
							<label asp-for="Email"></label>
							<input asp-for="Email" />
							<span asp-validation-for="Email"></span>
						</div>
						<div class="form-field">
							<input class="submit-btn" type="submit" />
						</div>
					</form>

Module 9: Client-Side Development
	Lesson 1: Applying Styles
		Introduction to Bootstrap:
			Bootstrap is an open-source framework for building responsive web applications using HTML, CSS, and JavaScript. It allows you to quickly develop the client-side of your application by using pre-built components and styles.
			Adding Bootstrap to Your Project:
				To add Bootstrap to your application, you should add the following dependencies:
					• jQuery. Bootstrap is based on jQuery. To use Bootstrap components, you should include jQuery in your application.
					• popper.js. Another Bootstrap dependency. This script library allows you to manage elements that "pop out" of the natural flow of your application.
				After adding the dependencies, you should also include bootstrap.css, which includes the styles and bootstrap.js, which includes the scripts. If you are using the production environment, include the minified versions.
				Adding Bootstrap by using npm:
					Example:
						{
						  "version": "1.0.0",
						  "name": "asp.net",
						  "private": true,
						  "dependencies": {
							"bootstrap": "4.1.3",
							"jquery": "3.3.1",
							"popper.js": "1.14.3"
						  },
						  "devDependencies": {   
						  }
						}
				Link a Layout to Bootstrap:
					Example:
						<!DOCTYPE html>
						<html>
						<head>
							<meta name="viewport" content="width=device-width" />
							<link href="~/node_modules/bootstrap/dist/css/bootstrap.css" rel="stylesheet" />
							<title>@ViewBag.Title</title>
						</head>
						<body>
							<div>
								@RenderBody()
							</div>
							<script src="~/node_modules/jquery/dist/jquery.js"></script>    
							<script src="~/node_modules/popper.js/dist/umd/popper.js"></script>    
							<script src="~/node_modules/bootstrap/dist/js/bootstrap.js"></script>
						</body>
						</html>
		Bootstrap Components:
			Bootstrap includes many components that can be added to your web application. Some of these components are composed of pure HTML and CSS markup and some components need JavaScript code to work.
			To make Bootstrap components work, it is important to remember that each component has a very specific HTML structure and a corresponding CSS code. If you create an HTML structure that is different from the one that Bootstrap expects, your website might not look as you wanted.
			Alerts:
				Alerts are used to display messages to users as a result of their actions. Bootstrap alerts are responsive and can include links inside them.
				To create an alert, you can add a div to your view and apply two CSS classes to this div. The first CSS class is alert, which defines this div as an alert. The second class that needs to be added will define the alert type and color. For instance, the alert-danger class defines the alert with a red color scheme. Other alert classes include alert-primary, alert-secondary, alert-success, alert-warning, alert-info, alert-light and alert-dark. You can add a role=”alert” attribute to the div to appropriately convey its purpose to assistive technologies such as screen readers. To create a link inside an alert, add an <a> tag and give it a CSS class of alert-link.
				https://localhost:44395/Bootstrap/Alerts
			Buttons:
				Buttons are used to create styled action elements inside forms, alerts, tables, grids and more. Bootstrap includes several predefined styles for buttons with multiple size options and states.
				To create a button, you can add a button HTML element to your view and apply CSS classes to it. A mandatory CSS class is btn, which applies Bootstrap design for buttons to this element. You can then apply several CSS classes that define the button characteristics. These CSS classes include:
					• Button color classes: btn-primary, btn-secondary, btn-success, btn-danger, btn-warning, btn-info, btn-light, btn-dark.
					• Button with outline: btn-outline-primary, btn-outline-secondary, btn-outline-success, btn-outline-danger, btn-outline-warning, btn-outline-info, btn-outline-light, btn-outline-dark.
					• Button size classes: btn-lg, btn-sm.
					• Button state classes: active, disabled.
				https://localhost:44395/Bootstrap/Buttons
			Dropdowns:
				Dropdowns are used to create lists of links that overlay the content of the page when they are shown. Dropdowns are togglable and interactive.
				To create a dropdown list, you can add a div with the dropdown class, which will include all the elements needed to be displayed. Then, you can add a button or link that when clicked will toggle the display of the dropdown list. This button can have all the classes that were covered above in the description of Bootstrap buttons. Also, this button needs to have the dropdown-toggle class and the data-toggle=”dropdown” attribute that will allow it to toggle the display of the dropdown list.
				After the button, you need to add a div with the dropdown-menu class. This div will include all the links and items displayed inside the dropdown list. An item inside the dropdown list should have the dropdown-item class. To separate items in a dropdown list, you can add a div with the dropdown-divider class.
				https://localhost:44395/Bootstrap/Dropdowns
			Forms:
				Bootstrap includes styles for many types of form controls that can be used practically in any web application. Bootstrap form controls include: textual form controls such as <input type="text">and <textarea>, password field, checkbox, radio button, select, file input, range input and more.
				To get the desired look, it is important to follow the required hierarchy of the HTML elements and the CSS classes. For example, to create an input element for some text field that has a label, the label and input HTML elements should be under a div with the form-group class. Each input control should have the form-control class.
				https://localhost:44395/Bootstrap/Forms
			Navs:
				Bootstrap has a large variety of responsive and mobile friendly navigational controls. They can allow you to add navigation menus easily. All navigational controls share the nav class. Each item in the nav menu should have the nav-item class. Each link inside the menu item should have the nav-link class. The disabled class should be used on disabled links and the active class should be used on the currently active link.
				https://localhost:44395/Bootstrap/Navs
			Navbars:
				A navbar is a Bootstrap responsive navigation header that can include the website name, menu and more. To get started with navbars, it is important to be familiar with the following:
					• Each navbar should be wrapped with at least two classes: navbar and a class compound of navbar-expand and one of the following endings: -sm, -md, -lg, or -xl. For example, navbar-expand-sm or navbar-expand-md and so on.
					• Each navbar wrapper can have a class that defines its color scheme, for example, navbar-light or navbar-dark. You can also customize the color schema with bg-* classes, for example, bg-light.
					• Navbar comes with pre-built sub-components which include:
						o navbar-brand. Can be used to display the name of the company or website.
						o navbar-nav. Can be used for navigation.
						o navbar-toggler. Can be used for toggling. You should also add the data-toggle=”collapse” attribute and the data-target=”#id” attribute, where id stands for the element needed to be toggled. In case you want the icon to look like a hamburger, you can use the navbar-toggler-icon class.
						o collapse and navbar-collapse. Can be used to group and hide navbar contents.
				https://localhost:44395/Bootstrap/Navbars			
			Modals:
				Bootstrap modal windows are used to display dialog messages and user notifications inside your application. They are placed over the site content and can be closed by clicking on the backdrop of the modal. Only one modal window can be shown at a time.
				To trigger a modal window, you can apply two attributes to a button. The first attribute is data-toggle=”modal”, which opens the modal window. The second attribute is data-target=”#id” where id stands for the id of the modal.
				The modal can be a div that has a modal class. To improve accessibility for people that use screen readers you can apply the role=”dialog” attribute to the div. You need to use the modal-dialog class to properly set the width and margin of the modal properly.
				To set the styles of the content of the modal, you need to use the model-content class. The modal content can have several parts which include:
					• modal-header. Can be used to style the header of the modal.
					• modal-body. Can be used to style the body of the modal.
					• modal-footer. Can be used to style the footer of the modal.
				https://localhost:44395/Bootstrap/Modals
		Styling Applications with Sass:
			There is an expectation today that websites should look good and provide an excellent user experience. This expectation has led developers to write an extensive amount of CSS. A need was born to find a method to make CSS easier to define and maintain. This is how the CSS preprocessors were born.
			A preprocessor is a language that is compiled to another language. The goal of a preprocessor is to improve the experience of working with the underlying language.
			A CSS preprocessor is a language that is compiled to CSS. Sass and Less are examples of CSS preprocessors. They allow you to add features to CSS, such as variables and functions which improve the maintainability of large and complex applications.
			By adding these features, you allow CSS to be more like a real programming language and this can help you to reduce the duplication of code and provide a better organization of your styles.
			Using Sass in your application:
				You can add a Sass file to your application by adding a file ending with .scss extension to your project. You will then need to compile the Sass file to a CSS file.
				To compile a Sass file, you can use several techniques, which include:
					• Install Sass globally by using npm and then using the sass command.
					• Configure a task runner.
				Installing Sass globally:
					Example:
						npm install -g sass
				Compiling a Sass File:
					Example:
						sass main.scss main.css
			Variables:
				Variables are one of the basic features Sass adds to CSS. Variables are a fundamental part of any programming language and now you can use them with CSS. Sass variables are often used to define colors, fonts, dimensions and any values that are repeated multiple times throughout the application. This allows you to define theme variables once and then inject them where needed.
				Variables help you to create maintainable CSS. When you need to change the general theme of your website, you will mainly need to change the values of your variables without changing the rest of the code.
				Using Sass Variables:
					Example:
						$highlights : #00FFFF;
						.message {
						  color: $highlights;
						  font-size: 12px;
						}
						h1 {
						  color: $highlights;
						}
				Sass Variables After Compilation:
					Example:
						.message {
						  color: #00FFFF;
						  font-size: 12px;
						}
						h1 {
						  color: #00FFFF;
						}
			Nesting Styles:
				CSS doesn’t support nested hierarchy of selectors. When working with Sass, you can define styles by using nested selectors and use a syntax that is more natural and easier to read.
				Using Nested Selectors:
					Example:
						.p {
							color: #000;
							font-size: 16px;
							span {
							color: #FF0000;
							em {
								text-decoration: underline;
								}
							}
						}
				CSS Output After Compilation:
					Example:
						.p {
						  color: #000;
						  font-size: 16px;
						}
						.p span {
						  color: #FF0000;
						}
						.p span em {
						  text-decoration: underline;
						}
			Mixins:
				Mixins are one of the most powerful and time-saving features of Sass. Mixins allow you to group CSS declarations and reuse them throughout your code. This feature can help you to reduce the amount of code and avoid duplication.
				Using a Mixin:
					Example:
						/* Define a mixin */
						@mixin normalized-text() {
						  font-weight: 300;
						  line-height: 1.2;
						}
						/* Use a Mixin in multiple places */
						.description {
						  @include normalized-text();
						  color: red;
						}
						.comment {
						  @include normalized-text();
						  color: grey;
						}
				CSS Output After Compilation:
					Example:
						.description {
						  font-weight: 300;
						  line-height: 1.2;
						  color: red;
						}
						.comment {
						  font-weight: 300;
						  line-height: 1.2;
						  color: grey;
						}
				Mixins in Sass act as functions - they allow you to pass in parameters and return sets of CSS properties. This allows mixins to be more flexible.
				Pass a Parameter to a Mixin:
					Example:
						/* Define a mixin */
						@mixin normalized-text($color) {
						  font-weight: 300;
						  line-height: 1.2;
						  color: $color; 
						}
						/* Use a Mixin in multiple places with different parameters */
						.description {
						  @include normalized-text(red);
						}
						.comment {
						  @include normalized-text(grey);
						}
				CSS Output After Compilation:
					Example:
						.description {
						  font-weight: 300;
						  line-height: 1.2;
						  color: red;
						}
						.comment {
						  font-weight: 300;
						  line-height: 1.2;
						  color: grey;
						}
			Control Directives:
				Control directives allow you to include certain styles only under specific conditions. They can be used inside mixins to include the same style with variations. Some of the control directives available in Sass are: @if, @else, @else if, @each and @for.
				Using Control Directives:
					Example:
						@mixin season($season) {
						  @if ($season == summer) {
							color: red;
						  }
						  @else if ($season == autumn) {
							color: orange;
						  }
						  @else if ($season == winter) {
							color: grey;
						  }
						  @else {
							color: green;
						  }
						}
						.description {
						  @include season(summer);
						}
				CSS Output After Compilation:
					Example:
						.description {
						  color: red;
						}
			Functions:
				Sass exposes a wide range of functions that allow you to perform manipulations on styles such as increase or decrease the lightness of a given color, perform mathematic calculations such as round and ceil and more.
				Following are examples of some functions available in Sass:
					• lighten. Increases the lightness of a color by a certain percentage. For example, lighten(#fe6700,20%) will increase the lightness of a certain orange hue by 20% percent.
					• darken. Increases the darkness of a color by a certain percentage. For example, darken(#fe6700,20%) will increase the darkness of a certain orange hue by 20% percent.
				Using Functions:
					Example:
						$main-color: #80E619;
						.main-headline {
						  color: darken($main-color, 20%);
						}
						.sub-headline {
						  color: lighten($main-color, 20%);
						}
				CSS Output After Compilation
					Example:
						.main-headline {
						  color: #4d8a0f;
						}
						.sub-headline {
						  color: #b3f075;
						}
		Styling Applications with Less:
			Less is another CSS preprocessor that shares many features with Sass while providing some of its own. Like Sass it enhances CSS with variables, mixins, functions and more. While many functionalities are shared, the syntax is usually different.
			Using Less in your application:
				You can add a Less file to your application by adding a file ending with .less extension to your project. You will then need to compile the Less file to a CSS file.
				To compile a Less file, you can use several techniques, which include:
					• Install Less globally by using npm and then using the lessc command.
					• Configure a task runner.
				Installing Less globally:
					Example:
						npm install -g less
				Compiling a Less File:
					Example:
						lessc main.less main.css
			Variables:
				Variables allow you to store information that you want to repeat throughout your code. Less variables behave like Sass variables with a slight change of syntax. To define a new variable in Less you will use the @ sign.
				Using Less Variables:
					Example:
						@highlights : #00FFFF;
						.message {
							color: @highlights;
							font-size: 12px;
						}
						h1 {
							color: @highlights;
						}
				CSS Output After Compilation:
					Example:
						.message {
						  color: #00FFFF;
						  font-size: 12px;
						}
						h1 {
						  color: #00FFFF;
						}
			Nesting Styles:
				Nesting is a feature of Less that allows defining styles by using nested code. The syntax of Less nested selectors is the same as in Sass.
				Using Nested Selectors:
					Example:
						.p {
						  color: #000000;
						  font-size: 16px;
						  span {
							color: #FF0000;
							em {
							  text-decoration: underline;
							}
						  }
						}
				CSS Output After Compilation:
					Example:
						.p {
						  color: #000000;
						  font-size: 16px;
						}
						.p span {
						  color: #FF0000;
						}
						.p span em {
						  text-decoration: underline;
						}
			Mixins:
				Mixins in Less behave and are declared differently than in Sass. Less mixins allow you to mix in values from one class into another class. This is a very important feature that can reduce the amount of code you have to write and to avoid duplication.
				CSS Without Mixins:
					Example:
						.description {
						  font-weight: 300;
						  line-height: 1.2;
						  color: red;
						}
						.comment {
						  font-weight: 300;
						  line-height: 1.2;
						  padding: 20px;
						  color: grey;
						}
				Using a Mixin:
					Example:
						/* Define a class */
						.normalized-text {
						  font-weight: 300;
						  line-height: 1.2;
						}
						/* Use a mixin in multiple places */
						.description {
						  .normalized-text();
						  color: red;
						}
						.comment {
						  .normalized-text();
						  padding: 20px;
						  color: grey;
						}
			Functions:
				Less exposes a wide range of functions that allow you to perform manipulations on styles such as increase or decrease the lightness of a given color, get the dimension of an image in a specific URL, convert one unit to another, perform mathematic calculations such as round and ceil and more. Some methods might be like the ones that exist in Sass.
				Following are examples of some functions that available in Less:
					• lighten. Increases the lightness of a color by a certain percentage. For example, lighten(#fe6700,20%) will increase the lightness of a certain orange hue by 20% percent.
					• darken. Increases the darkness of a color by a certain percentage. For example, darken(#fe6700,20%) will increase the darkness of a certain orange hue by 20% percent.
					• image-size. Retrieves the width and height of a certain image. For example, if the values of the width and the height of the image.png file are 150px, calling image-size("image.png") will return 150px 150px.
					• convert. Converts from one unit to another. For example, convert(9s, "ms") will convert 9s to 9000ms.
				Using Less Functions:
					Example:
						@main-color: #80E619;
						.main-headline{
						  color: darken(@main-color, 20%);
						}
						.sub-headline{
						  color: lighten(@main-color, 20%);
						}
				CSS Output After Compilation:
					Example:
						.main-headline {
						  color: #4d8a0f;
						}
						.sub-headline {
						  color: #b3f075;
						}
	Lesson 2: Using Task Runners
		Using Grunt:
			Grunt operates by having each task run separately of every other task. Grunt itself only supports the basic infrastructure for running tasks and you will need to add additional plugins or configure the logic for the tasks in order to support them. As a result, Grunt supports a large number of additional task libraries each of which can be run as required, while avoiding having to deal with a bloated environment full of unnecessary code, and you can run a very bare environment by using only the plugins that you need.
			Being a client-side task runner, Grunt will mainly be used to perform operations on client-side code. This can include things like performing minification (compressing JavaScript or CSS code to reduce space requirements, and therefore improve loading times), bundling (merging several different JavaScript or CSS files into one file, in order to reduce the number of requests), compiling nonstandard file types (such as Sass or TypeScript) and more.
			Configuring the Environment:
				The first step required to start working with Grunt is to configure the environment with the necessities to hold a Grunt environment. To do this, you will first need to add Grunt as part of the npm configuration. This can be done inside the package.json file by adding Grunt to the devDependencies section.
				Grunt in package.json:
					Example:
						{
						  "version": "1.0.0",
						  "name": "asp.net",
						  "private": true,
						  "devDependencies": {
							"grunt": "1.0.2"
						  }
						}
				After adding Grunt to the package.json file, you need to add a new js file in the base folder of the project. This file should be named Gruntfile.js. Inside the Gruntfile.js file you need to create a wrapper function in a specific format, every single Gruntfile.js requires it, and cannot be used without it.
				Sections of Gruntfile.js:
					The wrapper inside the Gruntfile.js file is the external shell in which all of the Grunt code will run. There can be several different sections inside the wrapper function, which include:
						• Project and task configuration information. The project and task configuration are handled inside the grunt.initConfig method. In here it is possible to configure various tasks and their behavior.
						• Loading the Grunt plugins and tasks. Loading the Grunt plugins is done by using the grunt.loadNpmTasks method. This method accepts the name of a plugin which is then loaded. Once the plugin is loaded, it is possible to run the task.
						• Custom tasks which are created by the user. This can range from a task designed to run several other tasks all the way to creating custom tasks with logic that is relevant for the specific application.
				Adding a Grunt Plugin to package.json:
					Example:
						{
						  "version": "1.0.0",
						  "name": "asp.net",
						  "private": true,
						  "devDependencies": {
							"grunt": "1.0.2",
							"grunt-sass": "2.1.0"
						  } 
						}
				Once the plugin is added to and downloaded by npm, you need to configure it in the grunt.initConfig method. The grunt.initConfig method receives an options parameter which is used to configure tasks. 
				To compile files from a folder, you will need to first update the dist property (dist is a shorthand for distribute) and underneath to set the files property which is used to determine which files are copied.
				The files property can accept an array of objects which can be used to declare the logic for Sass compilation, so that you can potentially copy from multiple sources into multiple destinations, or into the same destination.
				Each object in the array is defined in the following format which is a standard Grunt format which can be used across many Grunt plugins:
					• expand. When set to true, signifies working with directories rather than files.
					• cwd. Short for current working directory, denotes the folder from which to run the script.
					• src. Short for source, an array of regular expressions to match the files. Usually for Sass files [‘**/*.scss’] or [‘**/*.sass’] will be used.
					• dest. Short for destination, determines the folder to which Sass files will be compiled.
					• ext. Short for extension, determines under what file extension complied files will be saved. Usually .css for Sass.
				Gruntfile.js with Sass Configuration:
					Example:
						module.exports = function(grunt) {
							grunt.initConfig({
								sass: {
									dist: {
										files: [{
											expand: true,
											cwd: 'Styles',
											src: ['**/*.scss'],
											dest: 'wwwroot/Styles',
											ext: '.css'
										}]
									}
								}
							});    
						};
						The compilation for Sass is being configured from a directory (expand: true).
						The directory you will compile from is Styles (cwd: ‘Styles’).
						You will compile all files with the extension .scss inside of the folder (src: [‘**/*.scss’]).
						The compiled files will be deployed to the wwwroot/styles folder (dest: ‘wwwroot/styles’).
						The files will be deployed with the file extension css (ext: ’.css’).
				Finally, once you have configured the parameters for the task, you will need to load it by calling the grunt.loadNpmTasks method, and supply grunt-sass as a parameter to the loading function.
				Once you have completed this, you should be able to see the new task inside the Visual Studio Task Runner Explorer window and you can run the task by right-clicking it and clicking on Run.
				Sass Compilation Task Configuration:
					Example:
						module.exports = function(grunt) {    
						grunt.initConfig({
							sass: {
								dist: {
									files: [{
										expand: true,
										cwd: 'Styles',
										src: ['**/*.scss'],
										dest: 'wwwroot/styles',
										ext: '.css'
									}]
								}
							}
						});    
						grunt.loadNpmTasks("grunt-sass");
					};
			Registering a Custom Task:
				In addition to using existing plugins in the creation of tasks, you can also create your own tasks. Common tasks you may wish to create could involve various behaviors you desire such as adding tasks for logging or combining several tasks into one, which is also known as an alias task.
				In order to create a task you can use the grunt.registerTask method. The basic syntax for creating a task is grunt.registerTask(*task name*, [*task description*], *task function*) with the description being optional. This allows creating a task which when it is run calls for a function.
				Register a Custom Task:
					Example:
						module.exports = function(grunt) {
							grunt.registerTask('myTask', 'Saying hello', function() {
								grunt.log.write('Hello world...').ok();
							});
						};
				Occasionally, you may wish for multiple tasks to occur at a time. In order to implement this, you will once again use the grunt.registerTask method, however this time, the syntax you will use is grunt.registerTask(*alias task name*, [*task description*], *task list*), with the description being optional, and is often omitted entirely. Tasks like this are known as alias tasks, and in Visual Studio Task Runner Explorer they appear as such. By running this task all tasks in the list are executed in order.
				Alias Task:
					Example:
						module.exports = function(grunt) {
							grunt.initConfig({
								sass: {
									dist: {
										files: [{
											expand: true,
											cwd: 'Styles',
											src: ['**/*.scss'],
											dest: 'wwwroot/styles',
											ext: '.css'
										}]
									}
								}
							});
 
							grunt.loadNpmTasks("grunt-sass");
 
							grunt.registerTask('myTask', 'Saying hello', function() {
								grunt.log.write('Hello world...').ok();
							});
 
							grunt.registerTask('myMultipleTasks', ['myTask', 'sass']);
						};
				Binding Tasks to Visual Studio Events:
					A powerful feature of Grunt is the ability to bind tasks to run alongside Visual Studio events. This is useful since it allows running tasks before a build occurs, allowing you to update any compiled files inside the project. Thus, you can run the application in a convenient fashion without needing to manually run tasks each time you update.
					Alternatively, if you have tasks which you wish to run when the project is open, you can bind them to the project being opened and have the tasks run whenever you start Visual Studio.
					To bind a task, you can right-click it in the Task Runner Explorer and select a binding to use. You can later change the order, or delete bindings, by right-clicking a task in the binding sub-window of the Task Runner Explorer.
			https://docs.microsoft.com/en-us/aspnet/core/client-side/using-grunt
		Using gulp:
			Another task runner that you can use is gulp. Unlike Grunt, a gulp task can perform several sequential operations on a single pipeline, leading to a very different development approach from Grunt.
			Similarly to Grunt, gulp also supports only a very basic infrastructure for running tasks and additional plugins will need to be loaded in order to perform most operations. Configuration for gulp files is handled inside the tasks themselves, and since tasks run on a single pipeline, configuration often ends up being relatively simple. gulp supports a large number of plugins, which can grant gulp various different functionalities which can then be used inside of the tasks. This enables running a lean task environment where only required plugins are added.
			gulp is intended to be used for client-side tasks, and is often used for bundling, minification, and compilation for various client-side technologies.
			Configuring the Environment:
				As the first step in the process, you need to add gulp inside the devDependencies section of the npm configuration file, package.json. This will load up the JavaScript files required by gulp and will allow working with it.
				gulp Dependency:
					Example:
						{
						  "version": "1.0.0",
						  "name": "asp.net",
						  "private": true,
						  "devDependencies": {
							"gulp": "3.9.1"
						  }
						}
				After the dependency has been loaded, a gulpfile.js file should be created. Task configuration will be done inside the gulpfile.js file.
				Inside gulpfile.js you should load up gulp by calling the require(‘gulp’) function. The require function enables you to request that the gulp dependency will be loaded in the context of the current file. Doing this will enable you to start writing tasks.
				Writing a Task:
					After setting up the environment it is possible to start creating gulp tasks. Unlike Grunt which tends to use tasks that are created in advance, and then configure them, in gulp, the configuration will usually be done as part of a created task. Therefore, every task that is run in gulp will always act as a custom task.
					To add additional functionalities, you will need to load additional plugins. The first step is to add plugins to the devDependencies section of the package.json file. This will set npm to install them into the application. Afterwards, you will need to load them from inside the gulpfile.js file. This can be done by using the require method and providing it with the name of the plugin which you need to load.
					Adding a gulp plugin to package.json:
						Example:
							{
							  "version": "1.0.0",
							  "name": "asp.net",
							  "private": true,
							  "devDependencies": {
								"gulp": "3.9.1",
								"gulp-sass": "4.0.1"
							  }
							}
					After the gulp-sass plugin has been loaded, it will be available to use inside tasks.
					By convention, all file configurations should be defined at the top of the gulpfile.js file, directly under the section for loading all plugins by using the require method. This is done so that you will be able to easily change them when the requirement changes. It is particularly helpful when using shared configurations across several tasks. However, this segment is entirely optional. Examples of relevant configurations could include file paths, task names, important strings and numbers and more. There isn’t any one correct structure for configurations so you should create a JavaScript object that works for your requirements.
					gulpfile.js Configuration:
						Example:
							var gulp = require('gulp');
							var sass = require('gulp-sass');
							var paths = {    webroot: "./wwwroot/"}; paths.sass = "./Styles/*.scss";paths.compiledCss = paths.webroot + "styles/";
						A shared path configuration is created, which sets the webroot property to hold the path to the wwwroot folder. Additionally, it is then used to create a path to where the Sass files are stored and the destination for compiled Sass files.
						After the setup is completed, a task can be created by using the gulp.task function. This function uses the following syntax: gulp.task(*task name*, [*dependency tasks*], *task function*). The task name being the name of the task which will be created, dependency tasks is a list of other tasks which will be run as part of this task, and the task function is the code which will be run in the task. Both the dependency tasks and the task function are optional parameters. You can choose to use one or the other or both as needed.
						Inside the gulp.task function you can add your code and use it to perform any tasks you wish. Commonly gulp functions will perform various operations by creating a memory stream, writing into it, and finally closing it turning it into a read-only stream. By using the pipe method on the stream, it will write the current stream into the stream created by the following function, allowing it to continue working from the point at which the last function finished. The pipe function receives a method as a parameter which will receive the stream from the previous method in the pipeline.
						You will often begin gulp tasks by invoking the gulp.src method. The first parameter of this method is a regex file path or potentially a collection of regex file paths. This allows loading one or even more files as needed to perform your desired operation. You can then use the pipe function to continue processing the files.
						At the end of the process, the gulp.dest method will frequently be called to save the stream that was previously created into files. It receives a folder into which it will write all files currently stored inside the memory stream. It is usually the last method in the pipeline, however, you can still use the pipe function to continue using the stream.
						When loading the gulp-sass plugin, it is also possible to use the sass method. By using a stream that it receives from the pipe, the sass method compiles Sass files into CSS files and returns a stream which can be used with the pipe method to continue the process. It is also possible to attach to the on event.
					gulp Sass Compilation:
						Example:
							var gulp = require('gulp');
							var sass = require('gulp-sass');
							var paths = {
								webroot: "./wwwroot/"
							};
							paths.sass = "./Styles/*.scss";
							paths.compiledCss = paths.webroot + "styles/";
							gulp.task("sass", function() {
								return gulp.src(paths.sass)
									.pipe(sass().on('error', sass.logError)) 
									.pipe(gulp.dest(paths.compiledCss));
							});
			Running Multiple Tasks:
				By providing the gulp task function with a list of task names you can run several tasks at once. When running multiple tasks in gulp, each task will be run in parallel to the other. This means that tasks which need to run in a specific order will need to be handled specifically to run in that order. It is also possible to optionally run an additional code by adding a function after the task. This code will only execute once all other tasks have been completed. This can allow setting up chains of tasks running in order.
				Multiple Tasks in gulp:
					Example:
						var gulp = require('gulp');
						gulp.task("one", function() {
							console.log("one");
						});
						gulp.task("two", function() {
							console.log("two");
						});
						gulp.task("three", function() {
							console.log("three");
						});
 						gulp.task("all", ["one", "two", "three"], function() {
							console.log("four");
						});
				Binding Tasks to Visual Studio Events:
					As with Grunt, it is possible to bind gulp tasks to Visual Studio events. This can be done in the same way, by right-clicking a task name in the Task Runner Explorer window and selecting an event to which you can bind.
			https://docs.microsoft.com/en-us/aspnet/core/client-side/bundling-and-minification
		Bundling and Minification:
			One of the most frequent usages of task runners is to perform bundling and minification for the client side. In this topic, you will learn how you can perform bundling and minification by using gulp and the Visual Studio Task Runner Explorer.
			A big advantage to bundling and minification is that by reducing the number of files (bundling), it is possible to ensure that less file requests are made to the server and by reducing the size of files (minification), requests to the server occur faster. Both of these help web applications become dynamic and responsive, reducing the potential of a user becoming frustrated by the application for being “slow”.
			To perform bundling and minification, the following gulp plugins can be added:
				• gulp-concat. Used for combining multiple files performing a bundling operation.
				• gulp-cssmin. Minifies CSS files.
				• gulp-uglify. Minifies JavaScript files.
			To add these gulp plugins to a web application, you can add them to the devDependencies section of the npm configuration file, package.json.
			Adding gulp Plugins to package.json:	
				Example:
					{
					  "version": "1.0.0",
					  "name": "asp.net",
					  "private": true,
					  "dependencies": {
						"bootstrap": "4.1.3",
						"jquery": "3.3.1",
						"popper.js": "1.14.3"
					  },
					  "devDependencies": {
						"gulp": "3.9.1",
						"gulp-sass": "4.0.1",
						"gulp-concat": "2.6.1",
						"gulp-cssmin": "0.2.0",
						"gulp-uglify": "3.0.1"
					  }
					}
				By using these plugins, you will be able to perform bundling and minification. It is crucial to note that the logic for minifying CSS and JavaScript files is different, necessitating separate minification plugins. However, since gulp-concat only puts files together and renames them it is possibly to use it to bundle both.
			An Example of CSS File Minification:
				Example:
					var gulp = require('gulp');
					var concat = require("gulp-concat");
					var cssmin = require("gulp-cssmin");
					var paths = {
						webroot: "./wwwroot/",
						nodeModules: "./node_modules/"
					};
					paths.bootstrapCss = paths.nodeModules + "bootstrap/dist/css/bootstrap.css";
					paths.vendorCssFileName = "vendor.min.css";
					paths.destinationCssFolder = paths.webroot + "styles/";
					gulp.task("minify-vendor-css", function() {
						return gulp.src(paths.bootstrapCss)
							.pipe(concat(paths.vendorCssFileName))
							.pipe(cssmin())
							.pipe(gulp.dest(paths.destinationCssFolder));
					});
				The bootstrap.css file is read from the node_modules folder and saved as vendor.min.css by calling the concat function. Then the CSS content inside it is minified by calling the cssmin function. Finally, the file is written into the styles folder of the wwwroot folder. At this point, the file can easily be linked to from a layout or a view. A major advantage of this approach is that if new CSS libraries are added, they can easily be added to the gulpfile.js without affecting the client.
			Sass Preprocessing:
				Example:
					var gulp = require('gulp');
					var concat = require("gulp-concat");
					var cssmin = require("gulp-cssmin");
					var sass = require('gulp-sass');
					var paths = {
						webroot: "./wwwroot/",
						nodeModules: "./node_modules/"
					};
					paths.sassFiles = "./Styles/*.scss";
					paths.compiledCssFileName = "style.min.css";
					paths.destinationCssFolder = paths.webroot + "styles/";
					gulp.task("minify-sass", function() {
						return gulp.src(paths.sassFiles)
							.pipe(sass().on('error', sass.logError))
							.pipe(concat(paths.compiledCssFileName))
							.pipe(cssmin())
							.pipe(gulp.dest(paths.destinationCssFolder));
					});
				All .scss files inside of the application styles folder are read. Next, all of the .scss files are compiled into regular CSS files. Then, the CSS files are combined to a single file named style.min.css. Afterwards, the combined CSS file is minified. Finally, the file is written into the styles folder of the wwwroot folder. At this point, you can link this file to the layout or a view and it will be fully usable.
			JavaScript Bundling and Minification:
				Example:
					var gulp = require('gulp');
					var concat = require("gulp-concat");
					var uglify = require("gulp-uglify");
					var paths = {
						webroot: "./wwwroot/",
						nodeModules: "./node_modules/"
					};
					paths.bootstrapjs = paths.nodeModules + "bootstrap/dist/js/bootstrap.js";
					paths.jqueryjs = paths.nodeModules + "jquery/dist/jquery.js";
					paths.vendorJsFiles = [paths.bootstrapjs, paths.jqueryjs];
					paths.vendorJsFileName = "vendor.min.js";
					paths.destinationJsFolder = paths.webroot + "scripts/";
					gulp.task("minify-vendor-js", function() {
						return gulp.src(paths.vendorJsFiles)
							.pipe(concat(paths.vendorJsFileName))
							.pipe(uglify())
							.pipe(gulp.dest(paths.destinationJsFolder));
					});
				This task begins by reading both bootstrap.js and jquery.js by using an array of file paths. The task then merges both files under the file name vendor.min.js, and then it minifies the resulting file by calling the uglify function. Finally, the file is written into scripts/vendor.min.js. In the future, should the need arise to add more JavaScript files, they can be added into the gulpfile.js without needing to update the layout or a view.
			Bundling and Minifying JavaScript Files:
				Example:
					var gulp = require('gulp');
					var concat = require("gulp-concat");
					var uglify = require("gulp-uglify");
					var paths = {
						webroot: "./wwwroot/",
						nodeModules: "./node_modules/"
					};
					paths.jsFiles = "./Scripts/*.js";
					paths.minifiedJsFileName = "scripts.min.js";
					paths.destinationJsFolder = paths.webroot + "scripts/";
					gulp.task("minify-js", function() {
						return gulp.src(paths.jsFiles)
							.pipe(concat(paths.minifiedJsFileName))
							.pipe(uglify())
							.pipe(gulp.dest(paths.destinationJsFolder));
					});
				Task that reads all files in the Scripts directory of the application, combines them into a file called scripts.min.js, minifies it and finally saves it in wwwroot inside the scripts folder, which is easily linked to from the layout or a view.
			Integrating with Build:
				It is a good idea for JavaScript and CSS files to be bundled and minified whenever a new build is running. This will enable using the latest versions of JavaScript and CSS files, providing a hassle-free environment and allowing complete focus on the coding aspect, while not having to deal with manually running tasks. To achieve this, you can create a singular task assigned to running all the separate subtasks.
				Multiple Tasks:
					Example:
						gulp.task("all-css", ["minify-vendor-css", "minify-sass"]);
						gulp.task("all-js", ["minify-vendor-js", "minify-js"]);
						gulp.task("minify-all", ["all-js", "all-css"], function () {
							console.log("All tasks completed!");
						});
					The first performs both CSS bundling and minification tasks and the second does the same for JavaScript. Meanwhile, the third task calls both and when all subtasks are done, it prints: All Tasks Completed to the task runner output.
					By binding the final task inside the task runner to build start, you can ensure that whenever the application is run, the latest version of files will be served and available.
			Watcher Tasks:
				An additional tool at your disposal is watcher tasks. Watcher tasks are tasks that are usually run when the project is loaded and continue running until it is closed. The purpose of watcher tasks is to watch for any changes occurring in files and if a change occurs, to run the appropriate task. For example, this can allow making real-time changes in the .scss and .js files and have the appropriate compilation, bundling, and minification occurs immediately. This can help reduce load during the build process and to allow testing client-side file changes without recompiling server-side code.
				A watcher is configured by using the gulp.watch() method. The first parameter that will be supplied to the watch is a file or directory regex or a list of such. The second parameter is an array of tasks which will be run by the watcher.
				Watchers are commonly bound to the project.open event of the Task Runner Explorer, allowing them to run in the background and perform their operations as the developer engages in their day to day work.
				gulp Watcher Tasks:
					Example:
						gulp.task("sass-watcher", function() {
							gulp.watch(paths.sassFiles, ["minify-sass"]);
						});
						gulp.task("js-watcher", function() {
							gulp.watch(paths.jsFiles, ["minify-js"]);
						});
					Watcher has been applied to the styles folder of the application, and another watcher to the scripts folder. When Sass code inside the styles folder is added, removed or changed, the minify-sass task will be run. Meanwhile, whenever a JavaScript file inside the application’s scripts folder is added, removed or changed, the minify-js task will be run.
	Lesson 3: Responsive Design
		Responsive web design is used to make sure your application looks good on any screen and device. It is an approach that says that the design should respond to users’ behavior based on screen size, platform, and orientation.
		The HTML5 Viewport Attribute:
			It is possible to customize your web application’s display based on the capabilities and specifications of a web browser or a device by using adaptive rendering.
			Mobile browsers such as Microsoft Edge use the viewport attribute to render webpages in a virtual window. This virtual window is usually wider than the application screen. The viewport attribute helps eliminate the need to reduce the size of the layout for each page. Reducing the size of the layout can distort the display of non-mobile-optimized web applications. Creating the application interface by using the viewport attribute enables users to zoom into the different areas of a webpage.
			The viewport tag is a meta tag that helps you to control the width and height of webpages.
			Using the viewport Tag in a View:
				Example:
					<!DOCTYPE html>
					<html>
					<head>
						<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
						<title>Index</title>
					</head>
					<body>
					</body>
					</html>
				The width and height properties help you to specify the width and height of the virtual viewport window in pixels. You can use the device-width keyword to enable the content to fit the default screen size of the browser.
				The initial-scale property controls the initial scale or zoom level of a webpage. You can use other properties such as maximum-scale, minimum-scale, and user-scalable to control how the user can zoom in or out of the page.
		CSS Media Queries:
			To support different browsers and devices, you may sometimes need to apply different CSS styles in your application. HTML5 includes CSS media queries, which are special selectors that begin with @media. Media queries allow conditional application of CSS styles based on the device conditions or browser capabilities. You can apply media queries in CSS and HTML.
			Using a Media Query:
				Example:
					@media only screen and (max-width: 500px) {
						header{
						  float: none;
						}
					  }
			Using a Media Query in the link Element:
				Example:
					<link rel="stylesheet" type="text/css" href="smallscreen.css" media="only screen and (max-width: 500px)" />
			You can use CSS media queries to apply CSS styles when the screen size is less than 500 pixels. However, you can use CSS media queries only for the screen layout, but not the print layout.
			The following table describes properties that you can include in a media query.
				Property				Description
				width					The width of the targeted display area, which includes the browser window in desktop and mobile devices. In desktop computers, 
										when you resize the browser window, the width of the browser changes. However, on most mobile browsers, you cannot resize the 
										browser window. This implies that the width of the browser remains constant.
				height					The height of the targeted display area, which includes the browser window in desktop and mobile devices.
				device-width			The width of the entire screen of a device. For a desktop with a screen resolution of 1,024x768, the device-width is usually 1,024 
										pixels.
				device-height			The height of the entire screen of a device. For a desktop with a screen resolution of 1,024x768, the device-height is usually 768 
										pixels.
				orientation				The orientation of the device. If the device-width is larger than the device-height, the orientation value is set to landscape; 
										otherwise, it is set to portrait.
				aspect-ratio			The ratio of the width and height properties.
				device-aspect-ratio		The ratio of the device-width and device-height properties. The following example illustrates the device-aspect-ratio for a device 
										with a screen resolution of 1,280x720:
											@media screen and (device-aspect-ratio: 16/9) { }
											@media screen and (device-aspect-ratio: 1280/720) { }
											@media screen and (device-aspect-ratio: 2560/1440) { }
				color					The number of bits per color component of the device. If the device is not a color device, the value is zero.
				color-index				The number of entries in the color lookup table, of the output device.
				monochrome				The number of bits per pixel in a monochrome frame buffer. For non-monochrome devices, this value is zero.
				resolution				The resolution of the output device or the density of the pixels. The common units for this property include dpi and dpcm.
				scan					The scanning process of TV output devices.
				grid					The property that detects whether the output is in the grid or bitmap format. Grid-based devices return a value of one; all other devices 
										return a value of zero.
		The Bootstrap Grid System:
			Bootstrap includes a pre-built grid system that allows you to layout the content easily without writing any extra CSS code. Bootstrap allows you to divide the layout into up to 12 columns and display your content within them.
			In under the Bootstrap grid system the most basic layout element is a container. Each Bootstrap grid system should have a container. A container is an element with one of the following classes:
				• container. Use this class when you want to have a responsive container in which its min-width property is changed at predefined media query ranges called breakpoints. The following are the breakpoints which exist in Bootstrap:
					o xs. Used for extra small devices (portrait phones). min-width is 0px.
					o sm. Used for small devices (landscape phones). min-width is 576px.
					o md. Used for medium devices (tablets). min-width is 768px.
					o lg. Used for large devices (desktops). min-width is 992px.
					o xl. Used for extra-large devices (large desktops). min-width is 1200px.
				• container-fluid. Use this class when you want the grid to be spread across the full width of the viewport.
			In the container, you can create rows. A row is an element with the row class. Each row can include columns. A column is an element with the col class.
			The basic CSS classes that available for columns are: col-1, col-2, col-3, col-4, col-5, col-6, col-7, col-8, col-9, col-10, col-11, col-12. The number beside the col- defines how many grid columns the column will spread across.
			It is possible to set the Bootstrap grid system to behave differently based on the device on which the application is running. For small devices, use the col-sm class, for medium devices, use the col-md class, for large devices, use the col-lg class, and for extra-large devices, use the col-xl class.
			https://localhost:44395/Bootstrap/Grids
			Alignment:
				It is possible to vertically align the columns within the row in three main ways. To do so you need to add a class to the row element:
					• align-items-start. Aligns the columns to the top.
					• align-items-center. Aligns items to the center.
					• align-items-end. Aligns items to the bottom.
				Also, it is possible to align the items horizontally inside the row, using the following classes:
					• justify-content-start. Align columns to the start of the row.
					• justify-content-center. Align columns to the center of the row.
					• justify-content-end. Align columns to the end of the row.
					• justify-content-around. Spread the empty space around the columns.
					• justify content-between. Spread the empty space between the columns.
		Applying the Flexbox Layout:
			CSS flexbox is a module added to the CSS and is supported by all modern browsers. It provides an efficient way to layout and align items inside a specific parent container. The Bootstrap grid system, which was introduced earlier in this lesson, is based on CSS flexbox.
			Parent Container:
				Parent container in context of flexbox module is an HTML element that has other HTML elements within it and has the property display: flex applied. One of the main ideas of flexbox is that the parent container can alter its children items width, height, and order to fill the available space within it in diverse ways.
				Parent container properties are:
					• flex-direction: row | row-reverse | column | column-reverse;
						Defines the direction in which the container’s children flow.
					• flex-wrap: nowrap | wrap | wrap-reverse;
						Flexbox items will always try to fit in one row or column. You can change this behavior and allow the items to wrap to second line or column when the available space ends by using flexbox-wrap.
					• justify-content: flex-start | flex-end | center | space-between | space-around | space-evenly;
						Defines how the items are aligned on the main axis.
					• align-items: flex-start | flex-end | center | baseline | stretch;
						Defines how the items are laid on the cross axis on the baseline.
					• align-content: flex-start | flex-end | center | space-between | space-around | stretch;
				Similar to the justify-content but on the cross axis. Allows to align items inside a flex container when there is extra space on the cross-axis.
			Child Items:
				Each direct child inside the flex container can override the behavior specified by the container by using the properties described below:
					• order: -n…0…n;
						Allows to target individual items and change where they appear in the visual order. 
					• flex-grow: 0…n; default 0;
						This defines the ability for a flex item to grow if necessary. It dictates what amount of the available space inside the flex container the item should take up.
					• flex-shrink: 0…n; default 0;
						This defines the ability for a flex item to shrink if necessary. It determines how much the flex item will shrink relative to the rest of the flex items in the flex container.
					• flex-basis: length | auto;
						This defines the default size of an element before the remaining space is distributed. The flex grow and flex shrink are relative to flex basis.
					• align-self: flex-start | flex-end | center | space-between | space-around | stretch;
				This allows the default alignment (specified by align-items) to be overridden for individual child item.
			https://localhost:44395/Bootstrap/FlexBoxes

Module 10: Testing and Troubleshooting
	Lesson 1: Testing MVC Applications
		A unit test is a procedure that instantiates a component that you have written, calls one aspect of the functionalities of the component, and checks that the component responds correctly according to the design. In object-oriented programming, unit tests usually instantiate a class and call one of its methods. In an ASP.NET Core MVC web application, unit tests can instantiate model classes or controllers, and call their methods and actions.
		Why Perform Unit Tests?
			There are several types of tests you can use to identify bugs in your application, which include:
				• Unit tests. Unit tests check small aspects of functionality. A unit test may verify the return type of a single method, or the method is returning the expected results. By defining many unit tests for your project, you can ensure they cover all functional aspects of your web application.
				• Integration tests. Integration tests check how two or more components work together. You can use these tests to check how two or more classes interact with each other. You can also use integration tests to check how the entire web application, including the database and external web services, works to deliver content.
				• Acceptance tests: Acceptance tests focus on a functional or technical requirement that must work for the stakeholders to accept the application. Similar to integration tests, acceptance tests usually test multiple components working together.
			Unit tests are important because they can be defined early in development. Integration and acceptance tests are usually run later, when several components are approaching completion.
			What Is a Unit Test?
				A unit test is a procedure that verifies a specific aspect of functionality. Multiple unit tests can be performed for a single class and even for a single method in a class.
				A single unit test usually consists of code that runs in three phases:
					1. Arrange. In this phase, the test creates an instance of the class that it will test. It also assigns any required properties and creates any required objects to complete the test. Only properties and objects that are essential to the test are created.
					2. Act. In this phase, the test runs the functionality that it must check. Usually, in the Act phase, the test calls a single procedure and stores the result in a variable.
					3. Assert. In this phase, the test checks the result against the expected result. If the result matches what was expected, the test passes. Otherwise, the test fails.
			How Do Unit Test Help Diagnose Bugs?
				Because unit tests check a small and specific aspect of code, it is easy to diagnose the problem when the tests fail. Unit tests usually work with a single class and isolate the class from other classes wherever possible. If other classes are essential, the smallest number of classes are created in the Arrange phase. This approach enables you to fix issues rapidly because the number of potential sources of a bug is small.
				Unit tests should check the code that you write and not any infrastructure that the production system will rely on. Using this approach, you can distinguish bugs that arise from code, which must be fixed by altering code, from the bugs that arise from infrastructure failures, which must be fixed by changing hardware, reconfiguring web servers, reconfiguring connection strings, or making other configuration changes.
				By using dependency injection throughout the code instead of relying on explicit dependencies, the reliance on other classes and data sources can be bypassed, and it becomes possible to inject fake dependencies, allowing full testing of the class, without being hindered by external classes. Since the tested class expects interfaces, you can create fake dependent classes and implement various methods as needed with the expected results. This enables complete testing for a class, without reliance on other classes.
			Automated Unit Testing:
				It is important to understand that unit tests, after they are defined, can be rerun quickly and easily throughout the rest of the project life cycle. In fact, you can set up Visual Studio to rerun tests automatically whenever the related code changes. You can also manually initiate tests any time.
		Principles of Test-Driven Development:
			To spot potential bugs and improve the quality of the final application, you can use unit tests in any development methodology, including waterfall projects, iterative projects, and agile projects. Any project can benefit from unit testing, regardless of the development methodology used in the project. However, a specific development methodology, called test-driven development (TDD), places unit testing at the center of working practices. TDD is often described as a separate development methodology. Some authors consider it to be a core principle of the extreme programming methodology.
			Write the Test, Pass the Test, Refactor:
				In TDD, developers repeat the following short cycle to implement a functional requirement:
					1. Write the Test. The developer starts by creating and coding the unit test. This step requires a full understanding of the functional requirement, which can be obtained from use cases or user stories. Because the developer has not written any code in the application, the test fails.
					2. Pass the Test. The developer writes some quick and simple code in the application so that it passes the test. During the first iteration, this code is frequently inelegant and may include false assumptions such as hardcoded values.
					3. Refactor. The developer cleans up the application code, removes duplicate code, removes hardcoded values, improves readability, and makes other technical improvements. The developer reruns the test to ensure that refactoring has not caused a failure.
				The cycle is then repeated. In each iteration, the developer adds a small new element of the final functionality with a corresponding test.
				It is important that the code in your tests is treated as production code. It should be well thought out and easy to read so that other developers can understand the test and quickly diagnose any test failures.
				Test-Driven Development Principles:
					TDD is different from traditional approaches to application development. To use TDD effectively, you must understand its fundamental principles:
						• Write tests before code. In the TDD development cycle, you write the test before writing any code in the application. This means the test must fail the first time it is run. You can understand the test as a specification for the functionality you are building. By writing the test first, you ensure that you begin with a thorough understanding of the problem you are trying to solve.
						• Move in small steps. By breaking a large application down into small elements of functionality, you can improve developer productivity. You can do this by giving a developer a small problem to solve in each iteration. The developer can solve the simple problem quickly and understand all the possible circumstances in which their code runs.
						• Only write enough code to pass the test. In each iteration, do not be tempted to add extra code that is not related to the test. For example, if you know that users will call an action with other parameters than the ones in the test, do not write code to handle these parameters. Instead, during the next iteration, write a second test for those parameters. Then write and refactor that code.
					Developers can refer to tests as examples of how to use a class or method. This can increase developer productivity because developers can view the code that demonstrates the method or class.
		Writing Loosely Coupled MVC Components:
			A loosely coupled application is one in which each component requires few or no details of the other components of the system. In object-oriented programming, two classes can be described as loosely coupled if one class can call methods on the other class without any code that is specific to the other class. When system components are loosely coupled in this manner, it is easy to replace a class with another implementation of the same functionality. Loosely coupled components are essential for thorough unit testing because classes that deal with real data, such as data from a database, can easily be replaced with classes that deal with test data. When dependency injection is used inside an application, the application becomes more loosely coupled, since classes expect interfaces rather than classes.
			Using Loose Coupling in Tests:
				A loosely coupled application is easy to test because you can make tests simpler by replacing a fully functional instance of a class with a simplified instance that is specifically designed for the test. Replacing classes in tests in this manner can only be done when components are loosely coupled. Replacement instances used for unit tests are known as test doubles or fakes. A test double or fake includes just enough code and properties to pass the relevant test and prove the functionality.
			Other Benefits of Loose Coupling:
				Loose coupling has other benefits besides testing. Loose coupling makes it easier to replace simple components with more sophisticated ones. If the components are loosely coupled, you can perform this replacement without modifying any code outside of the averaging classes.
			Using Interfaces for Loose Coupling:
				In object-oriented programming, an interface defines a set of properties and methods. Any class that implements that interface must implement all the properties and methods it defines as a minimum. This creates loose coupling because you need not specify a class in code. Instead, you can specify any implementation of a particular interface.
		Writing Unit Tests for MVC Components:
			The ASP.NET Core MVC programming model is easy to integrate with the principles of unit testing and TDD because of its separation of concerns into model, controllers, and views, as well as its support of dependency injection. Models are simple to test because they are independent classes that you can instantiate and configure during the Arrange phase of a test. Controllers are simple classes that you can test, but it is complex to test controllers with in-memory data, rather than using a database. To test controllers with in-memory data, you must create a test double class, also known as a fake repository. Objects of this class can be populated with the test data in memory without querying a database. You need to understand how to write test doubles and how to write a typical test for MVC classes.
			Adding and Configuring a Test Project:
				In Visual Studio, you can test an ASP.NET Core MVC web application project by adding a new project to the solution, based on the MSTest Test Project (.NET Core) template. You must add a reference from the test project to the MVC web application project so that the test code can access classes in the MVC web application project. You should also install the Microsoft.AspNetCore.Mvc package in the test project.
				In a Visual Studio MSTest test project, test fixtures are classes marked with the TestClass attribute. Unit tests are methods marked with the TestMethod attribute. Unit tests usually return void but call a method of the Assert class, such as Assert.AreEqual to check results in the test Assert phase.
			Test Model Classes and Business Logic:
				In MVC, model classes do not depend on other components or infrastructure. You can easily instantiate them in-memory, arrange their property values, act on them by calling a method, and assert that the result was as expected. Sometimes, you create business logic in the model classes, in which case, the Act phase will involve calling a method on the model class itself. If, by contrast, you have created a separate business logic layer, code in the Act phase must call a method on the business object class and pass a model class.
			Testing Controller Classes:
				Unlike models, which are simple classes designed to be mainly self-sufficient without being reliant on other classes, controllers are more complex to test. To test controllers, you will need some additional work because an individual controller can potentially rely on models, repositories, and services and more.
			Creating a Repository Service:
				A repository is a common design pattern that is used to separate business logic code from data retrieval. This is commonly done by creating a repository interface that defines properties and methods that MVC can use to retrieve data. Usually, the repository handles the various CRUD operations on the data. In ASP.NET Core MVC applications, the repository is handled by creating a service. In general, you will want one repository per entity. Repositories are prominently useful for testing controllers because models usually do not use external data.
			Implementing and Using a Repository in the Application:
				The repository interface defines methods for interacting with data but does not determine how that data will be set and stored. You must provide two implementations of the repository:
					• A service implementation of the repository that will be used inside the application. This implementation will use data from the database or some other storage mechanism.
					• An implementation of the repository for use in tests. This implementation will use data from the memory set during the Arrange phase of each test.
			Implementing a Repository Test Double:
				The second implementation of the repository interface is the implementation that you will use in unit tests. This implementation uses in-memory data and a keyed collection of objects to function just like an Entity Framework context but avoids working with a database.
			Using the Test Double to Test a Controller:
				After you have implemented a test double class for the repository, you can use it to test a controller in a unit test. In the Arrange phase of the test, create a new object from the test double class and pass it to the controller constructor. The controller uses this object during the test Act phase. This enables you to check results.
			Running Unit Tests:
				Once you write a test, you can run it by right-clicking a test class, and then selecting the Run Test(s) option in the context menu. Alternatively, you can open the test explorer from the test menu inside the windows sub options. From the test runner explorer, you can then choose specific test classes or methods and run them as needed. There are also options to run all tests, or run tests while in debug mode, allowing you to observe specific failures and handle them accordingly.
			Live Unit Testing:
				When developing an ASP.NET Core application, it is possible to run Live Unit Testing. This allows you to run unit tests as you code, ensuring the integrity of your code without having to constantly run tests manually.
				https://docs.microsoft.com/en-us/visualstudio/test/live-unit-testing
		Using Mocking Frameworks:
			When you write a unit test, you must, during the Arrange phase, create test data on which the test will run. By using a test double or mock object to supply test data, you can test your code in isolation from other classes and from the infrastructure elements such as databases and network connections on which the application will rely. You can create test doubles by manually coding their instantiation, setting their properties, and populating collections. Such test doubles are known as manual mock objects.
			Alternatively, instead of creating mock objects manually with your own code, you can use a mocking framework to automate this work. A mocking framework automatically creates a mock object by using the interfaces that you specify. In the case of IoC containers, there are many mocking frameworks that can be used in testing MVC web applications. You can find many mocking frameworks by using the NuGet package manager.
			Popular mocking frameworks for ASP.NET Core MVC include Moq and NSubstitute.
			Why Use a Mocking Framework?
				There are many situations in which mocking frameworks can significantly ease unit testing. Even for simple tests, mocking frameworks reduce the amount of setup code that you have to develop. After you become familiar with the mocking framework that you choose and have learned how to write arrangement code for it, you will begin to save time. In more complex situations, such as the following, mocking frameworks have great advantages:
					• Creating multiple mock objects of a single type. You should try to keep each unit test as simple as possible, but inevitably, some tests require many mock objects to work with. For example, if you are testing a method that underlies paging functionality in your web application, you must create enough mock objects to populate multiple pages. Mocking frameworks can automate the creation of multiple mock objects of the same type.
					• Mocking objects with multiple interfaces. In some tests, where there are several dependencies between classes of different types, you must create many mock objects of different classes. In such situations, it is tedious to manually code many mock objects. Mocking frameworks can help by automatically generating the objects from the interfaces that you specify.
				In each unit test, you are interested in a small and specific area of functionality. Some properties and methods of an interface will be crucial to your test, while others will be irrelevant. A mocking framework enables you to specify irrelevant properties and methods in a given test. When the framework creates a mock object for your test, it creates stubs for the irrelevant properties and methods. A stub is a simple implementation with little code. In this way, the mocking framework frees you from having to implement all the requirements of the interface laboriously.
	Lesson 2: Implementing an Exception Handling Strategy
		Raising and Catching Exceptions:
			An error is an unexpected run-time event that prevents an application from completing an operation. When a line of code causes an error, ASP.NET Core or the common language runtime (CLR) creates an exception. This exception is an object of a class that inherits from the System.Exception base class. There are many exception classes. Often the object class identifies what went wrong. For example, if there is an ArgumentNullException, it indicates that a null value was sent to a method that does not accept a null value as an argument.
			Unhandled Exceptions:
				When an exception is not explicitly handled by an application, the application stops, and the user sees an error message. In ASP.NET Core MVC applications, this error message is in the form of a webpage. You can override ASP.NET Core default error pages to display your own error information to users.
				If an unhandled exception arises while you are debugging the application in Visual Studio, execution breaks on the line that generated the exception. You can use the Visual Studio debugging tools to investigate what went wrong, isolate the problem, and debug your code.
				Sometimes, you may also want to raise your own exceptions to alert components that something in your application went wrong. You can use the throw keyword to raise errors in C# code.
			Catching Errors with Try/Catch Blocks:
				The most commonly used way to catch an exception, which works in any Microsoft .NET Framework code, is to use the try/catch block. Code in the try block is run. If any of that code generates an exception, the type of exception is checked against the type declared in the catch block. If the type matches or is of a type derived from the type declared in the catch block, the code in the catch block runs. You can use the code in the catch block to obtain information about what went wrong and resolve the error condition.
			Creating Custom Exception Types:
				While there are many types of exceptions built in to the system, sometimes you may need to raise an exception that does not fit any of the standard ones. For this purpose, you can create a custom exception class. This can allow you to create exceptions that make sense for your application.
				In order to create a custom exception, you will need to create a new class that inherits from the class Exception, or any other class that derives from Exception. Doing this will allow you to use the class to throw exceptions, allowing for more specific exception handling in your application.
				It considered best practice to always end exception classes with the suffix Exception. This lets the developer immediately understand why this class exists and how to use it correctly.
		Working with Multiple Environments:
			In most applications, there is a distinct requirement for separate behaviors depending on whether the application is run in the developer’s computer or being used to serve clients on a production server. While developing, you will need to diagnose complex error messages, debug JavaScript code, ensure that you are working with the latest versions of files, and use frequent logging for the tracing of errors. However, on the production environment, the user experience should be prioritized for speed and performance. In production environment, compressed JavaScript code should be used. Having the client browser download the same files every visit can cause clients to become dissatisfied by the speed of your application. Furthermore, giving clients detailed exception details will cause an unpleasant experience for the average user, and allow malicious users to know more details about your application.
			In ASP.NET Core, the environment variable ASPNETCORE_ENVIRONMENT should be used to determine application environment for all ASP.NET Core applications. If not set, the default value of the ASPNETCORE_ENVIRONMENT environment variable is Production. This ensures that any application which is run on an environment without the specification will always run in Production mode. You can, at any time, add it to your environment, and give it other values. By default, Development, Staging, and Production are all commonly used values. However, you can also add any possible string value you wish, since you may want a specific environment for unit testing, multiple production environments, and so on.
			In an ASP.NET Core application, particularly while declaring middleware, it is possible to use the current runtime environment to make decisions which affect your application's behavior. A very common usage is for exception handling. While running an application in Development mode, you will often want error pages to contain detailed stack traces with as much information as possible about the exception that occurred. However, when doing so in a production environment, it will cause clients to receive entire pages full of error details which mean nothing and are undesirable for the majority of users. These pages contain information that you must not expose to malicious users to potentially exploit.
			In order to resolve this, you can use the IHostingEnvironment service. This service is frequently injected into the Configure method of the Startup class. The IHostingEnvironment interface exposes useful methods in the form of IsDevelopment, IsStaging, IsProduction, and IsEnvironment(*Environment Name*). You can use these methods to determine the currently running environment.
			Methods of Startup Class for Specific Environment:
				It is also possible to create specific Configure and ConfigureServices methods for a specific environment. This can be done by adding the environment name at the end of the method name. It is important to note that using this will override the default Configure or ConfigureServices behavior. Therefore, you should avoid relying on this too often. As an example for a good use case, if you want to test specific middleware during development, these methods will allow you to run that middleware and focus on debugging without possibly breaking production environments.
			Using Environments in Views:
				Another location in which it can be useful to differentiate between environments can be inside views, particularly when setting up the application. You will often want to use uncompressed versions of JavaScript and CSS files while running in a development environment, compared to a production environment where you will want to use bundled and minified files instead. In order to achieve this, you can use the environment tag helper. The environment tag helper allows you to use an environment tag, and by setting the include or exclude property on it, you are able to specify in which environments the code will be run. This is particularly useful for loading scripts and CSS files.
				Environment Tag Helper:
					Example:
						<!DOCTYPE html>
						<html>
						<head>
							<environment include="Development">
								<link rel="stylesheet" href="~/Styles/bootstrap.css" />
							</environment>
							<environment exclude="Development">
								<link rel="stylesheet" href="~/Styles/vendor-min.css" />
							</environment>
						</head>
						<body>
							<div class="container body-content">
								@RenderBody()
							</div>
							<environment include="Development">
								<script src="~/Scripts/jquery.js"></script>
								<script src="~/Scripts/popper.js"></script>
								<script src="~/Scripts/bootstrap.js"></script>
							</environment>
							<environment include="Production,Staging">
								<script src="~/Scripts/vendor.min.js"></script>
							</environment>
						</body>
						</html>
					The environment tag helper is located in the Microsoft.AspNetCore.Mvc.TagHelpers namespace. To use it in a view, you need to add the @addTagHelper directive to the view to define the tag helpers that this view will use. Alternatively, you can add the @addTagHelper directive to a _ViewImports.cshtml file that is located under the Views folder, so that it is available to all the views. 
				Using Environments in Configuration Files:
					There are many additional places in the application, in which differentiating environments is both simple and useful. A common example is the appsettings.json file. By using the following file naming scheme, appsettings.*environmentName*.json, you can create environment-specific configurations. Using this to support multiple separate connection strings is particularly useful because a production database will usually have separate databases. An appsettings.json file for a specific environment will always take priority over the general appsettings.json file.
				Using Profiles:
					While most servers run on a single consistent environment, however, in a development environment that is not the case. As a developer, you will often be tasked with checking a variety of issues that can be reproduced only on specific environments, and you will often end up switching environments many times. In order to facilitate this, you can set up various development environments within Visual Studio. To configure environments for your project, right-click the project file, select properties from the menu, and then select the Debug menu.
					Using Visual Studio, you can create a new runtime profile for the application. Each profile has a unique name. In addition, each profile has a commandName property, which is used to determine the web server to launch. Common values for the commandName property include IIS Express, which specifies that IIS Express will be launched, and Project, which specifies that Kestrel server will be launched.
					In the profile, you can also add and change environment variables. These variables determine various behaviors at run time, and of particular importance is the ASPNETCORE_ENVIRONMENT variable. By setting the value of this key, you can set the environment in which the application is run. In a profile, you can also specify if a browser will open on your application whenever you run it from inside Visual Studio.
					Note that after you have added a profile, it can also be seen inside of the project's Properties section inside of the launchSetting.json file. You can also directly modify the launchSettings.json file yourself to create new developments profiles.
					To run the development profile, you can use the debugging menu in the Visual Studio toolbar. By clicking the arrow next to the run button, a drop-down list will appear. From this list, you can select the development profile. You can then run the application using the selected profile. This allows you to switch development environments on the fly, allowing for a fast response time and the ability to test environment changes.
					Another easy way in which you can run your application in various environments is by using the command line to set the ASPNETCORE_ENVIRONMENT variable, and then execute the dotnet run command to run the application. The dotnet run command will run an application by using the first profile inside of the launchSettings.json file that uses the commandName property of the project. By setting ASPNETCORE_ENVIRONMENT beforehand the project will run by using the environment set in the ASPNETCORE_ENVIRONMENT. This configuration will persist only within that specific command line window. Note that this will only work if the ASPNETCORE_ENVIRONMENT variable is not specifically set inside the profile configuration.
					Using the dotnet run Command:
						Example:
							set ASPNETCORE_ENVIRONMENT=Production
							dotnet run
			https://docs.microsoft.com/en-us/aspnet/core/fundamentals/environments
		Configuring Error Handling:
			In a development environment, you will often want detailed information about errors. This can allow you to extrapolate important information about what went wrong and how. By adding the UseDeveloperExceptionPage middleware to the pipeline inside the Configure method, all exceptions that occur in middleware after the UseDeveloperExceptionPage middleware will be redirected by default to a page that has detailed information about the exception. Inside the page, you can switch between various tabs to see important information such as the stack trace of the exception, the query string data, the cookies data, and the headers sent as part of the request. This can all be extremely helpful while debugging the application and for reproducing known errors because the relevant information for the exception is present and easily accessible.
			Custom Exception Handler Page:
				While in a development environment, you will often want detailed exceptions and errors. In other environments, you will want to use more user-friendly and immediately helpful pages. For that purpose, you can use the UseExceptionHandler middleware. This middleware receives a string URL to which the request will be redirected. This URL can be used to redirect the problematic request to a controller that handles exceptions by providing a helpful user-friendly view, or to a static HTML file.
				Note that, in order to implement a controller as an exception handler, you will need to call the UseMvc or the UseMvcWithDefaultRoute middleware. Alternatively, if you prefer using a static HTML file, you can use an exception handler that redirects to a static file alongside the UseStaticFiles middleware.
				As a general rule, you will want to keep this page as static as possible since if an error handling controller throws exceptions, a generic server error page will be displayed instead. While creating controllers and views for displaying errors, ensure that you keep the logic simple.
				Avoid using HTTP method attributes inside error handlers because the request might be of different types, which could result in exceptions not getting handled.
			Status Code Pages:
				An additional method of handling errors at your disposal is the UseStatusCodePages middleware. This middleware is designed to provide basic handling for various errors that may occur by displaying a short explanation of the error code to the user. In general, the UseStaticCodePages middleware is designed to handle cases where some part of the request went wrong without throwing an exception. This method will capture html status codes between 400 to 599, which generally cover all cases where something went wrong (400s represent errors on the client side, while 500s cover server-side errors).
				You are not limited to calling just the UseStatusCodePages middleware for handling HTTP status codes. The following methods are also available:
					• UseStatusCodePagesWithRedirects. Allows sending a redirection URL to the client. This new URL will be used instead of the original one. Using the {0} parameter in the URL can be used to set status codes.
					• UseStatusCodePagesWithReExecute. Similar to redirection. However, this redirection occurs directly in the pipeline without affecting the client.
			Handling Exceptions in the Server:
				Sometimes, in most ASP.NET Core MVC applications, an exception might occur on the web server, instead of inside your ASP.NET Core MVC application code, and not be caught inside the application, but caught by the server instead. In this case, if the exception is caught before the response headers are sent, a response will be sent with no body and a 500 status code (internal server error). On the other hand, if the response headers were sent before the server catches the exception, the connection is immediately closed by the server. It is therefore important to avoid any exceptions leaving your application because such errors can be very difficult to trace.
			Handling Model State Errors:
				Another option for handling exceptions is through the model. By using the IsValid property on the model inside the controller action, you can stop further processing invalid data and take appropriate actions on the controller.
			Using Exception Filters:
				You can also configure a special filter called exception filter. You can create an exception filter by creating a class that inherits from the ExceptionFilterAttribute class. This filter can then be applied as an attribute to specific actions and controllers, which allows for handling specific exception cases.
				The class that implements the exception filter class should override the OnException method inside it. You can then use the ExceptionContext parameter to retrieve or set the current exception, you can update the Result property, potentially changing the view or the response context, and you can also set the ExceptionHandled property to stop the propagation of the exception (If it is left as false, the exception will keep propagating).
	Lesson 3: Logging MVC Applications
		Logging Exceptions:
			Writing Error Logging Code:
				When adding logs to your application, you need to first decide where you would like to store the logs. There are many potential options you can consider for your application, and no single solution is the right one because each solution has its pros and cons. Popular logging methods include writing log files with various details about the application, sending logging data to cloud-based services for ease of storage and analysis, storing logging data in databases, and sending e-mails about exceptions.
				By storing logs on files, you ensure that the logs are persistently written, and most reliably maintained. However, care must be taken to balance the logs to maintain a coherent way of tracking the flow of data and exceptions because logging too much can result in large, hard-to-use files that take up valuable space.
				By storing logs on cloud-based services such as Microsoft Azure, you can ensure easy access and powerful research tools for developers. However, cloud-based services rely on constant internet access.
				By storing logs on a database, you can store and divide the logs in a way that can make them easy to peruse However, this requires a steady connection to the database and appropriate database administration. Also, these logs may not be easily accessible to the developers on a daily basis.
				By logging to email, you can ensure that developers are informed very quickly and can handle the problem almost as soon as it occurs. However, it can also easily result in repeated emails for repeated issues, an excess of emails being sent, losses over network connections, and a difficult-to-manage email inbox.
			Where to Write Error Logging Code:
				When you decide where to write code that logs errors, you should consider that errors might arise in almost any part of your application. You should choose an approach that enables you to write error logging code once that will run for any exception anywhere in your application.
				A far more effective approach is to log by using middleware. By logging exceptions by using middleware, you can ensure that problematic code is properly logged and traced. This allows for a singular point of failure to handle all errors in the application.
			Using Third-Party Logging Tools:
				Because exception logging is a very common functional requirement for web applications, there are many third-party solutions that you can choose if you do not want to write your own logging code. Many of these are available within Visual Studio from the NuGet package manager.
		Logging in ASP.NET Core:
			In ASP.NET Core, there is a robust built-in logging library. Rather than working with a specific form of logging, using the logging library, you can setup multiple different logging providers, including built-in providers and various supported third-party logging providers. This allows you to implement the logging that is best for your application and update your logging mechanism without making any changes to the rest of your code.
			Configuring Logging Providers:
				The first step for setting up your logging is by setting up the providers you wish to use. This is done inside the CreateWebHostBuilder method of the Program.cs file.
				To add logging, you will need to add a call to the ConfigureLogging method as part of the CreateDefaultBuilder pipeline. The ConfigureLogging method exposes two parameters, a parameter of the WebHostBuilderContext type, which can be used to retrieve the IHostingEnvironment, and an ILoggingBuilder parameter, which is used to set up the logger providers and configurations.
				In the ConfigureLogging method, you will need to add the logging provider, which will enable logging by using the specific provider. You can log to more than one provider at the same time. ASP.NET Core has several built-in providers, which include:
					• Console. Logs the message to the application's console window.
					• Debug. Logs the message to the application's debug window.
					• EventSource. Uses event tracing API to store the event, behavior differs between operating systems, and it may not work on all platforms.
					• EventLog. Logs the message to the windows event log. Is exclusive to Windows.
					• TraceSource. Creates a trace source that can be listened to with a variety of trace listeners.
					• AzureAppServices. Creates logs that integrate with the Azure platform.
				The providers can be added by calling the add*Provider Name*() method. After you call this method, the provider is set up for logging throughout the application.
			Calling Logging Method:
				After the logging is set up, you can add logs to individual components by injecting a logger into the constructor for the component. The logger that you inject should be of the ILogger<*className*> type where the class name is used to create an identifier for the file from which the logger is called. This identifier contains the fully qualified namespace for the class and makes it easy for developers to find out which log entry was created from where.
				While it is possible to give the ILogger any class, it could result in logs becoming confusing to use. It is recommended to always create a logger for the current class.
			Log Levels:
				By setting a log severity, only logs that match the severity or of a higher severity will be displayed.
				The log levels and their severity values are as follows:
					• Trace (0). These should be used when trying to debug specific things and are often used to track very large amounts of data. By default, trace logs are never displayed and should not be used outside of a development environment.
					• Debug (1). Logs of this level are frequently used for development environments and keep track of large amounts of data. Logs of this level should not be logged outside of development environments.
					• Information (2). Logs of this level should be used at important points throughout the application to ensure the application is running correctly but should avoid getting to a point where they affect system performance. Information logs are frequently, but not always. enabled in production environments.
					• Warning (3). Logs of this level should be used whenever unexpected flows occur. Exceptions that are handled or flows with missing data will often log a warning. These logs should always appear in a development environment, which allows you to find problematic flows.
					• Error (4). Logs of this level should occur whenever exceptions cannot be handled. These should be cases in which a flow has broken down but did not result in application wide issues. These logs should always be kept.
					• Critical (5). Logs of this level should be used to signify application wide failures. Failures such as an invalid database connection and a failure on the web server should be tracked with critical logs.
				To call logs of different levels, you can use the Log*level* method.
			Log Event ID:
				An additional measure that can enable tracking specific errors is using log event ID, which is an optional parameter that you can add to as part of a call to a log method in order to track specific event cases. Commonly, these event IDs are declared as constants and used to further distinguish the events.
				By supplying an integer as the first parameter to the logging method, you can assign that event to use the specified event ID. This can help find specific events later when trying to find issues.
				A best practice is to store event IDs as enums or constants. Doing so allows you to quickly switch the ID if required. You should not provide hard coded IDs as it is difficult to maintain and understand the significance behind it.
			Logging Exceptions:
				Another useful parameter that can be accepted by the various logging method is exceptions. A log can be used to directly display exceptions. This is useful for handling exceptions that occur in try/catch blocks because later you will be able to see the exception details in the log. This can be very useful for finding issues that occur in production. The parameter for the exception should be the first parameter if event ID is omitted, or It will be the second parameter if event ID is used.
			Configure Logging:
				You can also utilize the appsettings.json file to set various details for your logging. It is most commonly used to set log levels. This can easily allow you to set a different log level in different environments. To load the configuration for logging, you will need to call the AddConfiguration method on the ILoggingBuilder parameter and supply it with a configuration by using the GetSection method on the Configuration property of the WebHostBuilderContext parameter. After this is done, the logging configuration will be loaded.
			Using Third-Party Logging Providers:
				In a modern application, there are innumerable ways to store logs, whether in cloud servers, databases, or other. It would be impossible to cover all possible storage methods in ASP.NET Core, and you may occasionally want to use one of many third-party logging providers.

Module 11: Managing Security
	Lesson 1: Authentication in ASP.NET Core
		Authentication is the act of utilizing several parameters to make sure that a user is who they claim to be. By implementing authentication, you can ascertain who a user is and provide them with appropriate content while utilizing your applications. 
		Authorization is the process where an already authenticated user in the application can be granted access to specific actions or resources. By utilizing authorization, you can prevent users from accessing sensitive material not intended from them or from performing actions which they should not be able to.
		The Need for Authentication:
			Authentication is the process of determining the identity of a user within the application. By providing a unique username alongside a password, the user can confirm their identity within the application and you can provide specific services to the user.
			In addition, authentication can also be used to acquire personal data about website users.
			A common type of authentication is authentication by username and password. The user will need to know both the unique username they use in the application and the related password. This is, however, not the only possible option for authentication and additional methods exist such as fingerprints, phone confirmation, email confirmation, and more.
			When a single type of authentication is used, it is called single-factor authentication. For the most part it mainly uses username and password. When multiple forms of authentication are required, it is called multi-factor authentication instead and usually involves both username and password, as well as additional forms of authentication.
			Creating a proper authentication environment is crucial, as it can add an important layer of security to your applications. Modern applications often involve sensitive data, and as such, require that it is only provided to appropriate users. Without authentication, data is accessible to all users, allowing malicious users to acquire sensitive information. 
			Most forms of authentication these days use what is known as token-based authentication. When the initial authentication is performed, a token is generated by the server and sent to the client. The client then sends the token again in every future request, allowing the server to confirm that it is the same user. If the token is incorrect or expired, the user will need to authenticate again.
		Setting up ASP.NET Core Identity:
			ASP.NET Core Identity is a commonly used solution for handling login functionality within ASP.NET Core applications. It is well supported within the framework and works with multiple different providers to assist developers in creating required authentication infrastructures. 
			As part of the ASP.NET Core Identity infrastructure Entity Framework is utilized by default in order to manage setting up and interacting with user data. In nearly all cases this will mean that you can use the same database as other parts of your application, regardless of type to handle login and logout, negating the requirement to set up additional infrastructure. 
			ASP.NET Core Identity also exposes various services, which you can inject throughout your application, for managing system users and performing login and logout operations. This makes adding it to your application simple and ensures that your application utilizes the various technologies you have learned previously in this course.
			Setting up the User Class:
				By default, properties such as email address, username, and password hash are all stored as part of the default IdentityUser class which is exposed from the Microsoft.AspNetCore.Identity namespace. 
				If you require any additional properties, you will need to create your own class which inherits from the IdentityUser class. If you do not require any additional properties you can use the IdentityUser class directly instead.
				Note that only a password hash should ever be stored for ASP.NET Core Identity users. Real passwords should never be stored, and it is a massive security exploit to do so.
			Setting up the Database Context:
				Setup a DbContext class to support identity, the context class will need to inherit from the IdentityDbContext class rather than DbContext.
				A nice advantage when using IdentityDbContext is that you will not need to add a DbSet instance to manage users. The logic for managing users is built in as part of IdentityDbContext itself.
				You can use the IdentityDbContext to manage additional datasets, in the same way as DbContext. It adds additional functionality on top of DbContext and isn't intended to be used exclusively for authentication.
			Configuring Identity in Startup Class:
				Inside the ConfigureServices method, in the Startup class, you will need to call the AddDbContext method on the IServiceCollection parameter with a class which inherits from IdentityDbContext. It is instantiated in the same way as any other DbContext, with the same parameters. 
				You will also need to add a call to the AddDefaultIdentity<*User*> () method. The AddDefaultIdentity method expects the class which will be used to manage users in the application. This can be either a custom class which inherits from IdentityUser or it can be IdentityUser itself. 
				You should also pipe a call to AddEntityFrameworkStores<*DBContext*> () immediately after AddIdentity. This is used to connect the IdentityDbContext to the IdentityServices added by the call to AddIdentity. It expects a database context type to be provided and you should use the same context class as the one you created for managing identity. 
				If you do not wish to use Entity Framework in your application you should not make a call to AddEntityFrameworkStores.
				Finally, in the Configure method of the startup class, you will need to load the middleware for handling the authentication process in order to enable Identity. You can do this by calling the UseAuthentication method of the IApplicationBuilder object. It is important that you call UseAuthentication before the call to UseMvc. This is to ensure that the authentication will always be ready for use within our ASP.NET Core MVC Controllers.
		Interfacing with ASP.NET Core Identity:
			Performing Login:
				Logging in is the process of the user entering their credentials in a dedicated form in an application and the server confirming that the credentials are valid before sending an authentication token to the client, which will be used to check the user identity on future requests.
				To facilitate login, you should create a controller named AccountController. While you can use a different name for it, much of the default logic in ASP.NET Core MVC relies on AccountController and using a separate name will require additional work.
				In order to be able to perform login operations, you will need to inject the SignInManager<T> service, where T is the class which inherits from IdentityUser as described in the previous topic. The SignInManager class exposes valuable methods for performing authentication operations.
				The PasswordSignInAsync method is designed to authenticate a user and create a session, while also updating the client with the authentication token required for future requests. It accepts parameters as follows: 
					*username*. The username used by the user. String variable. 
					*password*. The users non-hashed password, the non-hashed password should never be stored anywhere as that is a security exploit. String variable. 
					*isPersistant*. Determines if the cookie created by the sign in method persists after the browser is closed. Boolean variable. 
					*lockoutOnFailure*. Determines if the user will be locked out if the login attempt is unsuccessful. Lockout can be used to prevent malicious attempts to login at the cost of inconveniencing the user if the wrong password is typed. Boolean variable.
				Once the authentication method succeeds, you can then redirect the user to an appropriate URL using the addition of a RedirectUrl parameter in the query string. By using this parameter, you are able to navigate to the original destination page which requires authentication when it is successful.
				You will also need to create a view which will call the login method on the controller. Note that the view should use a different model then the one used by the DatabaseContext. Many properties in the UserIdentity are not likely to be used, while other, more sensitive properties such as passwords should be present in the model used by the view, while they should be stored in a hashed format on the class derived from UserIdentity.
				You may sometimes find that you require separate models for server data and client data. A common example for this is when using password. While the user will need to input his password on the client, it should not be stored within the database. As such, you will need a client-side model which does store a password, and a server-side model which does not retain the password. To differentiate this, you can name models which are intended to be used on the client side with ViewModel at the end. This will make them easily visible as models intended for usage within the view.
			Performing Logout:
				Occasionally, users that are logged in to your application will wish to logout of the application. This can be for a variety of reasons, such as switching to another user or preventing access to their account from potentially malicious sources nearby. To facilitate this and to allow users to log out at their discretion, you will want to add logic for handling logout requests.
				An additional step you should take before performing a logout is to check if there is a currently authenticated user logged in to the current session. This can be done by using the User property of the Controller class. The User property exposes various properties relating to the current user in the session. To check if a user has been successfully authenticated, you can check the IsAuthenticated property under the Identity property in the User. This will return the current authentication status and you can use it to make decisions on how to handle the user throughout your application.
				To actually perform a logout, you will need to call the SignOutAsync() method from the SignInManager. This method clears the stored identity tokens for the user, ensuring that future calls will not be on the current user, unless a new login is performed.
			Adding New Users:
				Unlike login and logout, adding new users does not depend on the SignInManager service. Instead you will need to inject the UserManager<T> service, with the T being the type that inherits from IdentityUser, which is used for modifying existing users. You can use the CreateAsync(*user*, *password*) method to create a new user in the system.
				The CreateAsync method will create a new user in the database by using the various user properties stored on your chosen class, while hashing the password into a more secure hash string. This helps ensure that the actual password is not stored anywhere, thereby maintaining the application security.
			Accessing the User Properties:
				By using the FindByNameAsync(*name*) method on the UserManager object and providing it with the value of the Name property on User.Identity, the UserManager will retrieve the user, if it is found. Afterward, you are able to extract properties off the user class you selected and apply them to the page model as is needed.
		ASP.NET Core Identity Configuration:
			User Settings:
				User settings are used to determine a variety of configurations regarding the registration of users in the system.
				You can use the User property of IdentityOptions to configure settings relating to the IdentityUser.
			Lockout settings:
				An important feature which is frequently present in various systems involving authentication is the concept of user lockout. When the application detects multiple attempts to log in with the same username but invalid passwords, it can be set to lock the user for future logins for a period of time. This can prevent malicious sources from logging in to the system by utilizing brute force tactics which involve attempting to guess a password, by vastly increasing the time between attempts rendering them unviable. 
				To apply the lockout, you will need to make sure the lockoutOnFailure parameter in the call to the PasswordSignInAsync method is true, or the lockout settings will be ignored. 
				You can use the Lockout property of the IdentityOptions to configure settings relating to the lockout.
			Password settings:
				Secure passwords are a requirement in modern authentication environments and it is the developer's duty to assign a password requirement that is appropriate for the specific environment. While in many cases a default password can be used, sometimes you will find that you need to customize it further.
				In order to configure password settings and complexity, you can access the Password property of the IdentityOptions.
			Sign In:
				The SignIn property of the IdentityOptions can be used to require the user to confirm their email or phone number in the application. Login will be impossible until successful confirmation occurs.
			Cookies Settings:
				An additional type of setting that affects Identity is the cookie configuration. By calling the ConfigureApplicationCookie method in ConfigureServices, you can use a CookieAuthenticationOptions parameter to set various properties for cookies in your application. Various settings such as cookie name and expiration time can be set here and will apply to the cookies used by the application.
		Customizing Providers with ASP.NET Core:
			Out of the box ASP.NET Core Identity utilizes Entity Framework to handle authentication and handles the end to end authentication within the application scope.
			Windows Authentication:
				When creating an application designed to be used in an intranet environment, you should consider using Windows Authentication in your application. By setting up Windows Authentication, users will be able to sign into the application by using their Windows credentials. 
				By using Windows authentication, the active directory will be used to determine user credentials and permissions, allowing users to be managed and created by internal administrators. This helps create safer applications and removes the need to manually manage users. 
				You may wish to use Windows Authentication when:
					• Your application is designed to be used within an organization.
					• When you want user creation to be limited and managed manually. 
					• When you want to prevent undesirable users in your application.
					• When your applications feature information sensitive to your organization.
				You should not use Windows Authentication when:
					• When you need to create applications which can be accessible to the wider internet.
					• When you want users to be able to register and create their own accounts in your application.
					• When your organization is not managed by Windows Active Directory
				Utilizing External Providers
					By default, in an ASP.NET Core application all logins are handled through your application itself. However, the registration process can often be tedious and many users already possess multiple different users across many different websites. 
					To help simplify this, you can implement signing in by using various external providers. Examples of external providers include: 
						• Microsoft – As used by a variety of Microsoft products, such as Microsoft Azure or Hotmail. 
						• Facebook – As used on Facebook and its mobile application.
						• Twitter – As used on Twitter and its mobile application. 
						• Google – As used for Google applications such as Gmail and Google Maps.
						• And many more.
					By implementing support for an external provider, you allow your users to sign in through a service in which they already have an account, reducing the amount of bookkeeping they require for their accounts, and enticing users who do not wish to run through complex registration procedures. 
					When external providers are used, the user is still saved in your storage; however, many of the user details remain with the provider. As a result of a successful connection, your application will receive a success token which can then be used to get additional information about the user. 
					Due to security concerns, most providers will notify the users about which details become accessible to your application during the first login. This is done by the various third-party providers to ensure personal information is not passed without the user's consent. You can configure your application to request additional details and the user will be notified accordingly. 
					There is no single uniform way of configuring providers, as each provider utilizes different protocols and logic. As a result, you should always consider the cost/benefit of adding each provider rather then providing as many as you can. 
					External providers can be used alongside the default ones and it may even be wise to offer the option to users without accounts.
				Using Alternative Storage Providers
					By default, ASP.NET Core MVC utilizes Entity Framework Core to manage users in the application. This can be convenient for setting up simple storage solutions for your application. However, it is not always a viable option. 
					Many times during the development process, you may find that you are required to utilize a different storage option and not be able to use SQL Server or even utilize data in a different format. Examples of possible scenarios include but are not limited to: 
						• Azure Table Storage.
						• Databases with a different data structure such as MongoDB or Redis.
						• Utilizing Data Access systems other than Entity Framework Core.
					In order to support these storage providers, you will need to write a new custom provider to support your chosen storage method. The specific code depends on the chosen provider. To implement it, you will need to create alternative identity store logic, as well as alternative data access layer. 
					The store is used to set up the datatypes you use for the application. These may be the default types as implemented in ASP.NET Core; however, they will need to be created to work with your chosen dataset. The store determines the setup of the basic entities and will need to include properties such as an IdentityUser derived class, as well as details about the login process related to external providers. 
					Meanwhile, the data access layer is used for providing the logic for communication with your chosen method of storage. It will need to be created to work with the chosen storage mechanism, but is easily customizable and extensible. When utilizing a storage provider most of your work is likely to occur in the database access layer. 
	Lesson 2: Authorization in ASP.NET Core
		Authorization is the process of determining what a user is allowed to do in an application.
		Introduction to Authorization:
			In many applications, you will want to prevent certain users from accessing various resources in your application. Whether you wish to prevent users from accessing specific pages, specific data, or specific actions, this can be handled by the ASP.NET Core Authorization infrastructure.
			ASP.NET Core authorization lets you create and implement different kinds of authorization within your application and utilize it to limit content as is appropriate.
			Setting Up Authorization:
				Authorization is also relatively simple to set up, requiring only a small change in the call to ConfigureServices. Instead of the call to the AddDefaultIdentity<*User*>() method on the IServicesCollection parameter, you will need to instead call the AddIdentity<*User*, *Role*>() method. The User parameter should remain the same, however the Role parameter is the IdentityRole class or a class derived from IdentityRole. 
				The IdentityRole class is designed to manage roles within the application, similarly to how IdentityUser manages users and it is used to start up the RoleManager class. However, unlike IdentityUser, which is often extended, IdentityRole isn't commonly extended and the base class is most frequently used. 
				While the names of AddDefaultIdentity and AddIdentity are similar, they operate very differently and if you wish to use ASP.NET Core MVC logic alongside authorization, you will need to utilize AddIdentity. If you try and use AddDefaultIdentity in an ASP.NET Core MVC application, you may find that redirection paths from failed authorization attempts will cause you to navigate to invalid paths to the AccountController. This is due to AddDefaultIdentity utilizing an ASP.NET Core Razor Pages infrastructure.
				If you wish to continue utilizing AddDefaultIdentity in your applications, you can pipe a call to AddRoles<*Role*>() after the call to AddDefaultIdentity. Note that this will require you to manage authentication by using ASP.NET Core Razor Pages.
			Simple Authorization:
				The most basic form of authorization revolves around blocking users who are not signed in from accessing certain pages or performing specific actions. This is called simple authorization, and it automatically redirects to the Account\Login action, alongside a return URL for easy reconnection. This allows you to easily gate off entire controllers or individual actions and prevents you from requiring to manually configure redirections. 
				In order to implement simple authorization, all you require is to add the [Authorize] attribute. This attribute can be added to a controller class, preventing unauthorized users from accessing the class at all while not logged in. Users who are not signed in but attempt to navigate to the controller will be redirected to the Login URL. 
				The Authorize attribute isn't limited to the controller class and can also be added to individual actions. This can be valuable for example when setting up a controller you wish to be publicly accessible but with specific actions blocked from users who have not yet logged in. For example, this could include browsing a shop being publicly accessible while actually making a purchase requiring logging in. 
				Alternatively, you may wish to limit most actions of a controller, while allowing one or two exceptions. While you can theoretically add Authorize to every action manually, it could result in an action accidentally being added without authorization to a secure controller, resulting in a security breach. Instead, in such a case, it would be better to add Authorize to the controller itself and add the [AllowAnonymous] attribute to actions which are exceptions. By setting AllowAnonymous, you enable access to the action itself while the rest of the controller is not accessible. An example for this could be a web page that allows you to view data, while requiring sign in to actually make changes.
		Authorization Options:
			Role-Based Authorization:
				A simple and common form of authorization, role-based authorization determines what users can access based on their roles within the system. There are no default roles defined within ASP.NET Core applications, however you can create your own roles to fit the requirements of your applications. 
				The first step towards working with roles is by populating the roles within the system. This can be done by using the RoleManager<*Role*> service which is instantiated by the call to AddIdentity within the ConfigureServices method. The RoleManager will be of the same type provided for the role in the call to AddIdentity, usually IdentityRole, and can be injected throughout the application allowing you to manage roles. 
			Creating Roles:
				In order to create a role, you will need to call the CreateAsync(*role*) method on the role manager. The role parameter expects a role of the same type which is configured by the AddIdentity method. To instantiate the basic role, all you require is to provide a role name. 
				You can, at any point, check if a given role exists by calling the RoleExistsAsync(*role name*) method. It accepts a string name, and checks if this specific role has already been created. 
				Generally, you can create roles at any point throughout the application, but it is recommended that you create them as part of the database instantiation, or in the Configure method. If you intend to create them in the Configure method, ensure that the database exists before creating any roles. 
			Assigning Roles to Users:
				In order to give a user a role, you will need to assign one or more roles to the user. This can be done at the time of registration or through different processes. For example, you may wish to give all users that register a "User" role, but only grant them administrator privileges from another admin. 
				In order to grant a user a role, you will need to call the AddToRoleAsync(*user*, *role name*) method. This method expects a user of the type defined by the user manager, as well as a role name string which is used to connect the user to an existing role. If the role has not been created, then the user will not be assigned to it.
			Authorizing with Roles:
				Finally, to authorize with a role, you can use the Authorize attribute. By providing a Role parameter to the attribute, you can determine which role or roles can access the class or method decorated by the attribute. This parameter is always a string and should match the name of one of the roles that were created. If more than one role can access the class or method, you can separate all valid role values by a comma.
			Claims-Based Authorization:
				Another common method of authorization in ASP.NET Core is the system of claims. A claim is a key-value pair which defines the user itself, rather than the user's permissions.
				Claims can be declared as is in an ASP.NET Core MVC application by just using a key and a value, but you can also optionally choose to add Issuer to claims. This denotes where the claim comes from and can be used to identify the source of the data. This can be useful when you have an application which communicates with other applications or providers and want to verify that the data is certified by a trusted source. 
				Claims-based authorization is used to determine if the user has a specific type of claim or not. This can be useful for whenever you require the user to input specific details before accessing specific pages.
			Creating a Claim:
				To create a new claim, you can use the Claim class. The simplest constructor, Claim(*claim type*, *claim value*), receives a claim type, which is a string denoting the key for the claim and a value which is the value for the claim. You can also utilize the ClaimTypes enum as the first parameter to provide a large variety of common claims. 
				After you have created a claim, you can use the AddClaimAsync(*user*, *claim*) method on the UserManager to add the claim to a specific user. The user being of the same type as the one used by the UserManager and the claim being the newly created Claim object.
			Creating a Claim Policy:
				To authorize for claims, you will need to create a simple authorization policy in your application. An authorization policy is a ruleset which is validated whenever the policy is invoked.
				In order to create a policy, you will need to add a call to the AddAuthorization(*authorization options action*) method on the IServiceCollection parameter in the Startup class. The authorization options action accepts a single AuthorizationOptions parameter, which allows you to call the AddPolicy(*policy name*, *policy action*) method to add a new policy. 
				The policy name provided to AddPolicy determines how you will refer to it when calling it from the Authorize attribute, while the policy action is the code that will be executed whenever the policy is validated. It will receive an AuthorizationPolicyBuilder parameter which is used to set up requirements for the policy. You can add claim requirements for a policy by using the RequireClaim(*claim name*, *valid values*) property of the AuthorizationPolicyBuilder object. The claim name is the name chosen for the claim itself, as defined by creating a claim, while the values are an optional array of values to require when validating the claim. If it is empty, only the presence of the claim will be checked.
			Using Claims-Based Authorization:
				After you set up the authorization logic, you can then use the Authorize attribute to enforce your claim requirement. By providing a Policy parameter to the attribute, you can supply it with a policy name and all calls to the method or controller which are decorated by the attribute will require the claim to be made.
				In addition to the previous methods, you can also create your own dynamic policies. These are considerably more complex to create, but in exchange offer you the option to create any policy you require. Custom policies can receive parameters to validate against, but in exchange need to be configured from scratch.
			Custom Policy-Based Authorization:
				In addition to the previous methods, you can also create your own dynamic policies. These are considerably more complex to create, but in exchange offer you the option to create any policy you require. Custom policies can receive parameters to validate against, but in exchange need to be configured from scratch. 
				https://docs.microsoft.com/en-us/aspnet/core/security/authorization/policies
			Using Multiple Authorization Methods:
				Finally, you can combine multiple different authorization methods, by adding multiple Authorize attributes. Whenever more then a single Authorize attribute is present on a controller or action, it will only be accessible if all of the authorization requirements are fulfilled.
			Whenever an authenticated user attempts to access a resource that they do not have permissions to access, the ASP.NET Core MVC application will redirect the request to "Account\AccessDenied". You should add a controller view and method to handle this path, and update the users that they do not meet the criteria to view the page. If you do not do this, users will encounter a browser error. 
	Lesson 3: Defending from Attacks
		Cross-Site Scripting:
			Cross-site scripting commonly known as XSS, is a security exploit by which a malicious user (commonly referred to as attacker) utilizes the input on a website to insert malicious scripts, such as JavaScript code, with the intention of altering the default behavior for a website. When these scripts are run, they can be used to steal private information such as session tokens or cookies and alter the behavior of pages, potentially redirecting users to malicious websites.
			XSS attacks are only such when it affects a browser other than the browser in which they were performed. It is completely impossible to prevent a user from sabotaging the browser which he is using, as most browsers give knowledgeable users full control over the HTML and JavaScript. If the user can affect a different browsing session, even if it only affects the same credentials, it would be considered an XSS attack. 
			As a general rule of thumb, you will need to be careful in the following areas whenever handling any data that was input from the user in order to avoid XSS attacks: 
				• HTML inputs. Never display any untrusted data which may come from another source. This can include data sent from another session, data influenced by headers, or even data loaded from databases. 
				• HTML elements. Putting untrusted data into an HTML element can cause scripts to be run, resulting in malicious activities. This form is particularly easy to abused, as it's very easy to write a script inside of an element. 
				• HTML attributes. A clever attacker can utilize a standard attribute structure to end an existing element and add a script or can abuse one of the on- events of HTML elements to call a function. 
				• JavaScript. Untrusted data added or inserted in your JavaScript could result in undesirable function calls or undesired code execution. 
				• Query string. A further vulnerability point could be the query string. In an unsecure environment, data parsed from inside the query string could result in script executions.
			You can avoid these issues by ensuring you use appropriate encoding on untrusted input. Encoding ensures that various symbols commonly used in HTML and JavaScript, such as '<', or '\' are replaced with string keys which are globally recognized by browsers and cannot be used to execute code. All encoded characters follow a specific format, for example HTML encoding begins with '&' and ends with ';', while JavaScript encoding utilizes a "\\u*key code*" format. This ensures that browsers will be able to identify that these elements are encoded and render them appropriately. 
			Furthermore, HTML itself requires encoded characters to display them. This is due to it being impossible to use several characters due to being used as part of the HTML markup.
			https://dev.w3.org/html5/html-author/charref
			HTML Encoding in Razor Pages:
				By far the simplest case to handle, by default, the @ directive when used in the HTML context of a view, will always encode strings by default. This means that you do not need to worry much about using external inputs within Razor. Therefore, the HTML context is secure by default in regards to XSS. It is however crucial to note that this is specifically for HTML and does not affect JavaScript tags within the HTML. 
				It is possible to allow non-encoded HTML to be used within a Razor page, but you will need to make a dedicated effort by using the HtmlString class which is not encoded and as such is prone to XSS attacks. It is advised to not allow non-encoded HTML.
			JavaScript Encoding in Razor pages:
				Unlike the HTML context of Razor pages, using a directive within JavaScript tags is not automatically encoded. This means that you need to be extra careful when trying to inject unknown data into a JavaScript function. 
				You can get around this issue by injecting the JavaScriptEncoder class. The JavaScriptEncoder class is a class designed to safely perform encoding on JavaScript objects, preventing malicious code from being able to execute functions. It is accessible by importing the System.Text.Encoding.Web namespace. It provides the Encode function which encodes JavaScript, ensuring that the desired text will be displayed safely, while not allowing for undesired functions to be invoked.
			Injecting Encoders:
				Occasionally, you may wish to work with encoding on the controller. You can do this by injecting HtmlEncoder, which is used for encoding for HTML content such as element content or attributes, and JavaScriptEncoder, which is used for injecting encoded elements into JavaScript code. This can allow you to pre-encode information from unknown sources and not have to encode it on the Razor page itself. 
				Razor pages utilize HtmlEncoder by default. Therefore, while it will always prevent invalid HTML, it can cause errors or security exploits while using the @ directive in JavaScript. 
			Encoding URL Parameters:
				Occasionally, you might have a situation where you build a URL while using a query string that includes unknown input. As with all other things, this can potentially have changed URLs or problematic query strings, which can potentially change your URL path or modify values of controls bound to them and create undesirable additional controls that benefit attackers. As such, you should always ensure you encode query strings while creating them in the controller.
		Cross-Site Request Forgery:
			Cross-site request forgery (CSRF or XSRF) is a specific type of attack which utilizes users being logged in on one site by adding calls to APIs on that site from another site. This form of attack can be particularly dangerous, as it can be used to perform operations using the logged in user without requiring any consent from the user. 
			In order to perform this kind of attack, the malicious website will have a call to an API used by the victim site and will use the user's authentication cookie to perform it. When the malicious site calls the API on the victim site, all cookies for that domain will be sent alongside the request. Thus, potentially allowing any action that can be performed by the user, including deleting or changing data, deleting the user, or any variety of other malicious options dependent on the API.
			An XSRF attack does not even require the user to click any buttons. The malicious website can create a hidden form within the HTML and submit it by using JavaScript, without requiring any input from the user. 
			XSRF attacks are not influenced by utilizing a more secure connection using HTTPS, since the form can just send the request by using HTTPS and it can also be used to target any possible method. 
			If your website API provides any GET methods which are capable of changing data, malicious users can target your site on websites which allow user images by providing a vulnerable URL to your website causing problems with any of your users which load any of these image links. It is therefore a best practice for a GET method to never change data. 
			ASP.NET Core MVC utilizes a form of anti-forgery validation method in which when a form is created by the controller as part of a view, it receives an additional hidden input with the name __RequestVerificationToken. This name is generated every single time the view is created and thus provides a unique key by which to identify the specific instance which posted the form. When XSRF protection is set up, all XSRF protected actions will validate that a correct token is provided or an error will occur instead. This guarantees that requests arrive only from within the application. 
			Protecting Controllers from XSRF Attacks:
				The first step towards protecting yourself from XSRF attacks is by protecting your controller APIs. This can be done by adding the ValidateAntiForgeryToken attribute to a controller or action. By doing this, the controller or action will require that a valid token will be present on the request. This helps to ensure that the action can only be accessed from within a valid ASP.NET Core MVC page.
				You may sometimes find yourself in need of marking multiple requests across a controller, while requiring some requests to not require anti-forgery, such as various GET based actions. There are several tools you can use to help the process become more convenient and comfortable for the user. 
				The first is the AutoValidateAntiforgeryToken attribute. It behaves similarly to the ValidateAntiForgeryToken attribute; however, it will automatically ignore actions which are called with the methods: GET, HEAD, OPTIONS, and TRACE, which are designed for data retrieval. This allows you to quickly and easily add anti-forgery to all methods which can change data, without affecting methods for retrieving data. 
				Finally, if you wish to specifically make an action accessible in a controller which requires anti-forgery tokens you can use the IgnoreAntiforgeryToken attribute to remove it for a specific action. This will let you make certain actions available in scenarios where they are usually blocked. 
			Using Anti-Forgery Tokens on Views:
				To support anti-forgery tokens inside your views, allowing you to interact with actions which require them, you will need to ensure that your forms support anti-forgery tokens. The simplest way to implement this is by ensuring tag helpers are loaded for the view. The forms will also need to possess the method attribute with the values: post, put, or delete. Forms with the get method will not receive a token by default. 
				When tag helpers are loaded into a view, all forms will also generate an input element with a type of hidden and the name __RequestVerificationToken. This input will also bear a uniquely generated value which will be sent alongside the form and be used to identify that the request came from a known form. This makes it very difficult for another website to replicate and makes the action more secure against XSRF attack.
				It is always possible to specifically disable Form tag helpers to prevent the form from utilizing anti-forgery tokens. However, it is not recommended since it means you choose not to utilize the protection offered by the anti-forgery mechanism. 
				Should you find yourself needing to use JavaScript to call your methods, you can create a form and either submit it via JavaScript or you can find the hidden element with the name __RequestVerificationToken and use it to create your request. 
				If you need a to add a secure call to a get method, such as to prevent hostile websites from getting data from your API or if you wish to not utilize the form tag helper, you can manually add a directive call to @Html.AntiForgeryToken()to create the request token. This can allow an alternative option and help you create secure get requests. 
		Sql Injection:
			SQL injection shares a similarity to XSS in that both attacks use parameters to add logic for performing operations. However, whereas XSS targets other clients in their attacks, potentially affecting other users, SQL injection directly targets the database itself. Similar to XSS, SQL injection occurs when a malicious user calls for APIs or changes inputs. But unlike code designed to run scripts, SQL injection instead manipulates input strings and utilizes them to change how SQL queries work. SQL injection could be caused by users providing malicious control input or even directly modifying query strings and server requests on their browsers.
			How to Defend Against SQL Injection:
				Utilize Entity Framework:
					The simplest approach would be to avoid using SQL commands directly in your code. When using Entity Framework and directly working with entity objects, you are not vulnerable to SQL injections. Since Entity Framework does not use an SQL syntax to work with, any attempts to use SQL injection will be stopped by the Entity Framework and at worst will result in the malicious user receiving error messages. 
				Utilize Parameterized Queries:
					As part of the SQL command, you can choose to not directly add parameters into your query, but instead add named parameters by using the @*name* placeholder, where the name is the relevant name for your application. This allows you to add the parameters by calling the AddWithValue(*name*, *value*) method on the SqlCommand object's Parameters property. The name property will need to be the same as the placeholder, while the value is the value which you wish to poll the database. This will not result in any SQL being executed, and at worse, the command will fail if the value cannot be applied. 
				Create Stored Procedures:
					Another good option would be to create stored procedures to handle various tasks for you. When you call a stored procedure, you are able to call parameters by name. This will prevent unpredictable SQL being run on your server and ensure that only expected code is run. However, creating stored procedures will require additional work to create and maintain separately from your application. 
				Use the Lowest Required Database Permissions for the Application:
					You can always configure the application connection string to use the lowest required access permissions to your database. This is useful when an SQL injection attack does end up occurring, since it can help to limit potential damage to the database by malicious operations. It is a good idea to utilize this among additional means, but should not be exclusively relied on, as it will usually not help protect against attempts to steal data. 
				Sanitize Parameters:
					Finally, as the least recommended option, you can sanitize parameters either by parsing or by using them as part of the routing. If the parameters are not string type you can parse them yourself or you can rely on ASP.NET Core MVC Routing to provide parameters of the correct types. Alternatively, you can use regular expressions and whitelisting to disallow specific strings from appearing. 
		Cross-Origin Requests:
			Modern browsers utilize a special restriction, known as same-origin policy, which is designed to prevent websites of different origins accessing resources on another website. This helps prevent malicious websites from being able to call APIs on other websites and helps protect both end users, as well as the application itself. However, you may occasionally find a need to have different web applications communicate with each other, despite having a different origin. To allow this, you can enable Cross-Origin Resource Sharing (CORS). 
			CORS allows servers to determine that they allow some or all of these normally blocked access attempts by allowing the developer to set conditions under which the application will accept requests from another origin. The behavior for CORS is a W3 standard, and thus standard across all major browsers and as such does not suffer from compatibility issues. It is important to note however that when utilizing CORS you do not accidentally open yourself up to various attacks. 
			Same-Origin:
				CORS needs to be enabled whenever the request origin is changed. The request origin is determined by matching up the following segments of the URL:
					• Schema. The protocol declaration at the beginning of the URL, such as http:// and https://.
					• Host. The host is comprised of both domains and subdomains and encompasses the section between the schema and the port.
					• Port. The port is an optional part of the URL, appearing immediately after the host when it is present. If the port is not specified the default values for the protocol will be used, such as 80 for http, and 443 for https. When a port is present it will be denoted by :*port number*. 
				Any parts appearing past the schema, host, and port will not affect the origin, but all three must be identical for it to count as being the same origin. If you have two applications which need to communicate on two separate origins, you will need to implement CORS. 
			How CORS Works:
				CORS relies on a specific HTTP Header being utilized as part of the request and the response to confirm to the browser that a request is intended to be CORS. As part of the request, the Origin header needs to appear. This header is considered a "forbidden" header and is specifically set by the browser and cannot be changed manually, through JavaScript, or other means. This prevents users from being able to switch the Origin values and calling services from unapproved websites. 
				If the browser supports CORS, such as Microsoft Edge, the Origin header will be added by default. CORS cannot be used in browsers which do not add Origin to the request. 
				CORS Request Header:
					Example:
						GET http://www.contoso.com/api/getEmployees HTTP/1.1
						Referer: https://www.adventure-works.com
						Accept: */*
						Accept-Language: en-US
						Origin: https://www.adventure-works.com
						Accept-Encoding: gzip, deflate
						User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/17.17134
						Host: https://www.adventure-works.com
				When the server receives a CORS request, it will validate that the request is from a legitimate source. If the request came from a legitimate source with a valid Origin header, the server will add the header Access-Control-Allow-Origin with a value equal to the original request Origin. The browser will then ensure that there is a match between both values, and if no match is found, the browser will disallow the request, even if the server returns a valid response.
				CORS Response Header:
					Example:
						HTTP/1.1 200 OK
						Cache-Control: no-cache
						Pragma: no-cache
						Content-Type: text/plain; charset=utf-8
						Access-Control-Allow-Origin: https://www.adventure-works.com
						Date: Mon, 31 Dec 2018 23:59:59 GMT
						Content-Length: 12
			Registering CORS Services:
				There are two ways in which you can use CORS in your ASP.NET Core MVC application, either through a single middleware or through attributes in ASP.NET Core MVC controllers. Regardless of which option is right for your application, you will need to add a call to the AddCors() method on the IServiceCollection parameter of the ConfigureServices method. This ensures that the appropriate services are loaded for enabling CORS. 
			Using CORS in Middleware:
				The first approach to using CORS in your application is by adding a dedicated middleware to handle CORS requests on a global application level. By taking this approach, you will need to customize which requests you allow and which you deny. This is vital, because if you do not limit access to your application from external applications while using CORS, you open your application for misuse from malicious websites. 
				In order to add CORS as a middleware you will need to add a call to the UseCors middleware. The UseCors middleware can be called with the AddCors(*cors policy builder*) to create a policy on which CORS will rely. This policy builder supports various methods with which to customize your policy. One of the most basic yet effective among them is the WithOrigins(*origin array*) method. It accepts an array of possible origins present as strings as covered earlier. When a request is made from an allowed origin it will be allowed to continue. However, if the origin does not match any of the options, the server will block the request and the browser will display an error. 
				Note that origins should not end in a trailing '/'. If they do, they will not be parsed correctly. 
				An alternative option is also present by instead defining a CORS policy in the AddCors method. This can be done by invoking it as AddCors(*cors options action*). This method accepts a CorsOption object which can be used to create multiple CORS policies by invoking the AddPolicy(*policy name*, *cors policy builder*) method. This will create a named CORS policy which can later be used in the application, such as within the middleware. To call it from the CORS middleware, use the method call UseCors(*policy name*) which will receive the policy name declared earlier. 
				It is important that the call to the UseCors middleware is made before any resources which may depend on CORS are called. If the call to UseCors appears after UseMvc, any ASP.NET Core MVC logic will ignore CORS.
			CORS in MVC Controllers and Actions:
				As an additional option in utilizing CORS is the ability to control CORS on a per action or controller level by utilizing the EnableCors(*cors policy*) attribute on the action or controller. It utilizes CORS policies which are created as part of the service registration and as such you can have multiple policies in effect for different areas within your application. 
				In the event you wish to disallow CORS on a specific action, you can use the DisableCors attribute on a specific action inside a controller implementing EnableCors. This will prevent CORS from being used on the specific action.
			Additional CORS Policy Options:
				As part of CorsPolicyBuilder you can use a wide variety of optional methods to configure which servers have permissions to access the application. You can use multiple methods in the same policy to create more specific policies and as a result provide a safer solution. 
				Several of the methods available to you include:
					• WithOrigins(*origins*). Expects an array of origins. All origins specified will be able to perform CORS operations. 
					• AllowAnyOrigin(). Allows all origins to access via CORS. 
					• AllowAnyMethod(). Allows using all HTTP methods. Note that without this setting, some types of request methods such as OPTIONS may fail. 
					• WithHeaders(*headers*). Expects a list of accepted header values. At least one must be present in the request. 
		Secure Sockets Layer:
			HTTP is not a secure protocol. Data transferred through HTTP is not encrypted and can be intercepted and changed by entities residing as part of the pipeline. These entities can include, but are not limited, to ISPs, various temporary internet providers such as hotels, or even malicious attackers throughout the internet. 
			While utilizing HTTP these entities can change data passing through the sites, whether to inject their own content instead of existing content, purposefully alter or steal data, or even redirect users to web sites completely under their control. 
			This presents a serious danger to both the user and the application and you can work to help reduce these effects by using the more secure HTTPS protocol. The HTTPS protocol utilizes the Secure Sockets Layer (SSL) protocol on top of an HTTP infrastructure to encrypt data sent between a client and a server. This is done by utilizing approved security certificates to provide hashing, which is then used to encrypt and decrypt the data that is sent. 
			The security certificates comprise of both a private and public key. When the initial connection is made the client and the server both agree on a security certificate which is approved by a certificate authority (CA) which is trusted by both the client and the server. There are multiple security authorities in charge of issuing certificates and certificates are usually released for individual websites, often with details specifically involving those sites. This is done to prevent communication being open for malicious attacks. The client will be able to access a public key for the site which can be used for performing the encoding on the information, while only the server has access to the private decryption key. 
			If the client and the server do not have any matching certificates, communication will not occur, as a secure connection cannot be established.
			How to Enforce HTTPS in ASP.NET Core MVC:
				In ASP.NET Core MVC, you can fairly easily ensure that your users utilize HTTPS while using your application. This can be done by adding the UseHttpsRedirection() middleware inside your application. By adding a call to it on the IApplicationBuilder object within the Configure method, the server will be configured to redirect any requests that reach the server's HTTP port to HTTPS. This ensures that users inside your application are using HTTPS for communication and not HTTP. 
				It is important to note that the call to the UseHttpsRedirection middleware must appear before any middleware which can provide the user with any html files, resources, or actions. This includes, but is not limited to, UseMvc and UseStaticFiles. If they are called before UseHttpsRedirection, these resources will be accessible to normal HTTP traffic.
			Setting up the Development Certificate:
				During development and while testing deployment, you may find that you do not always have an available certificate from a trusted source. Obtaining a certificate is an expensive process requiring you to select a certificate authority and buy a license to use a certificate. This is a required part of a product when it is released, but it is not usually feasible for testing purposes, which may occur years before the application is released to customers. 
				In order to enable testing your application on deployments, the .NET Core SDK which is used for running ASP.NET Core application adds a special certificate, the "ASP.NET Core HTTPS development certificate" on the server on which it is installed. You can then add the certificate to the trusted certificate list for the machine by using the .NET CLI. You will learn more about the deployment process and the .NET CLI in Module 14, “Hosting and Deployment”. 
				The command for adding the development certificate within the .NET CLI is: dotnet dev-certs https --trust. When you type it on a server with the Microsoft .NET Core SDK installed, it will be added to the trusted certificates and that machine will be able to access development builds on a browser. 
				Installing ASP.NET Core HTTPS Development Certificate:
					Example:
						dotnet dev-certs https --trust

Module 12: Performance and Communication
	Lesson 1: Implementing a Caching Strategy
		Caching allows you to store commonly repeated requests, preventing the need to perform the same logic repeatedly. By using caching, you can reuse data that has already been loaded and present it to the user. This provides the user with a fast response time and reduces system resources used in conducting the logic for the action. 
		Why Use Caching?
			Caching involves storing information in the memory of a web server, this information can include but isn't limited to rendered pages, the results of calculations, and retrieving information from a database. If the content that is rendered does not change much, you can choose to store it in the server memory or on distributed caching servers. When a user performs a request to content that is cached, the server will send the cached content instead of repeating costly operations, allowing the server to reply in a more time-sensitive way. 
			There are many advantages of using caching, which include: 
				• Reduces the need to repeatedly retrieve the same information from the database. 
				• Reduces the need to reprocess data, if a user repeats the same request multiple times.
				• Helps improve the performance of a web application, by reducing the load on servers.
				• Helps increase the number of users who can access the server farm simultaneously.
			However, you should consider several important details before implementing caching. If you perform caching for content that frequently changes, you will quickly end up in situations where users are receiving irrelevant content. When performing the caching, the data is stored in the web server and used instead of the most recent data existing. This can be highly useful for things like maintaining a shopping cart for the user or for rendering collections of strings which are calculated the first time but remain consistent. However, data that changes regularly, such as various real-time data such as product inventories and areas where the content changes between visits. 
			Caching can also be handled on environments running across multiple servers. When working with multiple hosting servers, you will need to either work with a sticky session, which means that the same server handles all requests from the same client, or use a distributed cache, which can handle caching for multiple servers in a separate dedicated server. Otherwise, it will result in multiple servers handling separate requests with each server creating its own cache, which results in a waste of memory. 
			Servers have a limited amount of free memory they can allocate towards caching at any given moment in time. This means that sometimes the server will need to clear out older cache entries on its own. To assist the server in handling these issues, you can set both the caching priority and the caching lifetime. By setting a specific lifetime, you can ensure that cached data doesn't remain for too long, particularly in systems where the cached data refreshes infrequently. By setting caching priority, you can determine that if there are memory issues, lower priority cached data will be deleted first, whereas higher priority cached data will remain unless there is no lower priority cached data. 
			Caching lifetime can include customization options such as:
				• Setting cache expiry time in absolute units of time.
				• Setting a date at which the cache will be cleared.
				• Setting a period of time for deletion since the cache is last accessed.
				• Setting if the expiration is sliding expiration, and the timer is reset whenever the cached resource is accessed.
			Caching priority can be set to Low, Normal, High, or NeverRemove to determine priority for removal whenever memory is cleaned up. 
			Cache Tag Helper:
				One the easiest ways to cache in an ASP.NET Core MVC application is by using the cache tag helper. The cache tag helper is used to envelop the ASP.NET Core MVC code which is expected to yield repeated results. By adding this tag helper, the code inside will be rendered on the first request by the user, followed by persisting the code rendered from the first call on future calls while the rendered data remains in the cache. 
				By default, the cache tag helper persists for 20 minutes before expiry.
				Additionally, the cache tag helper supports a variety of additional attributes that can be used to further customize the caching logic to fit with the requirements. 
				By using the enabled attribute, you can enable and disable caching for specific segments of code.
				You can also use a variety of attributes to specify expiration behavior for the cached data, which include: 
					• expires-on: Accepts a DateTime object which determines until when the caching will remain. 
					• expires-after: Accepts a TimeSpan object which determines the amount of time to keep the cached data since the original request was made. If a new request is made the caching time will not be changed. 
					• expires-sliding: Accepts a TimeSpan object which determines the amount of time to keep the cached data since the previous request was made. 
				An additional attribute that can be used to further customize when caching happens is the vary by attribute. This allows you to maintain separate caches for different properties and even use vary by on its own to set up your own logic. This allows you, for instance, to vary the cache by various model or ViewBag properties. 
				Several common options for vary-by are as follows: 
					• vary-by-query: Accepts single or multiple values separated by commas, which are key in the query strings. The caching will be based on the values of the specified query string keys and a separate cache will be used for every different value. 
					• vary-by-cookie: Accepts single or multiple values separated by commas, which are cookie names. The caching will be based on the values of the specified cookies and a separate cache will be used for every different value. 
					• vary-by-route: Accepts single or multiple values separated by commas, which are route data parameter names. The caching will be based on the values of the route data parameter names and a separate cache will be used for every different value. 
					• vary-by-user: Accepts the strings true and false. The caching will be based the currently logged in user and include non-authenticated users. Logging out or logging in will clear the cache. 
					• vary-by-header: Accepts single or multiple values separated by commas, which are field names in the request header. The caching will be based on the values of the specified request headers and a separate cache will be used for every different value. 
					• vary-by: Allows setting custom string value to be used for caching. Whenever the value changes a separate cache will be used. By concatenating strings, you can create a condition as specific as required. 
				Another crucial attribute for caching is priority. By setting this attribute you can determine when the cache should be cleared. The valid values for this attribute are the enum values of the CacheItemPriority enum, from the Microsoft.Extensions.Caching.Memory namespace, with Low being the first to be cleared and NeverRemove being the lowest priority to clear due to memory constraints. By default, Normal will be used. 
				Despite being named NeverRemove, if the memory issues become severe enough, items tagged as NeverRemove can be removed.
		The Data Cache:
			Caching in ASP.NET Core MVC isn't limited to views. Caching can be of great help inside other components such as controllers. This can be useful to retain frequently repeating data and can be used to greatly improve performance, removing repeated calculations, and attempts to access the database. Unlike the cache tag helper, data cache can be useful in multiple pages, leading both to have their own advantages and disadvantages. When creating a web application, it is a good idea to think of the best form of caching for each individual case. 
			One of the simplest means of server-side caching is by using the IMemoryCache service. This service allows you to create a cache for any type and later retrieve it if it is available. 
			The first step for configuring the data cache is adding the IMemoryCache service inside of the ConfigureServices method in the Startup class. This can be done by calling the AddMemoryCache on the IServiceCollection parameter. Once this is done, the IMemoryCache can be injected throughout the components of your application. 
			To store an item in the cache, you can use the Set method of the IMemoryCache parameter. This method operates on a key and value pair, with the first parameter being a key with the object type, and the second parameter being a value of the object type. This allows any key or value to be set in the cache. 
			To retrieve an item from the cache, you can use the TryGetValue method of the IMemoryCache parameter. It receives a parameter of the object type which is the key and a second output parameter of the object type, which is the variable that will receive the value from within the cache. This method returns true if the data is cached, or false if it is not.
			Additional methods for using caching include the GetOrCreate and GetOrCreateAsync methods. These methods will check the key for you and if the cache item with the matching key does not exist, perform the logic for retrieving the data. This can help further simplify the code. GetOrCreateAsync should be used if the caching logic is reliant on tasks. 
			Sometimes the default caching settings isn't useful to you and you want to customize it further. To do this, you can provide the Set method an additional third parameter of the type MemoryCacheEntryOptions. The MemoryCacheEntryOptions allows you to set useful parameters such as: 
				• AbsoluteExpiration. The absolute date in which the cache entry expires. 
				• PostEvictionCallBacks. A callback function that will be called when the cache entry is removed. 
				• Priority. The priority of the cache entry. 
				• SlidingExpiration. An offset after which the cache is cleared. This is reset every time the cache is accessed.
		Distributed Cache:
			A major issue with regular caching is that it is reliant on a specific web server. In a sticky server scenario, this means that you will need to keep working with the same server, even if the server becomes busy and non-responsive. In a non-sticky server scenario, any attempts to use caching are ineffective, possibly resulting in the same data getting cached across multiple servers. 
			A distributed cache is instead held in a centralized area and is not affected by individual web servers. Therefore, the cache will persist even when servers are taken down. Additionally, data stored on a distributed cache is used for handling requests from multiple users with the same cache. This can greatly reduce the amount of cached data, leading to less storage needed and offering more comprehensive solutions. 
			It is important to note that unlike other caching options, distributed caching saves and reads the data as a byte[]. Therefore, you will often need to convert data types to use distributed caching correctly. 
			Configuring a Distributed Cache:
				A distributed cache is used through the IDistributedCache interface. However, IDistributedCache can be implemented by multiple different providers. As a general rule of thumb, you will provide the actual cache implementation itself in the ConfigureServices method by calling for the appropriate method. 
				A distributed cache can be applied in many different ways, such as: 
					• SQL Server cache, added by calling AddDistributedSqlServerCache. This will connect to a cache on a specially configured SQL Server. It requires a connection string, schema name, and table name in its setup. 
					• Redis distributed cache, added by calling AddDistributedRedisCache. This will connect to a Redis data store, Redis being an open source in-memory data store. It requires configuration and Instance name in its setup.
				Working with IDistributedCache:
					Once the distributed cache has been configured, you are able to inject the IDistributedCache throughout your application components. The IDistibutedCache interface exposes several useful methods for interacting with the cache. Most commonly, the Set and Get method will be used. There is also an option to use asynchronous variants by calling SetAsync and GetAsync. 
					The Get method receives a key of type string as a parameter and returns a value of type byte[] if the key is found. If it is not found, null will be returned instead. It is important to remember you will often need to convert the data returned from byte[] to another data type used in the application. 
					The Set method receives a key of type string, a value to be stored of type byte[] and an optional object of type DistributedCacheEntryOptions, which allows setting expiration time for the cache based on sliding expiration, absolute expiration and absolute expiration relative to the current time.
				Working with Distributed Cache Tag Helper:
					An additional way to utilize distributed cache is by using the distributed-cache tag helper. This tag helper is nearly identical to the normal cache tag helper, with a few exceptions. 
					The main change is that it requires a name attribute. The name attribute is shared between all instances of the same name throughout the application and even across multiple separate clients. This means that if two separate users access the same cache on separate browsers, they will receive the same result. This can help boost performance by a large margin for pages which change on a predictable schedule. 
					If the distributed-cache tag helper is used without setting up a distributed cache, the system will use a fallback, and it will behave like a normal cache tag helper. 
	Lesson 2: Managing State
		State meanwhile allows achieving a state of consistency between different requests. By utilizing various forms of state management, you can transform the normally stateless web experience into one that is custom tailored to individual clients, with different users enjoying a separate and relevant experience in the same application. 
		Why Store State Information?
			HTTP is a stateless protocol. This means that every single request is independent of each other. However, sometimes you will want to handle several requests in order and to keep them connected to some degree.
			Fortunately, while HTTP itself is stateless you can use various methods to create a state on the server. There are many different methods you can use, each of which works differently and allows you to maintain persistent information across multiple connections. By utilizing these methods, you can create a persistent environment for the user, with actions leading to further changes as they use your application. 
			Most applications will end up supporting state in some form or another, but it can take many different forms. Common technologies used for state management from the client side include cookies, query strings and data from various HTML controls, such as hidden fields. Meanwhile, in the server, HttpContext, alongside the TempData property of the controllers can be used to synchronize different requests from the same client to result in a coherent and seamless experience for the user, with individual technologies used for implementing different requirements as needed. 
			In addition, the client itself can also maintain a degree of session management by using the HTML5 web storage API. This is a form of session management that is entirely in JavaScript on the client side. 
			It is important to note that authentication cannot be implemented in ASP.NET Core MVC without utilizing some form of state management. Without any state management, authentication would be required in every single request.
		State Storage Options:
			There are many different technologies that can be used to maintain the application state. Throughout your application, you will often need to decide which ones are relevant for you and each has its pros and cons towards storing state. 
			Client-Side Options:
				Query Strings:
					One of the simplest ways to obtain persistence throughout the application is by using query strings in requests. By doing this, you can pass a limited amount of extra information on every request. While a relatively simple way to preserve session state, it can quickly end up with sending very large amounts of information from multiple pages, as well as manually needing to be added to individual requests. Using query strings works best when you need to add very small amounts of additional state related information. To do this, you just need to append the data from previous requests to your current request. 
					Query strings are not secure, and you should never use them to send secure data such as usernames and passwords. They should only be used for adding minimal amounts of data, as it is easy to use them maliciously.
				Hidden Fields:
					Another possibility for retaining data is by using input elements with the type hidden. When included as part of a form, these will be sent whenever requests are made from a form and are frequently useful when trying to create multi-page forms, since a hidden field with an identifying value can be used. A major weakness of hidden fields is that they can be relatively easily maliciously changed by the user, requiring validation on the server side. 
				Cookies:
					Cookies are a form of client-side data retention that allows the storage of key-value pairs. Unlike the previous methods, cookies for a site are always sent on every request, which has both advantages and disadvantages. On the one hand, as developers, you do not need to explicitly add cookies at any point, but on the other hand, if cookies contain too much data, it can slow down requests to the server. 
					Cookies are usually limited in size and quantity on browsers, and therefore it is a good idea to use a relatively small amount of focused data. A good example of the kind of data frequently stored in cookies is authentication tokens which are returned to the client after a successful login. The client will then store that token on every request, which the server can use to validate the session for the sender. 
					It is important to note that cookies can easily be manipulated by knowledgeable users, and thus the server should always validate that the cookies are returning expected results. 
					Also note that cookies can be set up to remain after the browser is closed and can be customized to persist for different amounts of time, allowing behavior similar to caching. 
			Server-Side Options:
				TempData:
					One of the first methods to handle session state on the server side is by using TempData. This is a special data store which stores items until they are read, at which point they are deleted. The TempData is shared across all controllers of the application, allowing you to interact with it between completely unrelated requests. 
					TempData can have items added to it by setting a key, and the items are removed when getting a value for a key. Additionally, you are also able to use the Keep method, which will mark a key to not be deleted when getting from it, or the Peek method, which allows getting the value of a key without deleting it. 
					By default, TempData utilizes browser cookies as the storage mechanism. It is however also possible to use session-based storage by appending a call to AddSessionStateTempDataProvider method to the AddMvc method in the ConfigureServices method. By doing this, the storage mechanism used for sessions will be used instead of cookies.
				HttpContext.Items:
					One way of managing state on the server side is by using the Items property of the HttpContext object that is used by middleware in your application. The Items property is a Dictionary of object type keys and object type values, allowing you to store and access any property you need to be later used in other middleware. 
					This method of state management is relevant on a request-specific basis, since this property is part of the request itself. However, it allows you to communicate between different middleware and share important information without having to conduct complex methods repeatedly. 
					When creating middleware intended for specific applications, the use of strings as keys for the HttpContext.Items dictionary is very useful and will frequently be a good choice. However, when creating middleware that is intended to be used in more than one application, you should use specific objects as keys. This will ensure that your HttpContext.Items do not accidentally clash with those of another middleware. 
				Cache:
					An additional means of state management you can use is caching. Caching can be an efficient means of handling repeated requests and handling reoccurring information. Caching is useful for handling the state for repeating requests, ensuring data consistency. 
					When using distributed caching, you should not store any user-specific information in the cache. This could result in at best invalid information being displayed, and at worst present a major security breach.
				Dependency Injection:
					Since services can be injected into any component throughout your application, they can also be used for state management. There is no end to possible usages for services in state management, and by using them any desired behavior can be achieved.
				Session State:
					Another method for managing state is the Session State. This is a built-in mechanism designed to specifically handle sessions between the server and client. By utilizing specific cookies from the client, the server is able to identify the specific client and apply user-specific logic throughout the request. The same session can persist for lengthy periods of time, allowing users to keep using the system without requiring constant authentication. 
					A session will only be created when at least one value is retained as part of the sessions. Sessions will be specific to individual cookies, which are set to deletion when the browser is closed. This can allow a user to continue interacting with the application in a seamless manner. In the event an expired cookie is received, a new session will be opened for handling it. By default, sessions will persist for 20 minutes, but you can configure it as needed. You can also expire the session early by calling the Clear method of the ISession interface. 
					There is no default implementation for clearing sessions when the cookie expires, or when the client closes the browser. If you require this behavior, it will need to be created manually. 
					In single server scenarios, it is possible to store the session in a memory cache. In multiple servers’ scenarios, sessions will be stored by using a distributed cache. This allows your session to work across multiple servers, with the client being agnostic to which server handled the last request, allowing for a convenient user experience.
		Configuring Session State:
			The session state is a built-in mechanism allowing you to easily access a shared data storage that works on a per session basis. This allows you to couple between a specific client, and data relevant to it, without consistently managing identity on every single request. The built-in mechanism uses the distributed cache on the server, and cookies of the client to manage sessions, without requiring any additional work on your end. 
			To set up the session state, you will need to call the AddSession method of the IServiceCollection parameter in the ConfigureServices method. You can optionally supply an Action delegate as a parameter, which will be used to set up the SessionOptions. The SessionOptions includes important properties that allow you to configure session behavior, such as cookie behavior via the Cookie property, and how long sessions will be stored on the server via the IdleTimeout property.
			You also need to call the UseSession method on the IApplicationBuilder parameter in the Configure method. This will set up the Session property of the HttpContext, allowing it to be used in other middleware or in various components throughout the application. It is important to note that UseSession should be called before any middleware that will use the session or you will encounter an InvalidOperationException when trying to use the session.
			Configuring Session State in Distributed Environments:
				In order to set up a session state in a distributed environment, you will need set up a distributed memory cache, in the ConfigureServices method.
			AddDistributedMemoryCache method technically implements the IDistributedCache, but it behaves similarly to IMemoryCache and is not distributed. It is useful for creating caching with the intention of adding multiple servers in the future and as a method of quickly testing distributed cache methods on development environments. It does not actually handle distributed caching logic.
			Get and Set Session Values:
				To use the session in your application, you will use the HttpContext.Session static property. This property exposes the session API and allows getting and setting values through a variety of useful methods. 
				You can use the Set(*key*, *value*) method which receives a key of type string and a value of type byte[]. It will then store the value on the session until it is overwritten by a future call to Set or if the session expires. 
				You can use the Get(*key*) extension method which receives a key of type string to retrieve a value of type byte[] which has previously been set by using the Set method. If the value is not present in the session it will return null instead. 
				An alternative option to Get is TryGetValue(*key*, out *value*), which receives a key of type string, and an output value parameter which is of type byte[] and returns true if the value was found or false if it was not found. 
				Another useful method is Remove(*key*), which receives a key of type string, and removes the entry from the session if it exists. 
				Additionally, there are also type-specific variants for Get and Set: 
					• SetString(*key*, *value*). Same as Set, but value is string, rather than byte[]. 
					• GetString(*key*). Same as Get, but returns string, rather than byte[]. 
					• SetInt32(*key*). Same as Set, but value is int, rather than byte[]. 
					• GetInt32(*key*). Same as Get, but returns int, rather than byte[]. 
		Using the HTML5 Web Storage API:
			One additional option for state management that is entirely client-side based is HTML5 Web Storage API. By using a simple API, you are able to store data on the client side, which can then be used later by your application. These APIs are both specific to JavaScript and cannot be directly interacted with from the server. These forms of storage are specific to individual sites. Attempting to access local storage for a different application will not return any data. 
			There are two types of storage exposed by the HTML5 Web Storage API, differing by the way in which the data is stored. Each means of storage keeps its own separate data and they will not be shared. The same key strings can be safely used for both:
				• localStorage. Items stored with localStorage will persist until deleted. They will be relevant between different browser windows and tabs and will still exist when the browser is closed. This is useful with information that does not need to change often and data that needs to last for a long time. 
				• sessionStorage. Items stored with sessionStorage are specific to the individual browser tab and will not be accessible from other windows or tabs. It will always be cleared once the tab is closed. This is useful for information relating to the current browsing session and for sensitive information. 
			It is important to note that these APIs store data in browser specific storage. Data stored in one browser will never be accessible to another browser. 
			These APIs hold a dictionary of key-value pairs. Both the key and value are strings and trying to store any other type will store the toString value of that variable. 
			To use the API itself, you can use the following functions, which exist on both localStorage and sessionStorage: 
				• Set(*key*, *value*). Receives a string key parameter and a string value parameter. The value will be stored under the key in the storage. 
				• Get(*key*). Receives a string key parameter and returns the currently stored value for that key. If there is no value for the key, null will be returned instead. 
				• Remove(*key*). Receives a string key parameter and removes that entry from the storage if it exists. 
			If you wish to store an object or array rather than a string, you can parse JavaScript objects into JSON format by using the JSON.stringify() function and convert JSON data back into JavaScript data by using the JSON.parse function.	
	Lesson 3: Two-Way Communication
		SignalR is a framework that allows the abstraction of several different communication protocols into an easy to use API, which allows you to easily create a single set of tools on the server and client to facilitate two-way communications. This allows you to focus on the logic you wish to implement while allowing you to not have to cater to specific browsers.
		The Web Sockets Protocol:
			A major issue with the HTTP protocol is that it is a one-way protocol. The client makes a request to the server, and in return, the server returns a response. The server has no way of initiating a connection with the client. 
			Common resolutions involved sending requests to the server which were kept on the server until the event occurred on the server and it finally sent a response (this is called long polling) or creating an endless loop sending requests at set intervals to get updates. 
			Both of these methods have large flaws, such as spending resources on keeping multiple requests running, sending and receiving a large number of requests increasing the bandwidth used. These can quickly add up and reduce the server's ability to serve additional users. 
			A solution to these issues comes in the form of the WebSocket protocol. WebSocket is a method of achieving two-way communications between a server and a client. The client will send connection details to the server, which will then be able to use the existing details to make callbacks in the future. This allows the creation of a proper method of two-way communication. 
			The WebSocket utilizes the existing HTTP framework to create this binding. When the initial connection for WebSockets is made, the HTTP Upgrade header is used. This lets the server know that the WebSocket protocol needs to be used, and as long as the server supports it, the status code 101, which represents switching protocol, will be used. At this point, the client will send its payload and the server will store the client details for future requests. 
			Once the connection has been made both the client and server can use the existing connection to send messages to each other. This facilitates two-way communication and allows proper communication between both server and client. 
			The client can also choose to close the connection at specific points, allowing the server to clear up any related resources, and further improve our ability to handle requests. The server can also choose to close connections, such as with a timeout, allowing the server to not retain irrelevant data for long. 
			The WebSocket protocol is frequently used for real-time websites such as stock sites, gaming sites, and chatting sites, as all of those require frequent updates, often user reliant. For example, a multiplayer game requires tracking the movement of all games, while a chat will often alert users of other users writing messages or joining and leaving channels. Note that for websites with infrequent updates, it may be better to perform a request every so often and the WebSocket protocol is more useful for real-time uses. 
			While WebSockets is a standard protocol that is defined and required by the W3C (World Wide Web Consortium) as part of a modern browser, older browsers may not support WebSockets and other options will be required. As with many other technologies the use of WebSockets should be considered based on the target audience for your browsers. 
		Using SignalR:
			ASP.NET Core SignalR is a library which helps to create applications that utilize two-way communications throughout your application. SignalR will automatically determine and use optimal techniques for achieving two-way communication, preferring WebSockets when available. However, if the browser does not support WebSockets, other options such as long polling will be used instead. This allows you to create applications without having to worry about the client browser, while still offering a two-way communication experience. 
			SignalR itself is mainly handled in two sections. The first section, called the hub, is server based. The hub declares various methods to be called by the client, with every method being able in turn to reply to one or more clients. 
			The other section is in the client. It is responsible for connection to a specific hub, invoking methods on the hub, and receiving messages from the hub. This can be implemented in various different ways, but in ASP.NET Core MVC applications it is handled in JavaScript. 
			By default, SignalR uses JSON format to transfer data, although it also supports the MessagePack binary protocol, which can be enabled and used to transfer data even faster. Take note that enabling it will require additional steps to parsing it on the client. 
			SignalR in the Server:
				Configure SignalR:
					The first step in adding SignalR to an application is to add the necessary configuration in the Startup class. First, in the ConfigureServices method, on the IServiceCollection parameter, add a call to the AddSignalR method. 
					Second, in the Configure method, on the IApplicationBuilder parameter, add a call to the UseSignalR method. This method expects an Action that has an HubRouteBuilder generic parameter. This parameter allows you to create routes for the various hubs in use throughout your application. 
					It is a good idea to set up SignalR middleware before setting up ASP.NET Core MVC middleware in the pipeline. This ensures SignalR related routes will not accidentally be sent to the ASP.NET Core MVC framework. 
				SignalR Hubs:
					The next step is to set up a server hub. This can be done by adding a new class which inherits from the Microsoft.AspNetCore.SignalR.Hub class. This class will implement the various hub methods. Generally, hub methods will be async methods that return a Task. The only restriction on parameters is that they have to be serializable, but any number of parameters can be used. 
					Inside the hub methods, you will want to also send messages to the various clients. This can be done by using the Clients property. This property exposes a variety of possible client options you can send to, all of which implement the IClientProxy interface. This interface exposes the SendAsync method, which allows various overloads of sending parameters. The most common utilization for it is SendAsync(*method*, *arg1*, *arg2*…. *argN*). All parameters are objects which are serialized into JSON or MessagePack and then sent to the various clients. Not every method in the hub will necessarily call the SendAsync method, but most will. 
					The various client properties and methods implementing the IClientProxy include: 
						• All. Property which returns all clients connected to the hub. 
						• Caller. Property which returns the client which invoked the hub method. 
						• Others. Property which returns all clients other than the one which invoked the hub method. 
						• AllExcept. Method which receives a list of string connection IDs of clients and returns all clients except for the ones with matching connection ID. 
						• Client. Method which receives a string with a connection ID of a specific client and returns the matching client. 
						• Clients. Method which receives a list of string connection IDs of clients and returns all clients with a matching connection ID. 
						• Group. Method which receives a string group name and returns all clients which have been assigned to the matching group name. 
						• GroupExcept. Method which receives a string group name and a list of string connection IDs of clients and returns all clients which have been assigned to the matching group name with the exception of the matching connection IDs. 
						• Groups. Method which receives a list of string group names and returns all clients which have been assigned to the matching group name. 
						• OthersInGroup. Method which receives a string group name and returns all clients which have been assigned to the matching group name with the exception of the calling user. 
						• User. Method which receives a string user ID of a specific user, by default from the ClaimTypes.NameIdentifier and returns the matching user (This can apply to more than one client if the user is connected from multiple clients). 
						• Users. Method which receives a list of string user IDs, by default from the ClaimTypes.NameIdentifier and returns the matching user (This can apply to more than one client if the user is connected from multiple clients). 
					The hub can make multiple SendAsync requests, potentially with different method names, and perform calls to additional classes that perform additional logic. You can use dependency injection with hubs, as with other components of ASP.NET Core applications.
				SignalR Routing:
					Once the hub has been configured, you are able to map routes to it inside the Configure method. This can be done by calling the MapHub method on the HubRouteBuilder parameter. The MapHub method uses the following syntax: MapHub<T>(*path*). T is the type of the hub which will be instantiated and path is a string which represents the relative URL path on which the SignalR hub will be hosted. Once this is done, the hub is hosted and accessible on the server.
			SignalR in the CLient:
				Add SignalR Client Library:
					After the server is done, you will need to set up the client. Since controllers and views are rendered on the server during the initial request, you will need to use JavaScript to handle this. The first step is to add a dependency to the npm configuration. You will need to add a dependency to @aspnet/signalr as part of package.json.
				Use SignalR Client JavaScript:
					Once this is done, you will need to build a view or HTML file to accommodate your logic and to interact with the SignalR. This file can be as simple or as complex as you wish and does not require any particular references to SignalR. However, you will need to load the SignalR JavaScript file from the npm package either in the layout or in the script itself. The SignalR JavaScript file will need to be called before your own scripts as with most third-party libraries. 
				Connect to a Hub:
					The next step is to set up the client to handle the SignalR connection. You will need to build a connection. In order to do this, you must create a new object of type signalR.HubConnectionBuilder. After this object is created, you will need to call the withUrl(*hubUrl*) method on the connection builder, specifying the URL of the hub, and finally you will need to call the build method, which returns the connection object that you will use.
				Register for Calls from the Hub:
					The next step in configuring the client is to register to calls from the server. This can be done by calling the on(*methodName*, *methodFunction*) method of the connection object. The method name will match the method called by SendAsync on the hub, while the method function will receive the parameters from the SendAsync call and process them as needed. You can add as many calls to the on method as needed to handle different methods from the hub.
				Start, Error Handling and Logging:
					Once all methods have been properly registered by using the on method, you will need to start the connection by calling the start method on the connection object. This method will start the connection with the server, by initially checking which transport types the server supports, before creating the actual connection, allowing the server and client to communicate through SignalR. 
					Since connections on the web are not infallible and errors occur, it is common to pipe a call to the catch(*errorFunction*) method after the call to start. This allows you to handle errors on the client side if something goes wrong. The error function receives a single parameter of the error that occurred. You can then use it to take any necessary actions. 
				Call Hub Methods from Client:
					Finally, to call methods on the hub, you will use the invoke(*methodName*, *param1*, *param2*…) method on the connection object. The method name is a string which must match a method that has been defined on the hub, while the remaining parameters should match the parameters of the method on the hub. Doing this will call the method which is defined on the hub.
				It is important to note, SignalR connections which do not use WebSockets will require sticky servers to be defined. Since, by default, WebSockets create a socket with a specific server, you do not need any special logic to handle this case. 
		Additional SignalR Settings:
			Sometimes you need to call methods on specific clients.
			This can be managed by using the Context property of the hub class. The Context property is of the HubCallerContext type and can be used to identify the user that made the call. Additionally, by overriding the OnConnectedAsync and OnDisconnectedAsync methods on the hub, you can perform initial and final operations on clients, allowing you to add or remove from groups or remove clients from being displayed. 
			Another useful property of the hub is the Groups property. This property is of the IGroupManager type and exposes the AddToGroupAsync and RemoveFromGroupAsync methods. Both methods receive a connection ID and a group to which the connection will be added to or removed from respectively. The connection ID can be found by using the ConnectionId property on the Context property. Note that if a client disconnects, the connection ID will be removed from all groups automatically. 
			The Groups property is exclusively used to add to and remove from Groups and nothing else. If you need a list of people inside a group or a list of groups you will need to manage it yourself.
			Serializing with MessagePack:
				By default, SignalR serializes message by using JSON. JSON, being a common message format, is easily serialized through different clients and is a widely recognized format. However, JSON is a text-based format and therefore less efficient at containing data than binary formats. While the performance change is negligible in small quantities of data, the size can add up in larger quantities and across many requests. 
				The MessagePack serialization format allows you to improve performance by sending smaller more compact data. As an additional benefit to security, it is much harder to read by looking at network traffic, as it requires the data to be converted from a MessagePack format, whereas JSON is immediately readable. 
				Using MessagePack requires a few extra steps to set up:
					1. Install the Microsoft.AspNetCore.SignalR.Protocols.MessagePack Nuget package. This will load the required server-side package to use MessagePack. 
					2. In the ConfigureServices method of the Startup class, in the call to AddSignalR on the IServiceCollection object, pipe a call to AddMessagePackProtocol. This will cause the hub to serialize and deserialize in the MessagePack format. 
					3. Add a dependency for @aspnet/signalr-protocol-msgpack in the package.json file. This will retrieve the signalr-protocol-msgpack library, as well as msgpack5, which is a dependency for signalr-protocol-msgpack. 
					4. Add references to msgpack5.js and then to signalr-protocol-msgpack.js in the HTML files. The order of these two files is very important. 
					5. In the JavaScript file, as part of the HubConnectionBuilder pipe, add a call to withHubProtocol(new signalR.protocols.msgpack.MessagePackHubProtocol)) before the call to build. This will set up the connection to use message pack on the client. 
				Configuring the SignalR Connection:
					Sometimes there are additional requirements in running SignalR. For instance, you may not wish to maintain connections which are unused for some time or you may wish instead to keep up the connection as long as possible. Additionally, you may wish to remove support for specific transport protocols to improve performance or fulfill particular needs. 
					This can be done by using an overload of the AddSignalR method of the ConfigureServices class. The overload accepts an action, which gets an instance of the HubOptions class. In this class, you can set various configuration properties that will affect all SignalR connections on the application. 
					Common configurations include:
						• HandshakeTimeout. This is the duration during which the client must send a message after the initial handshake or the connection will be closed. 
						• KeepAliveInterval. This is the duration during which the server will send a ping to the client to keep it alive. This is useful in a system where maintaining connections are important, but a long period could occur with no messages. 
						• SupportedProtocols. A collection which contains the protocols to be supported. Changing this collection can allow removing support for specific protocols. 
						• EnableDetailedErrors. A Boolean value which indicates if detailed error messages should be sent. By default, its value is false. Turning this to true will cause detailed exceptions to be sent. This could potentially present a security exploit. 
				Additionally, it is also possible to set up configurations on the client side. It is possible to add an additional options parameter to the withUrl method, allowing you to set up additional configurations. 
				Common configuration options include: 
					• transport. Allows setting which transport types are used by the client. The client will not use any ones that are omitted and they are separated by a bitwise or operator. 
					• serverTimeoutInMilliseconds. Sets a period of time which the client will wait for a reply from the server, before closing the connection. 

Module 13: Implementing Web APIs
	Lesson 1: Introducing Web APIs
		HTTP is a communication protocol that was created by Tim Berners-Lee and his team while working on the WorldWideWeb (later renamed to World Wide Web) project. Originally designed to transfer hypertext-based resources across computer networks, HTTP is an application layer protocol that acts as the primary protocol for many applications including the World Wide Web. 
		Web API is a full-featured framework for developing HTTP-based services. Using Web API gives developers reliable methods for creating, testing, and deploying HTTP-based services.
		HTTP Services:
			HTTP is a first-class application protocol that was built to power the World Wide Web. To support such a challenge, HTTP was built to allow applications to scale, taking into consideration concepts such as caching and stateless architecture. Today, HTTP is supported by many different devices and platforms reaching most computer systems available today. 
			HTTP also offers simplicity, by using text messages and following the request-response messaging pattern. HTTP differs from most application layer protocols because it was not designed as a Remote Procedure Calls (RPC) mechanism or a Remote Method Invocation (RMI) mechanism. Instead, HTTP provides semantics for retrieving and changing resources that can be accessed directly by using an address. 
			HTTP is used to develop both websites and services. Services developed by using HTTP are generally known as HTTP-based services. 
			Using URI:
				Uniform Resource Identifier (URI) is an addressing standard that is used by many protocols. HTTP uses URI as part of its resource-based approach to identify resources over the network. 
				HTTP URIs follow this structure:
					"http://" host [ ":" port ] [ absolute path [ "?" query ]] 
					• http://. This prefix is standard to HTTP requests and defines the HTTP URI schema to be used. 
					• Host. The host component of the URI identifies a computer by an IP address or a registered name. 
					• Port (optional). The port defines a specific port to be addressed. If not present, a default port will be used. Different schemas can define different default ports. The default port for HTTP is 80. 
					• Absolute path (optional). The path provides additional data that together with the query describes a resource. The path can have a hierarchical structure similar to a directory structure, separated by the slash sign (/). 
					• Query (optional). The query provides additional nonhierarchical data that together with the path describes a resource.
				Different URIs can be used to describe different resources. 
				When accessing each URI, a different set of data, also known as a representation, will be retrieved. 
			Using Verbs:
				HTTP defines a set of methods or verbs that add an action-like semantics to requests. Verbs are a central mechanism in HTTP and it is one of the mechanisms that make HTTP the powerful protocol it is. The following list shows the widely used verbs that are defined in HTTP: 
					• GET. Used to retrieve a representation of a resource. Requests intended to retrieve data based on the request URI. 
					• POST. Used to create, update, and by some protocols, retrieve entities from the server. Requests intended to send an entity to the server. The actual operation that is performed by the request is determined by the server. The server should return information about the outcome of the operation in the result. 
					• PUT. Used to create and update resources. Requests intended to store the entity sent in the request URI, completely overriding any existing entity in that URI. 
					• DELETE. Used to delete resources. Requests intended to delete the entity identified by the request URI. 
				Introduction to REST:
					Representational State Transfer (REST) describes an architectural style that takes advantage of the resource-based nature of HTTP. It was first used in 2000 by Roy Fielding, one of the authors of the HTTP, URI, and HTML specifications. Fielding described, in his doctoral dissertation, an architectural style that uses some elements of HTTP and the World Wide Web for creating scalable and extendable applications. 
					Today, REST is used to add important capabilities to a service. Services that use the REST architectural style are also known as RESTful services. RESTful services use the different HTTP verbs to allow the user to manipulate the resources and create a full API based on resources. 
			Media Types:
				HTTP was originally designed to transfer hypertext. Hypertext is a nonlinear format that contains references to other resources, some of which are other hypertext resources. However, some resources contain other formats such as image files and videos, which required HTTP to support the transfer of different types of message formats. To support different formats, HTTP uses Multipurpose Internet Mail Extensions (MIME) types, also known as media types. 
				In HTTP, media types are declared by using headers as part of a process that is known as content negotiation. When a client sends a request, it can send a list of requested media types, and in order of preference, it can accept them in the response. The server should try to fulfill the request for content as specified by the client. Content negotiation enables servers and clients to set the expectation of what content they should expect during their HTTP transaction.
		HTTP Messages:
			HTTP is a simple request-response protocol. All HTTP messages contain the following elements: 
				• Start-line
				• Headers 
				• An empty line
				• Body (optional)
			Request Messages:
				Request messages are sent by the client to the server. Request messages have a specific structure based on the general structure of the HTTP messages.
				Example:
					GET http://localhost:4392/travelers/1 HTTP/1.1
					Accept: text/html, application/xhtml+xml, */*
					Accept-Language: en-US,en;q=0.7,he;q=0.3
					User-Agent: Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; WOW64; Trident/6.0)
					Accept-Encoding: gzip, deflate
					Host: localhost:4392
					DNT: 1
					Connection: Keep-Alive
				The first and the most distinct difference between the request and response messages is the structure of the start-line, called request-lines. 
				Request-line:
					This HTTP request messages start-line has a typical request-line with the following space-delimited parts: 
						• HTTP method. This HTTP request message uses the GET method, which indicates that the client is trying to retrieve a resource. 
						• Request URI. This part represents the URI to which the message is being sent. 
						• HTTP version. This part indicates that the message uses HTTP version 1.1. 
				Headers:
					This request message also has several headers that provide metadata for the request. Although headers exist in both response and request messages, some headers are used exclusively by one of them. For example, the Accept header is used in requests to communicate the kinds of responses the clients would prefer to receive. This header is a part of a process known as content negotiation. 
				Body:
					The request message has no body. This is typical of requests that use the GET method.
			Response Messages:
				Response messages also have a specific structure based on the general structure of HTTP messages. 
				Example:
					HTTP/1.1 200 OK
					Server: ASP.NET Development Server/11.0.0.0
					Date: Tue, 13 Nov 2012 18:05:11 GMT
					X-AspNet-Version: 4.0.30319
					Cache-Control: no-cache
					Pragma: no-cache
					Expires: -1
					Content-Type: application/json; charset=utf-8
					Content-Length: 188
					Connection: Close
{"TravelerId":1,"TravelerUserIdentity":"aaabbbccc","FirstName":"FirstName1","LastName":"LastName1","MobilePhone":"555-555-5555","HomeAddress":"One microsoft road","Passport":"AB123456789"}
				Status Line:
					HTTP response start-lines are called status-lines. This HTTP response message has a typical status-line with the following space-delimited parts: 
						• HTTP version. This part indicates that the message uses HTTP version 1.1. 
						• Status-Code. Status-codes help define the result of the request. This message returns a status-code of 200, which indicates a successful operation. Status codes will be covered in the next topic, “Status Codes”. 
						• Reason-Phrase. A reason-phrase is a short text that describes the status code, providing a human-readable version of the status code. 
				Headers:
					Like the request message, the response message also has headers. Some headers are unique for HTTP responses. For example, the Server header provides technical information about the server software being used. The Cache-Control and Pragma headers describe how caching mechanisms should treat the message. 
					Other headers, such as the Content-Type and Content-Length, provide metadata for the message body and are used in both requests and responses that have a body. 
				Body:
					A response message returns a representation of a resource in JavaScript Object Notation (JSON). The JSON, in this case, contains information about a specific traveler in a travel management system. The format of the representation is communicated by using the Content-Type header describing the media type.
		Status Codes:
			Status-codes are three-digit integers returned as a part of response messages status-lines. Status codes describe the result of the effort of the server to satisfy the request. The next section of the status line after the status code is the reason-phrase, a human-readable textual description of the status-code. 
			Status codes are divided into five classes or categories. The first digit of the status code indicates the class of the status: 
				Class					Usage																		Examples
				1xx – Informational		Codes that return informational response about the state of the				• 101 Switching Protocols
										connection.
				2xx – Successful		Codes that indicate the request was successfully received and accepted		• 200 OK
										by the server.																• 201 Created
				3xx – Redirection       Codes that indicate that additional action should be taken by the client	• 301 Moved Permanently
										(usually in respect to different network addresses) in order to achieve		• 302 Found
										the result that you want.													• 303 See Other
				4xx – Client Error		Codes that indicate an error that is caused by the client’s request.		• 400 Bad Request
										This might be caused by a wrong address, bad message format, or any kind	• 401 Unauthorized 
										of invalid data passed in the client’s request.								• 404 Not Found
				5xx – Server Error		Codes that indicate an error that was caused by the server while it tried	• 500 Internal Server
										to process a seemingly valid request.										• 505 HTTP Version Not Supported
		Introduction to Web API:
			HTTP has been around ever since the World Wide Web was created in the early 1990s, but its adoption as an application protocol for developing services took time. In the early years of the web, SOAP was considered the application protocol of choice by most developers. SOAP provided a robust platform for developing RPC-style services. 
			With the appearance of internet-scale applications and the growing popularity of web 2.0, it became clear that SOAP was not fit for such challenges and HTTP received more and more attention. 
			HTTP in the .NET Framework and the .NET Core:
				For the better part of the first decade of its existence, the .NET Framework did not have a first-class framework for building HTTP services. At first, ASP.NET provided a platform for creating HTML-based web-pages and ASP.NET web services, and later-on Windows Communication Foundation (WCF) provided SOAP-based platforms. For these reasons, HTTP never received the attention it deserved. 
				When WCF first came out in .NET 3.0, it was a SOAP-only framework. As the world started to use HTTP as the application-layer protocol for developing services, Microsoft started to make investments in extending WCF for supporting simple HTTP scenarios. By the next release of the .NET Framework (.NET 3.5), WCF had new capabilities. These included a new kind of binding called WebHttpBinding and the new attributes for mapping methods to HTTP requests. 
				In 2009, Microsoft released the WCF REST Starter Kit. This added the new WebServiceHost class for hosting HTTP-based services, and also new capabilities like help pages and Atom support. When the .NET Framework version 4 was released most of the capabilities of the WCF REST Starter Kit were already rolled into WCF. This includes support for IIS hosting in addition to new Visual Studio templates available through the Visual Studio Extensions Manager. But even then, WCF was still missing support for many HTTP scenarios. 
				The need for a comprehensive solution for developing HTTP services in the .NET Framework justified creating a new framework. Therefore, in October 2010, Microsoft announced the WCF Web API, which introduces a new model and additional capabilities for developing HTTP-based services. These capabilities included: 
					• Better support for content negotiation and media types.
					• APIs to control every aspect of the HTTP messages.
					• Testability.
					• Integration with other relevant frameworks like Entity Framework and Unity.
				The WCF Web API team released six preview versions until in February 2012, the ware united with the ASP.NET team, forming the ASP.NET Web API. 
				In June 2016, Microsoft released the first version of .NET Core and ASP.NET Core. ASP.NET Core Web API is the framework for developing HTTP services in .NET Core. 
		What is a Web API?
			Web API is a framework that enables you to build REST-enabled APIs. REST-enabled APIs help external systems use the business logic implemented in your application to increase the reusability of the application logic. Web API facilitates two-way communication between the client system and the server through tasks such as: 
				• Instructing an application to perform a specific task
				• Reading data values
				• Updating data values
			Web API enables developers to obtain business information by using REST, without creating complicated XML requests such as Simple Object Access Protocol (SOAP). Web APIs use URLs in requests, thereby eliminating the need for complicated requests. 
			Web API uses such URLs in requests and obtains results in the JSON format by default. 
				Example:
					[
					  {
						"Id": 1,
						"Name": "Tomato soup",
						"Category": "Groceries",
						"Price": 1.0
					  },
					  {
						"Id": 2,
						"Name": "Yo-yo",
						"Category": "Toys",
						"Price": 3.75
					  },
					  {
						"Id": 3,
						"Name": "Hammer",
						"Category": "Hardware",
						"Price": 16.99
					  }
					]
			REST and Web API enable all kinds of different applications, including mobile device applications, to interact with services. In particular, REST and Web API provide the following benefits for mobile applications: 
				• They reduce the processing power needed to create complex request messages for data retrieval. 
				• They enhance the performance of the application by reducing the amount of data exchange between client and server.
	Lesson 2: Developing a Web API
		Using Routes and Controllers:
			Web API uses controllers and actions to handle requests. A controller is a class that derives from the ControllerBase base class. By convention, controllers are named with the Controller suffix. Controllers contain methods, known as actions, which process the requests and return the results.
			Defining Routes:
				Web API uses routing rules to map HTTP requests to the Web API controllers and actions by using HTTP verbs and the request URL. You can configure routes by using convention-based routing and you can configure routes by using attributes. 
				Although it is possible to configure routes of Web API by using convention-based routing, the recommendation is to prefer configuring the routes of Web API by using attributes. 
		RESTful Services:
			REST uses URLs and HTTP verbs to uniquely identify the entity that it operates on and the action that it performs. REST helps retrieve business information from the server. However, in addition to data retrieval, business applications perform more tasks such as creating, updating, and deleting information on the database. Web API and REST facilitate handling such additional tasks. They use the HTTP method to identify the operation that the application needs to perform. 
			The following table provides information on some HTTP methods that Web API and REST use:
				HTTP Verb			Description
				GET					Use this method with the following URL to obtain a list of all customers.
									/api/customers
				GET					Use this method with the following URL to obtain a customer by using the ID detail.
									/api/customers/id
				GET					Use this method with the following URL to obtain customers by using the category detail.
									/api/customers?country=country
				POST				Use this method with the following URL to create a customer record.
									/api/customers
				PUT					Use this method with the following URL to update a customer record.
									/api/customers/id
				DELETE				Use this method with the following URL to delete a customer record.
									/api/customers/id
			Web API allows developers to use a strong typed model for developers to manipulate HTTP requests. The following code shows how to use the POST, PUT, and DELETE methods for the create, update, and delete requests to handle the creation, retrieval, updating, and deletion (CRUD) of the customer records.
		Action Methods and HTTP Verbs:
			When a client requests a Web API, first the controller is chosen. The next step is choosing the method that will handle the request. The method selection can be done based on the request that the HTTP method has used and on the request-URI. 
			The HttpGet, HttpPut, HttpPost, and HttpDelete Attributes:
				You can use the HttpGet, HttpPut, HttpPost, or HttpDelete attributes in your controller action to specify that a function is mapped to a specific HTTP verb. The following table describes how the HTTP attributes map to the HTTP verbs:
					Attribute			HTTP Verb
					HttpGet				GET
					HttpPut				PUT
					HttpPost			POST
					HttpDelete			DELETE
			If you want to have multiple methods that are mapped to the same HTTP verb, you can specify a template parameter to the Http[verb] attribute.
			The AcceptVerbs Attribute:
				The use of the AcceptVerbs attribute allows you to specify multiple HTTP verbs to the same actions in the controller.
			The ActionName attribute:
				In addition to the controller placeholder in routes, ASP.NET Core Web API also has a special placeholder for action. 
				You can use the ActionName attribute to specify the action name to be used in the routing.
		Binding Parameters to Request Message:
			After locating the controller and action method, mapping data from the HTTP request to method parameters should be done. In ASP.NET Core Web API, this process is known as parameter-binding. 
			HTTP messages data can be passed in the following ways:
				• The message-URI. In HTTP, the absolute path and query are used to pass simple values that help identify the resource and influence the representation. 
				• The Entity-body. In some HTTP messages, the message body passes data. 
			By default, ASP.NET Core Web API differentiates simple and complex types. Simple types are mapped from the URI and complex types are mapped from the entity-body of the request.
			You can use the template parameter of the HttpGet, HttpPut, HttpPost, and HttpDelete attributes to map the route and the parameters of the action.
			Binding Source Attributes:
				A parameter’s value of an action can be found in several locations. The location can be defined in the binding source attributes. There are several binding source attributes which include: 
					• FromBody. Determines that the parameter is bound by using the request body. 
					• FromQuery. Determines that the parameter is bound by using the request query string. 
					• FromRoute. Determines that the parameter is bound by using route-data from the request. 
					• FromForm. Determines that the parameter is bound by using form-data in the request body. 
					• FromHeader. Determines that the parameter is bound by using the request headers. 
			Using the ApiController Attribute:
				It is possible to annotate a controller with the ApiController attribute. Annotating a controller with the ApiController attribute indicates that the controller serves HTTP API requests. It is used to target conventions on the controller.
				In case you have a controller which is not annotated with the ApiController attribute, the binding source attributes should be defined explicitly. In a controller which is annotated with the ApiController attribute, there are inference rules which determine the data sources of the action parameters.
		Control the HTTP Response:
			Action methods can return both simple and complex types that are serialized to a data return format based on the content negotiation. Although ASP.NET Core Web API can handle the content negotiation and serialization, it is sometimes required to handle other aspects of the HTTP response message (for example, returning a status code other than 200). 
			When an action method returns an object, ASP.NET Core Web API automatically serializes the object and returns it. The status code of the response in this case is 200 (OK) in case no exception was thrown. In case there is an unhandled exception in the action method, a 5xx error is returned.
			Return IActionResult:
				An action method can return an object that implements the IActionResult interface. This gives the flexibility to return a status code that is different from 200. For example, to return a 404 Not Found status code you can return a NotFoundResult object by using the NotFound method. If the status code of the response is 200, you can use an Ok method to return the content.
			Return ActionResult<T>:
				In addition to returning an IActionResult, a Web API controller action can return ActionResult<T>. Returning ActionResult<T> enables you to return a specific type or an object which inherits from ActionResult.
		Data Return Formats:
			When a client makes a request to a Web API controller, the controller action often returns some data. Web API can return this data in different formats, including JSON and XML. Both JSON and XML are text formats that represent information as strings. 
			When a client makes a request, the client can specify the data format for the response. If the data format is not specified, Web API formats data as JSON by default.
			Output Formatters:
				Web API uses an output formatter to format or serialize the information that a Web API REST service returns. Web API usually uses the default formatter to return a data in JSON format from an action. However, you can alternatively use other output formatters. To use an output formatter, you need to configure ASP.NET Core MVC to support it. You can use pre-defined output formatters such as an XML output formatter. You can also use custom output formatters to format the responses.
			Return Data in XML Format:
				XML output formatters are located in the Microsoft.AspNetCore.Mvc.Formatters.Xml package. Therefore, in case you would like your action to return data in the XML format, install the Microsoft.AspNetCore.Mvc.Formatters.Xml package. 
				To use an XML output formatter, you can add the formatter to an MvcOptions object and pass it as a parameter to the AddMvc method inside the ConfigurationServices method of the Startup class. 
				Alternately, you can add the XML serializer formatters to MVC by using the AddXmlSerializerFormatters method.
			Specifying the Return Format:
				You can use the ProducesAttribute attribute to force a particular format of the response from an action. 
	Lesson 3: Calling a Web API
		Calling Web APIs by Using jQuery Code:
			When HTTP was built in the early 1990s, it was made for a very specific kind of client: web browsers running HTML. Before the creation of JavaScript in 1995, HTML was using two of the three HTTP methods in HTTP 1.0: GET and POST. GET requests are usually invoked by entering a URI in the address bar or in kinds of hypertext references such as img and script tags. 
			Another way to start HTTP requests from a browser is by using HTML forms. HTML forms are HTML elements that create a form-like UI in the HTML document that lets the user insert and submit data to the server. HTML forms contain sub-elements, called input elements, and each represents a piece of data both in the UI and in the resulting HTTP message. 
			The most flexible mechanism to start HTTP from a browser environment is by using JavaScript. Using JavaScript provides two main capabilities that are lacking in other browser-based techniques: 
				• Complete control over the HTTP requests (including HTTP method, headers, and body).
				• Asynchronous JavaScript and XML (AJAX). Using AJAX, you can send requests from the client after the browser completes loading the HTML. Based on the result of the calls, you can use JavaScript to update parts of the HTML page. 
			Calling Web API Get Method by Using jQuery:
				It is possible to use the ajax function to call other methods in a Web API controller, such as a Post method. In the following example, you will first see a Web API controller that has a Post method and then you will see how to call the Post method by using the jQuery ajax function. 
				You can use the JSON.stringify method in the data parameter of the ajax function to serialize the JavaScript object into a JSON object, which will be sent to the Web API method. 
			Calling Web APIs by using Server-Side Code:
				ASP.NET Core provides a client-side API for consuming Web APIs. The main class for this API is System.Net.Http.HttpClient. This provides basic functionality for sending requests and receiving responses. 
				HttpClient keeps a consistent API with ASP.NET Core web API by using HttpRequestMessage and HttpResponseMessage for handling HTTP messages. The HttpClient API is a task-based asynchronous API providing a simple model for consuming HTTP asynchronously. 
				In order to consume ASP.NET Core Web API methods, the HttpClient class provides several methods, which include: 
					• GetAsync. Sends a GET request to a specified URI. 
					• PostAsync. Sends a POST request to a specified URI. 
					• PutAsync. Sends a PUT request to a specified URI. 
					• DeleteAsync. Sends a DELETE request to a specified URI. 
				To create HttpClient instances, you can use the IHttpClientFactory service. To register the IHttpClientFactory service you can call the AddHttpClient method inside of the ConfigureServices method. 
				After the IHttpClientFactory service is registered, dependency injection can be used to inject it. Once injected, you can use the CreateClient method to get an instance of type HttpClient. 
			Calling Web API Get Method by Using HttpClient:
				-
		Working with Complex Objects:
			When working with complex objects, a more useful approach is to obtain a deserialized object based on the entity body. In the following example, you will first see a Web API controller that has a Get method that returns an instance of type Person. Then you will see how to call the Get method by using the GetAsync method of the HttpClient class and use the ReadAsAsync method to deserialize the response to an instance of type Person. 
			Passing Complex Objects to a Web API:
				It is also possible for a client to pass a complex object to a Web API. In order to do this, the object that needs to be sent must be serialized. The HttpClient class provides several methods to serialize the object, which include: 
					• PostAsJsonAsync. Sends a POST request to a specified URI with the given value serialized as JSON. 
					• PostAsXmlAsync. Sends a POST request to a specified URI with the given value serialized as XML. 
					• PutAsJsonAsync. Sends a PUT request to a specified URI with the given value serialized as JSON. 
					• PutAsXmlAsync. Sends a PUT request to a specified URI with the given value serialized as XML.

Module 14: Hosting and Deployment
	Lesson 1: On-Premises Hosting and Deployment
		Web Servers:
			ASP.NET core applications run on a process-based server implementation, which listens to requests from clients, and uses them to create an HTTPContext object. This object is then injected into the middleware, and you can use it to control the flow of your application.
			ASP.NET Core provides three server implementations:
				• Kestrel (Cross Platform), which is the default server implementation.
				• HTTP.sys (Windows) server implementation, which is based on the Kernel Driver and HTTP Server API
				• Custom server implementation, which is based on Open Web Interface (OWIN).
			A reverse proxy server retrieves resources on behalf of a client from one or more servers. These resources are then returned to the client as if they originated from the web server itself.
			The benefits of reverse proxy include operating multiple different application servers on a single server, providing a firewall, limiting the exposure of application components, supporting load balancing architecture, and helping reduce distributed denial-of-service (DDOS) attacks.
			Kestrel:
				Kestrel is the default server implementation for ASP.NET Core, and it can be used with or without a reverse server proxy. Since Kestrel is designed to work cross-platform, it runs on its own process, allowing the use of technologies such as IIS, NGINX, or Apache. Those technologies cannot be used without Kestrel in ASP.NET Core.
				Kestrel supports features such as HTTPS, WebSockets, and performance improvements while running on NGINX on Unix platforms. It is cross-platform and can be used across many different operating systems. It is a lightweight web server designed for efficiency and quick performance but does not support many features such as windows authentication, port sharing, direct file transmissions, and response caching.
				To set up Kestrel, all you need to do is to call WebHost.CreateDefaultBuilder(*arguments*) in the CreateWebHostBuilder method of the Program.cs file. By default, all new ASP.NET Core applications are created with this configured, and the CreateDefaultBuilder method will call the UseKestrel() method behind the scenes.
				You can also further customize the Kestrel options by manually calling the UseKestrel(*Action<KestrelOptions>*) method as part of the CreateDefaultBuilder pipeline. You can configure many settings here. By using the Limits property of the Kestrel Options object, you can set a variety of connection limits such as:
					• MaxConcurrentConnections. Used to set how many connections the server can handle at a time.
					• MaxConcurrentUpgradedConnections. Used to set the number of upgraded connections, such as those used in WebSockets.
					• MaxRequestBodySize. Limits the size of the body in requests, set in number of bytes.
					• KeepAliveTimeout. Sets the timeout for connections.
				https://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/kestrel
			HTTP.sys:
				HTTP.sys is an alternative to Kestrel if ASP.NET Core is running on Windows. HTTP.sys supports Windows 7 or later and Windows Server 2008 R2 or later. It is a complete and robust web server implementation, which, unlike Kestrel, does not require external hosting solutions. However, it is not a lightweight solution.
				HTTP.sys supports Windows Authentication, running multiple applications on the same port, HTTPS, Direct transfer of files, caching responses, and WebSockets. However, you can run HTTP.sys only in a Windows environment, and it does not support reverse proxy. It is also a heavy implementation, intended to handle all facets of a web server, and HTTP.sys does not work in conjunction with IIS or IIS Express.
				In order to setup HTTP.sys, you will need to perform the following steps:
					1. After calling the CreateDefaultBuilder(*arguments*) method in the application builder, you will need to pipe in a call to the UseHttpSys() as part of the builder. This will set up HTTP.sys with a basic configuration.
					2. Add a new debug environment profile that does not use IIS or IIS Express, such as project in the project properties debug menu or in the launchsettings.json file. This was covered in Module 10, "Testing and Troubleshooting".
					3. Select the new environment in the debug menu and start the application.
					4. Go to "http://localhost:5000". By default, the application will be hosted on port 5000.
				You can configure additional parameters by using an Action that returns an HttpSysOptions object. This object allows you to configure several options that can affect the behavior of the server:
					• Authentication. Exposes various sub-properties that can affect the application authentication, such as the Schemas property to set the application authentication schema.
					• MaxConnections. Sets the maximum number of concurrent connections.
					• MaxRequestBodySize. Sets the limit for the size of the body on requests.
					• UrlPrefixes. Allows you to override the default URL.
				https://docs.microsoft.com/en-us/aspnet/core/fundamentals/servers/httpsys
			Open Web Interface for .Net (OWIN):
				OWIN is an open-source interface designed to operate between ASP.NET web servers and web applications. It was made with the intent to decouple the client and server architecture and allow for the development of simple modules.
				OWIN is designed around ASP.NET rather than ASP.NET Core. Servers that run OWIN architecture will use ASP.NET. However, it is possible to create client applications, which interface with OWIN, in ASP.NET Core.
			ASP.NET Core Module:
				ASP.NET Core Module is an IIS module that enables you to set up a reverse proxy configuration for IIS applications. Being an IIS based module, it is incompatible with HTTP.sys, and you should use Kestrel instead. ASP.NET Core module runs by plugging into the IIS pipeline and redirecting requests to the appropriate backend application.
				The ASP.NET Core module is responsible for managing the IIS process and runs in a separate process. When the initial connection occurs, the ASP.NET Core Module will start the kestrel application, and it will also restart it the application has crashed.
				Forwarding occurs in the ASP.NET Core Module when the client communicates with the IIS server that hosts the ASP.NET Core Module. The ASP.NET Core Module will then create an internal HTTP connection (It will use HTTP even when the connection with the client is HTTPS), to the ASP.NET Core application that runs on a Kestrel server. It will access the ASP.NET Core application by using an environment variable that is configured in the IIS startup. Kestrel will then handle the request in the normal flow before returning the response to the IIS server, which will forward it to the original client.
				The ASP.NET Core module is an IIS-based solution, which requires an IIS Server. If you work on a multiplatform or non-Windows environment, you can use Apache or NGINX instead. You can use either solution as reverse proxies when using a Kestrel server-based application.
			Running the server:
				In Visual Studio, you can do this by using the Debug menu. From this menu, select an environment profile and then start the application. This will run the application in either IIS Express, ASP.NET Core Module, or through the windows console. 
				In Visual Studio Code, you can use the Omnisharp extension to run the application by using the CoreCLR debugger, which is an open source Microsoft .NET execution engine which is a part of .NET Core. This allows it to work cross-platform.
				At times, you might not be able to run the application from an IDE. In such situations, you can use the various .NET CLI commands to run the application. You can run .NET CLI commands from various command line consoles such as Windows Command Prompt, Windows PowerShell, and Linux Terminal. You can usually add these commands to the system PATH by installing Visual Studio, but you can also add the commands by installing the .NET Core SDK, which is required for running .NET Applications. When not working on a development server, it is usually better to use the .NET Core SDK.
				To run a server by using .NET CLI commands, you will need to browse to the project folder where the .csproj file is located. To run the commands, enter the cd command inside the command line console followed by the path to the folder that contains the .csproj file. The cd command exists in most common command line consoles.
				To host a server, use the dotnet run command. This will run the build process, and then run the project in the current folder. This will allow you to run your project even without access to Visual Studio. This solution works across different operating systems. It can also be used alongside projects in Visual Studio Code, which allows you to bypass the need to install related extensions. Note that calling dotnet run will use any cached NuGet packages if they are available and will only download any which are missing.
				While working on a production environment, it is best to use dotnet run because the faster setup is conducive to rapid testing. However, for production environments, it is best to use dotnet publish. This can allow you to publish once, create the necessary DLL files and ensure the latest relevant NuGet packages are used. Also, it is the recommended solution for hosting on production servers.
				As an alternative option to navigating to the project file, you can choose to provide the dotnet run command a full file path to run from. This will remove the need to browse to the project folder.
		Hosting ASP.NET Core Application:
			The basic steps of hosting an application are as follows:
				1. Publish the application. In this step, you will create a basic application. It will contain the DLLs and the resource files required to run your application, all inside a single folder, ready for deployment.
				2. Set up a process manager. In this step, you will need to choose a dedicated process runner for your application, such as IIS or NGINX. This will keep the application up and handle application crashes and automatic startup.
				3. Set up a reverse proxy configuration and load balancing. This is an optional step you can perform if you want the benefits of using a reverse proxy, which include protection from various security attacks (such as DDOS) and load balancing to support multiple servers.
			Publishing the Application:
				The first step in hosting an ASP.NET Core MVC Application is to publish the application. This will create a compiled copy of the application, alongside all required resource files such as JavaScript and HTML files, and store them in a single folder for ease of use. Publishing ensures that vital system files are not easily readable, while also being lighter than development files.
				An important decision to make while publishing an application is to choose between a framework-dependent deployment (FDD), or a self-contained deployment (SCD).
				If you use FDD, the deployment will not contain any .NET Core specific files because the files installed on the host will be used instead. You also do not have to specify a target operating system. This creates a smaller package, while also using the same .NET Core application installation as other applications on the server. However, using FDD requires the host to have the .NET Core version used by your application or a newer version, and if the .NET Core libraries are changed significantly enough in a future version, the application may start working in an unexpected manner.
				If you use SCD, the deployment will contain a specific version of .NET Core, which is selected by you and will be packaged alongside your application. This ensures that you will not need to install it on the hosting server and your application will keep working even if the host updates its versions of .NET Core. However, because .NET Core is included as part of your package, you will need to select target platforms in advance, each additional platform increasing the installation size. This can particularly be troublesome on servers that run multiple .NET Core-based installations because a lot of space and memory in those servers might be taken up by multiple .NET Core installations that are running simultaneously.
				Note that any third-party files used by your application will also be present in both FDD and SCD deployments.
				By default, a new project in a Visual Studio environment will be set with Debug configurations. However, builds that are intended for production environments will usually use Release configurations.
				In order to set up the project for a publishing build in Visual Studio, in Solution Explorer, right-click the project, and then select the properties from the context menu. To update settings related to builds and publishing, click the Build tab. To publish for a production build, select Release from the Configuration drop-down list. This will ensure your project is set up for the release environment. Then go to the Output section of the Build properties. Inside the Output path box, set an output path of your choice or use the default one. This will determine where the compiled DLL files will be stored for the project. Note that it will add the version of the .NET Framework as a part of your path in the following format: "*your path*\*.NET Core version*\". Remember the final path because you will be using it later.
				After this is done, browse to the project in your command line console of choice and call the dotnet publish command, just like you would call the dotnet run command. Rather than immediately running the application, it will get the latest version of all used NuGet packages and create a publish directory by using the directory you set in the Output path inside the publish folder. The resulting directory contains the compiled version of your application, and you can copy it to additional locations such as production servers.
				To test your published build, you can call the dotnet command. The dotnet command accepts a path to a DLL file that contains a .NET Core application. It will then host the application in the command line console the command is called from. Remember that calling the dotnet CLI will require you to install the .NET Core SDK.
				https://docs.microsoft.com/en-us/dotnet/core/deploying/deploy-with-cli
			Setting up a Process Manager:
				While you are technically able to keep the application running by using the dotnet command, the resulting server will be very basic and limited, and the application lifespan will need to be managed manually. This means that if the application crashes or the machine hosting it encounters a temporary issue, you will need to manually manage the process. This is obviously not a desirable circumstance.
				To resolve these issues, you can use one of many dedicated process managers. These are programs such as IIS, NGINX, Apache, or Windows Services, all of which are designed to run applications and ensure they are kept running. Many of these are specifically designed to host web applications, but some such as Windows Services, are more generic options. In this module, you will learn about the basics of the most popular options.
			IIS:
				Internet Information Services (IIS) is a server architecture developed by Microsoft for handling requests. It is a popular option for maintaining web server processes, and natively supports many Windows-related technologies such as Windows Authentication. It is only supported in Windows Servers, and as such, you should validate your business requirements before using it to host your application.
				IIS is highly flexible and manageable, allowing for features to be enabled and disabled as required. This can prevent issues caused because of applications running multiple unrequired features, leading to wasted resources.
			NGINX:
				NGINX is a process manager that is primarily used on Linux servers. It is a popular web server that can act as a standard server, as well as a reverse proxy server. In addition, it can act as a general usage TCP/UDP server. While you can run it on Windows servers, it is not optimized for it, and will generally underperform.
				NGINX works by utilizing a single master process and multiple worker processes. The master process mostly handles the NGINX configurations and ensures that the configurations are used, as well as ensures that worker processes are maintained. In turn, the worker processes handle the actual processing of the various communication requests. The number of workers is dependent on the configuration file.
				https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/linux-nginx
			Apache:
				Apache is an open-source HTTP Server designed to operate effectively across multiple operating systems. It is designed to be flexible, highly compliant with technologies, and offers great extensibility through third-party modules because of it being an open-source tool. It is also constantly evolving, with new technologies being added on a frequent basis. Its main strength is the flexibility of use across different platforms.
				The Apache HTTP server is configured by using various text files. These configurations further specify its behavior, enabling or disabling features as needed, including the behavior of third-party libraries that are in use.
				https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/linux-apache
					Apache can be a good server solution when multiple servers on different operating systems are required, such as when multiple clients require their own servers. In addition, Apache is the easiest solution when trying to support deployments across multiple different servers.
			Windows Services:
				A less common solution for a process manager is to host your application on a Windows Service. Unlike the previous options, Windows Service is not designed as a dedicated HTTP Server and therefore lacks many of the features common in other options. However, all installations of the Windows operating system include Windows Services as a core feature and as such, it can be used on any Windows installation.
				When installing an ASP.NET Core application on a Windows Service, you can manage it through the Windows Services, enabling a relatively simple setup.
				https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/windows-service
				Windows Service hosting can be a reasonable choice if there is a requirement for minimum external requirements because it can work without additional applications. However, it is far less configurable than other options and requires Windows. Use this option when targeting Windows and IIS is not a viable option.
			Setting up a Reverse Proxy:
				An optional step in deployment is setting up your web server to use a reverse proxy. By using a reverse proxy, you can ensure a more secure and better load-balanced application at the cost of a more difficult setup process, higher latency due to an additional step in the navigation, and possible security issues with forwarding cookies between applications on the reverse proxy.
				It is important to remember that using a reverse proxy will require you to use Kestrel and host using IIS, NGINX, or Apache. If you cannot fulfill these conditions, then you will be unable to gain the benefits of a reverse proxy.
			Load Balancing:
				Often, modern applications are designed to be scalable, leading to a requirement for load balancing. A load balancer is a system designed to direct incoming HTTP traffic between multiple servers. It will prefer sending new requests to servers with a lesser load at the time, allowing to optimize the user experience by providing a connection to available servers, while avoiding creating connections with busy ones.
				If you use a reverse proxy, you might use one server to handle both load balancing and act as the reverse proxy. However, it is possible to keep these two systems separate, in which case, the load balancer will have to be between the application and the proxy server.
				https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/proxy-load-balancer
		Deploying to IIS:
			Requirements in an ASP.NET Core Application:
				The first step in the set-up process occurs from within the ASP.NET Core application, and it is very likely that your application is already set up to handle the deployment. As part of the call to WebHost.CreateDefaultBuilder() the UseIISIntegration() method is invoked, enabling Kestrel to integrate with IIS. If you prefer to build the IWebHostBuilder manually, you will need to call the UseIISIntegration() method to support IIS.
			Another important step is to ensure that the web.config file is present in the published application. The web.config file is an important configuration file that is used by the IIS. These configurations determine the behaviors in working with the web application. Among other things, it tells the IIS to not serve some of the files present in the application root, causing potential security issues. It is important to ensure that the file is present in the published application root and to use the default name of web.config. If the file is renamed, the system will ignore it.
			Setting up IIS to Host ASP.NET Core Applications:
				You will need to ensure your designated host is able to run IIS to perform the deployment. In order to host, you will need to ensure IIS is installed on the designated computer.
			Creating the Web Site in IIS:
				1. Create a destination folder to host your ASP.NET Core MVC application. This folder will be used by IIS and will hold the physical files for your application.
				2. Ensure that a logs folder is present. If it is not present, create one. This will ensure logs from the stdout will be added to the Logs folder. Alternatively, different configurations can be set from the web.config file.
				3. Open the IIS Manager, and in the connection segment on the left, open the server node by clicking the arrow next to it.
				4. Under the server node, right-click the Sites node, and then select Add Website….
				5. In the Add Website window, set the Site name to the name you wish to choose and set the Physical path to the folder you created in the first step. You can also set the bindings to use either HTTP or HTTPS as needed, set an IP address from the addresses available to the server and the port. By default, HTTP uses port 80, and HTTPS uses port 443. If you know what host name the application will be served under, you can provide it now, if not you can change the host name later. After filling out your requirements, click OK.
				6. At this point, if a default web site or another web site is already set up on your chosen port, you will get a notification message that only one site is supported for a single port. You can choose to either accept (you will only be able to run one site at once) or click No and change the port.
				7. If you want the application to run in a process managed by ASP.NET Core instead of through IIS, you must also complete the following optional steps. This can help preserve the usage of specific versions of ASP.NET Core Runtime:
					a. In the connection type, under the server node, select Application Pools.
					b. Right-click the application you added and click the Basic Settings… option from the menu.
					c. From the .NET CLR version drop-down list, select No Managed Code, and then click OK.
			Deploying the Application:
				A relatively simple way of doing this, which can be particularly useful in one-time deployments, is to copy the results of dotnet publish into the folder for the application. After this is done, you should stop and start the application in IIS, and then browse to it to verify the changes.
				Alternatively, if you need to facilitate regular deployments, you might wish instead to use publish profiles in Visual Studio to automate the deployment process. These enable you to store various publishing properties, such as destination and automate several steps.
				https://docs.microsoft.com/en-us/aspnet/core/host-and-deploy/visual-studio-publish-profiles
		File Providers:
			In an ASP.NET Core MVC application, you can easily access any file or directory within the project structure by using file providers. This allows you to focus on the project structure itself, and not deal with the headaches of project structures, which may change as part of the build and publish process. In fact, both the UseStaticFiles() middleware and the Razor engine use the built-in File providers to locate resources, pages, and views.
			In general, the primary interface you will use while working with files is IFileProvider. It provides methods for reading files and directories.
			There are three separate implementations for IFileProvider available in ASP.NET Core applications:
				• PhysicalFileProvider. Used to access files within the file system scoped to a specific root directory. This will be used for reading various non-compiled files such as XML, Text, and even HTML files.
				• ManifestEmbeddedFileProvider. Used for accessing files embedded within the project assemblies. This can allow access to files that are normally embedded to the compiled assemblies of your ASP.NET Core application.
				• CompositeFileProvider. Used for combining multiple other providers. This can allow you to use a single provider that can handle both compiled and non-compiled files.
			PhysicalFileProvider:
				You can use the most commonly used file provider, PhysicalFileProvider, to read physical operating files such as XML configurations, log text files, JSON based resources, and media files.
				PhysicalFileProvider uses the System.IO.File library for file management. However, it limits the file scope to a base directory and any children files and directories inside it. This makes it a safe option because it cannot access files from outside the project. Therefore, it cannot be used maliciously to cause harm to other applications. PhysicalFileProvider uses a directory path in its constructor to set the base directory, limiting access to subdirectories.
				A convenient alternate option to instantiating a PhysicalFileProvider manually is to use the IHostingEnvironment service. The IHostingEnvironment service exposes the ContentRootFileProvider property. This is a pre-instantiated PhysicalFileProvider originating in the application root. By using this, you can safely use a PhysicalFileProvider, which is already configured to use the root of the application.
				After obtaining a PhysicalFileProvider, you can then use the GetFileInfo(*filePath*) method to get an IFileInfo object, which is used for interacting with files. The file path parameter supplied to GetFileInfo should direct to the file you wish to read.
				In order to read the actual file, you will need to use the CreateReadStream() function on the IFileInfo object. This will create a memory stream designed to hold the content of the file as it is read. It is advised to encapsulate this step in a using statement, as the stream implements IDisposable and creates a connection to the file.
				After the stream is established, you will need to use it to instantiate a new StreamReader(*stream*) object, which is used for performing the actual reading. It requires the previously created stream as a parameter.
				Finally, you can use the various functionalities of the StreamReader to read the file. For example, ReadToEnd() will read all segments of the stream, which have not yet been read.
			ManifestEmbeddedFileProvider:
				Another useful option for a file provider, particularly when security is a big concern, is using ManifestEmbeddedFileProvider. Compared to PhysicalFileProvider, this file provider offers an option to use files specifically embedded into the application DLL. This can act as an additional layer of security, allowing you to add sensitive files inside the application and use them internally or expose specific segments as needed.
				The first part of this process is to set up the project configuration to support manifest embedded files. By right-clicking the project and selecting Edit *Project file*, you can edit the project configuration. Under the PropertyGroup element, you will need to add a new GenerateEmbeddedFilesManifest element. Inside the element, set the value to true to support embedding files into the manifest.
				You will then need to find the ItemGroup element. Inside it, you can add EmbeddedResource tags to specify files that will be embedded inside the application. Each EmbeddedResource must contain the Include attribute, and the value provided for the attribute needs to be a valid file path. You can either use the wildcard * in your paths, or you can provide specific file paths, so you could use the string '*.txt' to embed all text files in the root directory, or you can provide a specific file.
				Once this is done, you can create a new ManifestEmbeddedFileProvider(*assembly*) to receive a file provider. To get the root of the current assembly, you can use System.Reflection.Assembly.GetEntryAssembly() as the assembly parameter, which will be used by the ManifestEmbeddedFileProvider to act as a file provider for embedded files. This will allow you runtime access to files that are not actually present as physical files.
			CompositeFileProvider:
				Occasionally, you may end up in situations where you need both external resources (such as configuration files) and embedded files (files with sensitive data). In such a case, rather than separately maintaining multiple separate file providers inside your application, you can instead use the CompositeFileProvider to unify several different providers.
				In order to set up CompositeFileProvider, you will need to first set up at least one of the providers it will use. As such, it is recommended to set up CompositeFileProvider as a service in the ConfigureServices method. To do this, create a new CompositeFileProvider and provide it with an array of file providers. Note that because IHostingEnvironment is not injected into the ConfigureServices method, you will need to add a constructor to the Startup class and add it as a property. Once the composite provider has been instantiated, use services.AddSingleton<IFileProvider>(compositeProvider) to add it as a service.
	Lesson 2: Deployment to Microsoft Azure
		Microsoft Azure is a cloud service provided by Microsoft for organizations to build and manage solutions easily. This service has multiple hardware and software abstraction levels, which include:
			• Infrastructure as a service (IaaS). Virtualization of physical infrastructure components, which means virtual machines being provided as a service.
			• Platform as a service (PaaS). Platform-level resources that provide a running environment to run a deployed application, such as application servers, which are provided as a service residing on the cloud.
			• Software as a service (SaaS). Using existing software functionality through services residing on the cloud.
		Azure also provides tools for developers to be able to remotely debug applications deployed on the cloud. Azure Application Insights provides developers with the ability to perform in-depth monitoring of their application. Starting from request rates and response times to performance counters, such as CPU, memory, and network usage, developers can monitor various metrics. Application Insights provides live streaming of exception traces, a snapshot of data assembled from live operations, and log traces that allow developers to diagnose and resolve issues quickly.
		What is Microsoft Azure?
			Microsoft Azure is a cloud service provided by Microsoft. A “cloud” makes computing resources available on-demand and over the internet. The computing resources include servers, storage, networking, and databases. Azure also makes lots of software and platforms available as resources. These resources are available on-demand and billing is as per use.
			The most basic of these services is Infrastructure as a Service (IaaS). This allows you to use any kind of IT infrastructure, such as servers, persistent disks, networks, and operating systems without the hassle of having to buy any hardware or configure any machines. You can provision these machines over the internet at the click of a button. Once you are done using them, you can “delete” them, again at the click of a button. You will be charged for these machines only for the duration that you used them. Some of the services available under IaaS in Azure are:
				• Virtual Machines. Provision Windows and Linux virtual machines in seconds.
				• Container Instances. Easily run containers with a single command.
				• Traffic Manager. Route incoming traffic for high performance and availability.
				• Azure DNS. Host your DNS domain in the Cloud.
			On the other hand, Platform-as-a-service (PaaS) is an advanced service that provides a complete environment to run your applications. In the case of IaaS; deployment, maintenance, monitoring, and scaling of resources to run your application is your responsibility. In the case of PaaS, the only thing you need to do is deploy your application. The platform provides servers with pre-installed software such as any middleware and development tools needed to run your application. Features such as scalability, high-availability, and multi-tenant capability are provided by the platform, reducing the amount of coding that developers must do. The need for additional resources to monitor, update, and manage the applications is also reduced because the platform provides these as built-in services. PaaS services provided by Azure include:
				• App Service. Quickly build, deploy, and scale enterprise-grade web, mobile, and API apps running on any platform.
				• Service Fabric. Develop microservices and orchestrate containers.
				• Azure SQL Database. Managed relational SQL Database as a service.
				• Azure Functions. Process events with serverless code.
				• Azure Storage. A massively scalable object store for data objects, a file system service for the cloud, a messaging store for reliable messaging, and a NoSQL store.
		Benefits of Hosting in Microsoft Azure:
			Flexibility and Elasticity:
				Provisioning and using resources is extremely simple with Azure. In Azure, the resources needed, such as virtual machines, can be provisioned in seconds and it offers a range of options to provide a better fit for your needs. Azure offers users the ability to scale from one to thousands of virtual machines. With data centers across the world, you can scale to where your customers are.
			Security and Compliance:
				Security is integrated into every aspect of Azure. Data encryption at rest is available for services across the SaaS, PaaS, and IaaS cloud models. Azure supports various encryption models, including server-side encryption that uses service-managed keys, customer-managed keys in Key Vault, or customer-managed keys on customer-controlled hardware. With client-side encryption, you can manage and store keys on-premises or in another secure location.
				Services such as Azure DDoS protection and Azure Advanced Threat Protection help you keep your applications and systems secure from any malicious or suspicious user or device activity.
				Azure has compliances for most of the regulations across the world. The following are a few of them:
					• Federal Financial Institutions Examination Council (FFIEC)
					• Health Insurance Portability and Accountability Act (HIPAA)/Health Information Trust Alliance (HITRUST)
					• Payment Card Industry Data Security Standard (PCI DSS)
					• Federal Risk and Authorization Management Program (FedRAMP)
			Developer Tools and DevOps:
				Azure has a wide range of tools that integrate completely with Azure to make building, managing, and continuously delivering cloud application very simple. Azure supports all the commonly used third-party DevOps tools such as Ansible, Chef, Terraform, Puppet for automated infrastructure management.
				Azure DevOps Services allows developers to create a complete continuous integration/continuous delivery (CI/CD) pipeline by allowing them to automate building, testing, and deploying applications. Azure provides complete integration with all commonly used source repositories such as Git and Bitbucket. Azure application Insights provides developers with the ability to get real-time metrics from production systems, helping them resolve issues quickly. To summarize, all the tools needed for all life cycle stages of an application are fully integrated with Azure. This makes application management easy and fast.
			Cost Effectiveness:
				The most obvious cost-benefit of Azure is that there no upfront payment required for any of the infrastructure or services being used. The pay-as-you-use model ensures that you pay for the resources only for the duration that you use them. However, there are additional pricing models, where you can opt for committed usage and get up to 70% discounts. Azure also offers special Dev/Test pricing for running your development and QA workloads.
		Deploying Web Application on Microsoft Azure:
			To deploy a web application on Azure, there are various options available:
				• Azure Virtual Machines. This is the most basic service available. With this, the entire responsibility of installing and configuring all required software, monitoring application, scaling application as needed must be done manually. This option is best for legacy applications.
				• Azure App Service. Azure App Service is the best choice for most web apps. Deployment and management are integrated into the platform, sites can scale quickly to handle high traffic loads, and the built-in load balancing and traffic manager provide high availability. 
				• Service Fabric. Service Fabric is a good choice if you’re creating a new app or re-writing an existing app to use a microservice architecture. Apps, which run on a shared pool of machines, can start small and grow to massive scale with hundreds or thousands of machines as needed. 
			The simplest and easiest way to deploy ASP.NET Core applications to Azure is by using Visual Studio Publishing Wizard.
			To deploy your application, in Solution Explorer, right-click the project, and then select Publish. In case you are deploying your application for the first time, you will be presented with the Pick a publish target dialog. Use the default value, App Service. Make sure Create New is selected and then click Publish. A new dialog will appear, which displays the details to create a new app service on Azure. This is a one-time activity and in subsequent deploys, you just select the service you have already created.
			In the Create App Service window, if you have not already done so, sign in to your Azure account by using the Add an account button in the top-right corner. You will be prompted for an App Name. App names are globally unique and an immediate check to verify the uniqueness of your app name is performed. If the name is already used, a notification will appear and you will have to choose a new name.
			Choose your subscription, this subscription determines the access to resources in Azure. You will also be required to choose a Resource Group. A resource group is used to group related resources so that they can be managed as a group. This is useful to automate monitoring, deployment, etc. If you already have a resource group, choose that. You can also create a new resource group by, choosing the New option. Next, you need to choose the App Service Plan. An App Service plan defines a set of compute resources for a web app to run. The plan specifies the region where you want to host your application. Ideally, this should be where most of your customers are located. The plan also helps you choose the size of the virtual machine instance, number of virtual machines, and pricing tier. The pricing tier governs how scaling is handled. If you do not have a plan, you can create a new one, and then select that.
			Once all the necessary options are chosen, a summary of what’s going to happen appears at the bottom of the window. Make sure everything is as you planned.
			Click Create. This will create all the necessary resources in Azure. Right after this step, Visual Studio builds your project and deploys it. Once deployment is completed, a new tab opens in your browser, and will automatically point you to your deployed application. The URL to access your application is in the form: "http://<APP-NAME>.azurewebsites.com" where APP-NAME is the name you entered while publishing your application.
		Azure Deployment Strategy:
			The deployment process is the process by which the code for your application is built and deployed to the servers that will host it and eventually be available for all the users.
			The deployment strategy is the strategy that you choose for the deployment to happen. There are various ways to deploy to Azure and it all depends on your specific scenario:
				Visual Studio Publishing Wizard:
					The easiest way to deploy to Azure is by using Visual Studio Publishing Wizard, as demonstrated in the previous lesson. Even if you’ve only started with Azure, with few simple clicks your Web App is up and running.
				FTP:
					In order to use the FTP option, you need an existing Web App inside Azure. To use it you need to know your hostname and credentials, which can be found inside Azure, where your Web App resides. After you have the credentials, you can use your favorite FTP client to deploy the site directly to Azure.
				Azure CLI:
					The Azure CLI tool lets you make deployments straight from your console. In concept, this is similar to what the publishing wizard does, except this happens through a command.
					While all the above options are fast and easy, they are obviously not suitable to deploy production-level applications. Some of the drawbacks of this kind of deployment are:
						• Such deployments generally require a downtime wherein the old application version is stopped and the new version is installed and started.
						• Such deployments are high risk since if there are any issues, the impact is felt directly on the live system.
						• In case there are any problems with the deployment, there is no easy way to roll back. The rollback is also a manual process mostly and adds to the downtime.
			To handle these issues, Azure provides different deployment strategies.
				Staging Environment, Deployment Slots, and Swapping:
				In general, a good practice is to deploy your changes or application to an environment that is as close to the production environment as possible. Such environments are called staging environments. Once an application is deployed to a staging environment, tests can be run on this system and after all tests pass, the application can then be deployed to production.
				Azure provides a feature called deployment slots which makes this possible very easily and efficiently. Once you deploy your application to App Service, you can create a deployment slot. This deployment slot is a replica of your original deployment environment. The original deployment environment is also called the production deployment slot.
				At the time of creating a deployment slot, you need to specify a name for the slot. This gets appended to the App Name and is used to create the URL to access your particular deployment.
				For example, if your app name is "MyAzureApp", the URL to access the original deployment will be "http://MyAzureApp.azurewebsites.net". Now if you create a deployment slot with the name "beta", the URL to access the deployment in this slot will be "http://MyAzureApp-beta.azurewebsites.net". So, in short, you now have two different deployment environments that are identical to each other but have their own hosts and configurations.
				Once you have additional slots, you can use these to run all kinds of testing on the staging environment. After all tests have successfully passed, you can swap your environments. Once you swap, your application that was deployed to the staging deployment slot will now be available in the production deployment slot and vice-versa. Internally this is achieved by swapping the virtual IP addresses of the source and destination slots, thereby swapping the URLs of the slots.
				All of this is achieved without any downtime. It is also possible to attach certain settings to slots. Database is a good example; your production and staging environments will connect to different databases. So, database connection settings can be attached to their own slots. So, while your application gets swapped, they continue to connect to the correct database.
				If after swapping development slots, you think that there are any issues with production deployment, you can always roll back your deployment by doing another swap. This reverses the original swap. All of this makes transitioning from staging to production environments very smooth and easy, enabling your deployments to remain robust.
			Azure Resource Manager Templates:
				This template is basically a JSON document in which you declare imperatively all the resources you need, and the dependencies between them. Using the template, Azure Resource Manager can deploy, update, or delete all the resources for your solution in a single, coordinated operation. You use a template for deployment and that template can work for different environments such as testing, staging, and production. Resource Manager provides security, auditing, and tagging features to help you manage your resources after deployment.
				You can also use deployment slots in conjunction with Azure Resource Manager by specifying the type of deployment as slots and giving the name of the slot to which the deployment is done.
		Debugging a Microsoft Azure Application:
			For any application running in production mode, the ability to constantly monitor the application, get insights into the performance of the application, and be able to quickly diagnose any problems is a very important requirement. Azure Application Insights provides tools for all these requirements.
			Application Insights Configuration:
				To start using this service, you will need to do the following:
					1. Create an Application Insights Resource in the Azure Portal. In the portal, click All Resources, select Management tools, and in the list that appears on the right, select Application Insights.
					2. In Visual Studio, go to Solution Explorer, select the project folder, right-click and choose Add. In the options, choose Application Insights Telemetry.
						a. In the Application Insights Telemetry configuration window for, register your application for Application Insights. Provide your Azure account details and choose the Application Insights resource created in Step 1.
						b. At the bottom of this window, you will see the Add SDK option. This will add the Application Insights SDK to your solution. This will help you see live streaming data on the portal.
				Analysis from the Azure Portal:
					In the Azure Portal, from All Resources, select the Application Insights resource created earlier. The Overview page shows you a few default graphs such as:
						• Failed Requests. This shows the number of requests made to the application that returned an error response.
						• Server Response Time. The time it took for the server to respond to a request.
						• Server Request Count. The number of requests made to the server
						• Page Load Time. The time it takes for a page to load.
					In the Investigate section on the left-hand side, you will see several features that will help in application monitoring.
					Application Map helps you spot performance bottlenecks or failure hotspots across all components of your distributed application. This view displays all the components that are part of the application. In the case of complex applications, this is very helpful in visualizing the system and quickly spotting the problem areas. Clicking on a component gives more details about that component and you can Investigate Further for more analysis of that component.
					Live Metrics Stream shows you key metrics of your application in a streaming fashion. This is helpful for monitoring your application during specified periods to understand live application usage patterns and behavior.
					Metrics Explorer allows you to create your own graphs. Application Insights telemetry collects various metrics from your application. Metrics Explorer allows you to create charts for any metric you wish to track. You can create charts with specific granularity, specific metrics, and aggregations. You can also set alerts from the Metrics Explorer. To create an alert, you need to do the following:
						1. Choose the metric
						2. Choose the condition
						3. Choose the threshold
						4. Choose the period for which the condition violation should occur before raising an alert.
						5. Choose the action to be taken. The action can be an email or an Http/Https request to a webhook.
					The metrics explorer has a wide range of performance counters available for you to monitor. These are categorized into:
						• Usage. This gives you information about the usage of your application in terms of events, page views, sessions, users, etc.
						• Failure. This gives you information about different failures, how many were server exceptions, browser exceptions etc.
						• Performance Counter. Such as available memory, CPU counters, request rate, etc.
					Availability allows you to create availability tests. In these tests, Application Insights sends requests to endpoints configured by you. If the application does not respond or responds slowly, an alert is triggered. There could be multiple tests that can be configured for an application, from a simple index page URL to dependent requests. A Dependent Requests test parses embedded links and images and performs requests for each of these as well. It is also possible to set up a multi-step test, where a series of URLs representing various stages of a workflow can be configured.
					Failures section shows the details of failed requests. This view can be used to get more details about the failures such as response codes, the exception being thrown, etc.
					Performance section gives you details of the time taken for various requests to get completed. This gives you information about the most occurring requests, the average time taken by them, etc.
				Telemetry Analysis from Visual Studio:
					As mentioned earlier, once you have configured your application to provide Application Insights Telemetry, in addition to the Azure Portal, you can also view the telemetry data in Visual Studio.
					For this analysis, go to Application Insights Trends. To reach this window, on the View menu, click Other Windows. If you are coming to this window for the first time, you will be asked to Select the Application Insights Resource. Subsequently, Visual Studio remembers the last resource used and displays data for that resource.
					The Application Insights Trends window lets you analyze telemetry based on:
						• Type: This is the type of data you want to analyze, such as server requests, exceptions, dependencies, page views, etc.
						• TimeRange: You can further split your data to a specific time range.
						• Groups: The data you want to analyze can be grouped on different parameters such as performance, response code, failure types etc. This grouping is dependent on the type of data you are analyzing. For example, if you are analyzing page views, you can group the data by Performance, Page names, Application Version, etc.
						• View: This lets you decide whether you want to view data points as counts or percentages as well as whether anomalies (data that is out of normal range) should be shown.
					Once all the selections are done, click Analyze Telemetry. This will result in a time series histogram being shown in the top half of the screen followed by tables with dimension counts in the bottom half. You can use the dimension counts for further filtering of data.
					If you have chosen to view the data with anomalies, the histogram will show the anomalies in red. The size of the circle shows the size of the anomaly. You select the anomaly to get details such as the deviation. Double-clicking this will take you to Application Insights Search view. This lets you get into details of the instance, the request and other fine-grained information of the anomaly.
					In this view, you can choose custom time filters and a combination of data types. Since the time range is custom, this gives you the ability to view time series over a longer period and then narrow down to a smaller range, by clicking the histogram bars. The bottom part of the window lets you further filter data with a large number of options. You can also see details for each data point (each request, each page view) to further drill down and get to any problem areas. For example, if you are looking at page views, the bottom view will give you a list of page views. Clicking on any of these views gives you further details of that particular view or request. These details include browser version, the location of the request, the instance details that handled the request and many more.
					On top of this view, you will see the Track Operation tab. Clicking this will show you the breakup of time taken in various subrequests. This is a very useful view to drill down into to find the time taken by various components and comes in handy for performance tuning.
				Remote Debugging:
					You can also debug your application running on Azure App Service. To be able to do this, you need to do two things:
						1. Deploy the app to Azure App Service with Debug configuration.
						2. Attach a debugger to the deployed app
				Cloud Explorer:
					The Cloud Explorer view provides an overview of your Azure resources and resource group, allowing you to perform resource related diagnostics actions. In Visual Studio, from the View menu, go to Cloud Explorer. In this window, you should see your Azure subscription. In case you do not, click Account Management and login to Azure from Visual Studio. Once you are logged in, you should see all your Azure services in Cloud Explorer. Select the App Services option and choose your application deployment. In the bottom half of the window, you should see the Actions panel. In this panel, click Attach Debugger. Once this is done, you can create breakpoints in your code and debug just like your local application. You can step through functions, examine data, etc.
				Server Explorer:
					The Server Explorer view allows you to manage your web app running in Azure App Service from Visual Studio. You can check your applications configuration, settings, etc. from this view. You can navigate to Server Explorer from the View menu. If you are not already connected to Azure, connect through the Server Explorer window. Once connected, this window shows you the relevant Azure services. Under App Service, you will see the relevant resource groups. Expanding a resource group will show the application under this resource group. Select your application, right-click, and choose View Settings. This will show you the configuration of your application for Azure App Service such as whether Remote Debugging is turned on, the connection strings configured for this application, etc.
					The server explorer also allows you to attach a debugger. From Server Explorer, you can also view all the files uploaded to App Service. Under your Web App, if you expand Files, you can see all the files uploaded to the app service including the config files. You can edit these files to change log levels, debug settings, etc. and the changes will take effect immediately. This is a very powerful option to be able to see the exact configuration that your application is running under.
	Lesson 3: Microsoft Azure Fundamentals
		Microsoft Azure Storage:
			Azure Storage is a managed service providing cloud storage that is highly available, secure, durable, scalable, and redundant. All access to data in storage happens through storage accounts. An Azure storage account provides a unique namespace to store and access your Azure Storage data objects. There are various types of storage available.
			Azure File Share:
				Azure Files offers fully managed file shares in the cloud that are accessible via the industry standard Server Message Block (SMB) protocol. Azure file shares can be mounted concurrently by cloud or on-premises deployments. This is a good option in systems that have traditional file servers and need to be migrated to the cloud.
			Azure Queues:
				Azure Queue storage is a service for storing large numbers of messages that can be accessed from anywhere in the world via authenticated calls by using HTTP or HTTPS. The most common usage of queues is for asynchronous communication between applications. A queue can contain a set of messages. Each message can be up to 64KB and can remain in the queue for no more than 7 days.
			Azure Table Storage:
				Azure Table storage is a service that stores structured NoSQL data in the cloud, providing a key/attribute store with a schema-less design. Since Table storage is schema-less, it's easy to adapt your data as the needs of your application evolve. Access to Table storage data is fast and cost-effective for many types of applications and is typically lower in cost than traditional SQL for similar volumes of data.
			Azure Blob Storage:
				Blob storage is object-based storage. This can be used to store image files, audio and video clips, log files etc. Blob storage is optimized for storing large amounts of data – both text as well as binary. Some of the most common uses of Blob storage are for backup and restore and archiving, storing files for distributed access. Data stored in Blob storage can also be used for analysis by any analytics systems. It is possible to configure objects to be stored in Azure storage to be directly accessed by using HTTP or HTTPS. This can be used to load static images from websites. Loading images this way improves the performance of websites.
				To start using Blob storage, first you need to create a storage account. Each storage account has its own endpoint. There are different types of storage accounts such as General Purpose storage accounts and blob storage accounts. Blob storage account is a specialized account in which you can specify the access tier. The access tier is chosen based on the frequency of blob access.
				Within a storage account, you can create containers. A container organizes a set of blobs, similar to a folder in a file system. There can be multiple containers within a storage account and each container can store multiple blobs. Each blob stored in a container in a storage account has its own unique URL. This URL is of the form:
					http://<storageaccountname>.blob.core.windows.net/<containername>/<blobname>
			All the above storage options have their own APIs.
			To use these storage options in your .NET applications, you need to add the storage service as a Connected Service. To do this, in Solution Explorer, right-click your project, and click Add. Choose Connected Service. In the box that appears, choose Cloud Storage with Azure Storage. This will then prompt you to create a storage account if you don’t already one or choose from the ones you have. Once done, the required Nuget packages will get installed and you can now work with any of the above storage options by using the appropriate library.
			To access blobs from an application, you first need to access the storage account. To access the storage account, you need a connection string, this can be obtained from the Azure portal. From the storage account, a client needs to be created. From the client, we need to get a reference to the container and from the reference to the container, you can upload and manage blobs. It is also possible to stream content into blobs by using the API.
		Microsoft Azure SQL:
			Azure SQL is a managed relational database as a service provided by Azure. When applications are running on Azure, it follows that the database should also be running on Azure. Azure SQL provides an easy and efficient way to use a database running on Azure. With Azure SQL, there is no need to manage the underlying infrastructure of the database. All patching of the code, maintaining the infrastructure and other management is handled by Microsoft and this happens seamlessly.
			Scalability Options:
				One of the critical requirements of applications running in production mode is to be able to scale quickly. Azure SQL provides a few options based on the deployment model. Azure SQL provides two deployment models:
					• Database Transaction Unit (DTU) based. This deployment model has a set of pre-defined compute sizes – that include storage, IOPS, CPU and backup size. There are different tiers of DTU that give the users choices depending on workloads to be run.
					• vCore based. In this deployment model, the user can choose CPU, memory, and storage based on requirements.
				In both the deployment models, Azure SQL provides the ability to dynamically scale – the ability to go to a higher tier in case of DTU or higher configuration in case of vCore deployment model, without any downtime.
			Availability Options:
				To be able to run your application 24/7, the underlying infrastructure and services should run at the highest availability. Azure provides a whole lot of features to support this. This includes automatic backups, replications, failure detection, handling underlying hardware, software and network failures. Azure SQL also provides active geo-replication allowing you to configure up to four read replicas of the database in any of the Azure data centers.
			Intelligent Insights and monitoring:
				Azure SQL provides automatic performance monitoring and tuning. Azure SQL learns about your database and the queries being executed and based on this provides performance tuning recommendations. It can also automatically monitor SQL database performance at scale and notifies users of performance degradation issues, identifies the root cause, and suggests improvements and fixes for them. With automatic tuning, users can also ask Azure SQL to directly apply tuning recommendations. Azure SQL, with this feature, ensures your database is carefully monitored and compared before and after every tuning action, and if the performance doesn’t improve, the tuning action is reverted.
			Security and Compliance:
				Azure SQL provides all the basic security measures and compliances. By default, data encryption is provided for data in transit as well as at rest. However, users can also choose to use their own keys for encryption and choose different encryption keys for specific columns. One of the most important security measures is to protect sensitive data such as personal identification, financial data, etc. Azure SQL provides advanced capabilities for discovering, classifying, labeling, and protecting the sensitive data in databases. SQL Database secures your data by limiting access to your database using firewall rules, authentication mechanisms requiring users to prove their identity, and authorization to data through role-based memberships and permissions, as well as through row-level security. Azure SQL also provides a centralized security dashboard by using the SQL vulnerability assessment tool that can help discover and track potential database vulnerabilities.
			Working with Azure SQL:
				To create an Azure SQL database, you need to login to Azure portal and go to Sql Databases and create a server. This is where the database will be hosted. You can choose the configuration to suit your needs. At the time of server creation, you also need to specify the username and password for the server admin login. After the server is created, you can create a database.
				Accessing the database is like accessing a SQL database installed on any server and all the usual tools will work. In addition, it is also possible to execute queries from the Azure portal.
				The only change needed in your applications to work with Azure SQL is to use the connection string for your SQL instance in Azure. You can obtain this connection string from the Azure portal. In the portal, navigate to the SQL databases page and click the database you want to work with. In the database page, in the left-hand menu, click Connection Strings which will show you the connection strings to be used for various connection options in the right-hand side. Copy the one that is relevant to you and use it in application settings.
				In most situations, you would want to use a local database during testing and Azure SQL Database for production. You can configure this at the time of publishing. At the time of publishing the application to Azure, in the Publish window, click configure. In the Configure page, click Settings and then expand the Databases section. Select Use this connection string at runtime check box and type the connection string for Azure SQL here.
		Design a Distributed Application by Using Microsoft Azure:
			With cloud; designing, deploying and maintaining a distributed application is fairly simple. A distributed application consists of an application running on one or more servers. A distributed application can scale much better since there are multiple servers available to handle requests. A distributed application also makes your application highly available. The multiple servers in a distributed environment are behind a load balancer. The load balancer directs requests to one of the many servers, either round-robin fashion or based on the load on a server.
			When you are deploying your application to Azure App Service, you have the option of turning on autoscaling for your application. Once autoscaling is enabled, depending on the server load, Azure App Service will create more instances running with your application. The load balancer will automatically be configured to start sending requests to the new servers.
			While autoscaling is very efficient and reduces the overhead of manually monitoring the server utilization and taking action once capacity is reached, it is important to consider the impact of a distributed environment on the application itself. For example, most applications save some form of user data in the user session. The common practice is to store user session In-proc – i.e. in the memory of the server where the application is running. However, the drawback of this is that if the server crashes, all the user session data is lost. In a distributed environment an additional consideration is determining which server has the user session data for a particular session. This is important because if the user session data is stored on one server, all requests related to that user must be sent to that server only. This is also called session-affinity. This is achieved by use of cookies and storing the server information in a cookie. Load balancers will examine this and redirect the request to the server with the session data.
			Obviously, session-affinity has an impact on scalability. If most requests are tied to a particular server, even though additional servers are added, they will not be utilized properly. One solution to this is to store session data in such a manner that all servers can access it. There are two options for this. The session data can be stored in a database or a common cache from where it can be accessed by all servers. While storing data in a database is the most secure of all options, in terms of performance it is not the best. That leaves us with the option of using a common cache.
			Azure Cache for Redis provides a session state provider that you can use to store your session state in-memory with Redis Cache instead of a SQL Server database. To use the caching session state provider, first, configure your cache, and then configure your ASP.NET application for the cache by using the Redis Cache Session State NuGet package.
			You can create a Redis cache from the portal and provide a DNS name and resource group name. The DNS name is used to access the cache. Configuring Azure Cache for Redis can also be done from the Azure portal. Some properties that can be configured are the cluster-size, eviction policy to free up cache space, firewall, access control etc.
			On the application side, use the Nuget Package Manager to install Microsoft.Web.RedisSessionStateProvider. The NuGet package downloads and adds the required assembly references and adds the necessary section into your web.config file. This section contains the required configuration for your ASP.NET application to use the Redis Cache Session State Provider. Also, remove the standard InProc session state provider section in your web.config.
			Deferred Processing:
				To build highly scalable distributed architecture, one key consideration is to have loosely coupled components and to make use of deferred processing where ever possible. Such systems need ways in which components can communicate without being dependent. Azure provides various services for this.
				Microsoft Azure Service Bus is a fully managed enterprise integration message broker. Service Bus is most commonly used to decouple applications and services from each other and is a reliable and secure platform for asynchronous data and state transfer. Data is transferred between different applications and services by using messages. A message is in binary format, which can contain JSON, XML, or just text. Azure Service Bus supports queues as well as topics. While a queue is often used for point-to-point communication, topics are useful in publish/subscribe scenarios. Listed below are some important features of Azure Service Bus:
					• Scheduled Delivery. Ability to submit messages for delayed processing. For example, at midnight every day.
					• Batching. All messages sent by a client in a certain period of time are grouped in a single message.
					• Transactions. Supports grouping operations against a single messaging entity. For example, several messages sent to one queue from within a transaction scope will only be committed to the queues log when the transaction completes.
					• Auto-forwarding. Enables chaining a queue or subscription to another queue or topic that is part of the same namespace.
				Microsoft Azure WebJobs is a feature that allows for the creation of programs/scripts that can run as background tasks within your application context. WebJobs is a feature of an Azure App Service and runs in the same instance as your application. WebJobs can be run in the following ways:
					• Continuously. Starts as soon as the application starts.
					• Triggered. Runs when this is triggered explicitly.
					• Scheduled. Runs as per a schedule given at creation.
				Microsoft Azure Functions is a solution for easily running small pieces of code or functions, in the cloud. Unlike an application, this is not meant to be an always running website, but a function that gets executed at the occurrence of any particular event. The events or triggers can be:
					• HttpTrigger. Trigger the execution of your code by using an HTTP request.
					• TimerTrigger. Execute cleanup or other batch tasks on a predefined schedule. 
					• BlobTrigger. Process Azure Storage blobs when they are added to containers.
					• QueueTrigger. Respond to messages as they arrive in an Azure Storage queue.
				Hybrid Environments:
					In some situations, for regulatory reasons or data location concerns, it might not be possible to use public cloud such as Azure to host your applications and store data. For such situations, Azure provides Azure Stack. Azure Stack is a hybrid cloud platform that lets you provide Azure services from your datacenter. Azure Stack is an extension of Azure. With Azure Stack, developers can now build modern applications across hybrid cloud environments, balancing the right amount of flexibility and control. Developers can build applications using a consistent set of Azure services and DevOps processes and tools, then collaborate with operations to deploy to the location that best meets the business, technical, and regulatory requirements. In short, use of Azure Services from an application development point of view will remain the same, while deployment of the application can happen either on Azure, or on-prem with Azure Stack. Some common use cases where hybrid deployments using Azure Stack are:
						• Cloud applications that meet varied regulations: Customers can develop and deploy applications in Azure, with full flexibility to deploy on-premises on Azure Stack to meet regulatory or policy requirements, with no code changes needed. Financial and banking related applications are the ones where regulatory requirements are very strict and can benefit from Azure Stack.
						• Edge and disconnected solutions: Customers can address latency and connectivity requirements by processing data locally in Azure Stack and then aggregating in Azure for further analytics, with common application logic across both.
						• Cloud application model on-premises: Customers can use Azure services, containers, serverless, and microservice architectures to update and extend existing applications.
					Azure Stack has two deployment options:
						Azure Stack Integrated Systems. Azure Stack integrated systems are offered through a partnership of Microsoft and hardware partners, creating a solution that offers cloud-paced innovation and computing management simplicity. Because Azure Stack is offered as an integrated hardware and software system, you have the flexibility and control you need, along with the ability to innovate from the cloud. Use Azure Stack integrated systems to create new scenarios and deploy new solutions for your production workloads.
						Azure Stack Development Kit (ASDK). ASDK is a free single server deployment that’s designed for trial and proof of concept purposes. The portal, Azure services, DevOps tools, and Marketplace content are the same across ASDK and integrated systems, so applications built against the ASDK will work when deployed to an integrated system.
						Some of the features of Azure Stack are:
							• Azure Stack makes all the same services available in local data centers that are available in Azure. These include IaaS (Virtual Machines, Storage, Networking etc.), PaaS (Azure App Service, Azure Functions etc.).
							• The user experience for Azure Stack and Azure remains the same, with the same portal being available in both scenarios. The same tools – for monitoring, deployment, configuration and administration work on Azure as well as Azure Stack. This makes working in hybrid environments very easy and hassle-free.
							• Any innovations in Azure, including new Azure services, updates to existing services, and additional Azure Marketplace applications are continuously delivered to Azure stack. This helps in keeping all environments in sync all the time.
							• You can use the same identities (for users as well as applications) across Azure and Azure Stack, since both work with Active Directory.
		Design a Caching Strategy:
			Caching is a common technique that aims to improve the performance and scalability of a system. It does this by temporarily copying frequently accessed data to fast storage that's located close to the application. In situations where data is mostly static and there is a latency in accessing data from the original data store, using a caching mechanism will show dramatic performance improvements. Caching data from a persistent store, such as a database also helps since most databases have a limit on the number of concurrent connections that are available.
			It is important to decide what kind of data is to be cached and how the data in the cache should be handled. The most obvious type of data is static data such as configuration information, reference information, etc. While caching is ineffective for highly dynamic data, since the data can get stale very quickly, it is possible to cache only a portion of the data that is not updated frequently. In the case of highly dynamic data, caching is still possible if the data is non-critical.
			Another important aspect of caching is how the expiration of data is handled and how data is kept up-to-date. There are various expiration policies such as:
				• A most-recently-used policy (in the expectation that the data will not be required again).
				• A first-in-first-out policy (oldest data is evicted first).
				• An explicit removal policy based on a triggered event (such as the data being modified).
			The main options available on how to cache data is an in-memory store, local file system storage, or a shared global cache.
			An in-memory cache is a primitive option and is good only for stand-alone applications. In a distributed application, each instance would end up having its own copy of cache data leading to synchronization issues. Also, the size of such a cache is limited to available memory. This limitation can be overcome by using a local file system, however, this introduces a read-write overhead which makes this type of cache slower.
			Using a shared global cache can help alleviate concerns that data might differ in each cache, which can occur with in-memory caching. Shared caching ensures that different application instances see the same view of cached data. An important benefit of the shared caching approach is the scalability it provides. Many shared cache services are implemented by using a cluster of servers and utilize software that distributes the data across the cluster in a transparent manner. An application instance simply sends a request to the cache service. The underlying infrastructure is responsible for determining the location of the cached data in the cluster.
			Azure Cache for Redis is an implementation of the open source Redis cache that runs as a service in an Azure datacenter. It provides a caching service that can be accessed from any Azure application. Azure Cache for Redis is a high-performance caching solution that provides availability, scalability, and security. It typically runs as a service spread across one or more dedicated machines. It attempts to store as much information as it can in memory to ensure fast access. It is compatible with many of the various APIs that are used by client applications.
			You can provision a cache for your application by using the Azure portal. When you create a Redis cache from the portal you need to provide a DNS name and resource group name. The DNS name is used to access the cache. Using the Azure portal, you can also configure the eviction policy of the cache, and control access to the cache, monitoring and alerting policies, firewall rules, etc.
			In an earlier section, we have already seen how to use Redis as a session state provider. In addition to this, Azure Cache for Redis can be used generally for normal caching needs as well. Redis is a key-value store, where values can contain simple types or complex data structures such as hashes, lists, and sets. Keys can be permanent or tagged with a limited time-to-live, at which point the key and its corresponding value are automatically removed from the cache.
			To add Redis to your application, in Visual Studio, use NuGet Package Manager to install StackExchange.Redis.
			Content Delivery Network:
				While Redis cache is suitable for caching data used within an application, an application front end has a lot of static files such as HTML files, images, logos, etc. that can also benefit from caching. This type of caching is possible with Content Delivery Network (CDN). A CDN is a distributed network of servers that can efficiently deliver web content to users. Azure CDN is a global CDN solution for delivering high-bandwidth content. With Azure CDN, you can cache static objects loaded from Azure Blob storage, a web application, or any publicly accessible web server, by using edge servers in point-of-presence (POP) locations that are close to end users, to minimize latency.
				The benefits of using Azure CDN to deliver web site assets include:
					• Better performance and improved user experience for end users, especially when using applications in which multiple round-trips are required to load content.
					• Large scaling to better handle instantaneous high loads, such as the start of a product launch event.
					• Distribution of user requests and serving of content directly from edge servers so that less traffic is sent to the origin server.
				To add Azure CDN to your web application, in the Azure portal, navigate to your app in the App services page. In the App services page, in the Settings section, go to Networking. On the right-hand side pane, expand the Azure CDN section and in this, you will see the Configure Azure CDN for your app option. In the Azure Content Delivery Network page, provide the New endpoint settings. You will need to specify:
					• Name of your CDN
					• Pricing Tier - Specifies the provider and available features. 
					• CDN endpoint name - Any name that is unique in the azureedge.net domain. Cached resources are accessed as <endpointname>.azureedge.net
				That CDNs are best for static files is now established, Azure CDN provides an option to improve the performance of web pages with dynamic content. This can be achieved with Azure CDN dynamic site acceleration (DSA) optimization. This option can be chosen while creating the CDN endpoint.
				One important consideration while using CDNs, is to be aware of the effects of cached files. In case your application makes changes to any static files, they may not get reflected. Azure CDN edge nodes will cache assets until the asset's time-to-live (TTL) expires. After the asset's TTL expires, when a client requests the asset from the edge node, the edge node will retrieve a new updated copy of the asset to serve the client request and store the new copy inside the cache.
				The best practice to make sure your users always obtain the latest copy of your assets is to version your assets for each update and publish them as new URLs. CDN will immediately retrieve the new assets for the next client requests. Sometimes you may wish to purge cached content from all edge nodes and force them all to retrieve newly updated assets. This might be due to updates to your web application or to quickly update assets that contain incorrect information.
		Security in Microsoft Azure:
			While deploying applications on the cloud and running them in a distributed environment, one key thing to consider is securing your application configuration or application secrets. i.e. database connection strings, third-party API keys, passwords, etc. It is possible to do so by storing these in configuration files in an encrypted manner. Managing keys is an overhead. Managing change in these – for example, change of a password/connection string– could possibly mean redeployment of the application and is not ideal.
			Azure provides Azure Key Vault that can be used by applications to store keys and secrets. Key Vault is a cloud service that works as a secure secrets store. Key Vault helps safeguard cryptographic keys and secrets used by cloud applications and services. By using Key Vault, you can encrypt keys and secrets (such as authentication keys, storage account keys, data encryption keys, and passwords) by using keys protected by hardware security modules (HSMs).	
			Using Key Vault provides the following benefits:
				• Centralize application secrets: Centralizing storage of application secrets in Key Vault allows you to control their distribution. This greatly reduces the chances that secrets may be accidentally leaked. When using Key Vault, application developers no longer need to store security information in their application. This eliminates the need to make this information part of the code. 
				• Securely store keys: Secrets and keys are safeguarded by Azure by using industry-standard algorithms, key lengths, and HSMs.
				• Monitor access and use: Since keys and secrets are stored centrally, it is easy to monitor how and when these are being used.
				• Simplified administration: The overhead of having to encrypt data, perform key management by rotating keys periodically, having to manage an in-house HSM, certificate management, etc. is removed by Key Vault.
			To use Key Vault, the following steps need to be performed:
				1. Create a Key Vault.
				2. Authorize your application to access Key Vault
				3. Generate a token to be used by your application for authenticating itself with the Key Vault
				4. Create a Key Vault client to access keys and secrets.
			Creating a Key Vault:
				To create a key vault, go to the Azure portal and in the search bar on top, search for "key vault", and then go to the Key Vault page. On this page, click Create. On this page, provide the requested details. You will need to specify a globally unique name, choose an appropriate Azure subscription, resource group, and location for the vault. After entering all relevant information, click Create. Your vault has now been created. The URL to access the vault will be https://<YOUR_VAULT_NAME>.vault.azure.net. The vault is now empty.
			Authorize your application to access Key Vault:
				Before you can start accessing the key vault from your application, you will need to authorize the application to access the vault. For this, you need to do two things:
					• Create an identity for the application in Azure Active Directory (AD).
					• Provide this identity in Key Vault to authorize the application to access Key Vault.
				Azure Active Directory (AD) is an identity service provider responsible for verifying the identity of users and applications that exist in an organization’s directory and ultimately for issuing security tokens upon successful authentication of those users and applications. Key Vault uses this service to authorize applications.
				To create an identity in Azure AD, in the portal search for "App Registrations" and on this page click New application registration. On the Create page provide your application name, and then select WEB APPLICATION AND/OR WEB API (the default) and specify the URL to your web application. Click the Create button. When the app registration is complete, you can see the list of registered apps which will also contain your application. The list will display the Application ID. If you click your application, a pane opens on the right-hand side and will have a Settings button on top. Click this button and in the pane below, click Keys. Enter in a description in the Key description box and select a duration, and then click SAVE. The page refreshes and now shows a key value. The application Id and key are the identity of your application.
				The next step is to configure Key Vault to allow your application access. For this, in the portal, go to your key vault. From the menu, choose Access Policies and click Add New. In the window that appears, click Select Principal. In the list that appears, select the name with which you had created an identity for your application in AD. Also select the appropriate key, secret and certificate permissions necessary for your application. Generally, GET operations will be the most often performed operations. Click Save and with this, your application is now authorized to access Key Vault.
			Generate token to be used by your application for authenticating itself with the Key Vault:
				Once the above configuration is done, you are now ready to start accessing the keys and secrets from the Key Vault. We now look at the changes needed in your application to perform these operations. There are two packages that your web application needs to have installed.
					• Active Directory Authentication Library has methods for interacting with Azure Active Directory and managing user identity.
					• Azure Key Vault Library has methods for interacting with Azure Key Vault.
				To use the Key Vault API, you need an access token. The Key Vault client handles calls to the Key Vault API but you need to supply it with a function that gets the access token.
			Create a Key Vault client to access keys and secrets.:
				The next thing to do is to start accessing data from the Key Vault. For this, you need to create an instance of KeyVaultClient. A reference to the GetToken method needs to be passed while doing this.